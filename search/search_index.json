{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"\u00d7 Made With ML <p>BY</p> <p>Join 40K+ developers in learning how to responsibly deliver value with ML!</p>  Subscribe              View lessons"},{"location":"#course","title":"ML for DevelopersWho is this content for?","text":"<p>Design \u00b7 Develop \u00b7 Deploy \u00b7 Iterate</p> <p>Learn how to combine machine learning with software engineering to design, develop, deploy and iterate on production ML applications. \u2192  GokuMohandas/Made-With-ML</p> 1. \ud83c\udfa8 Design <ul> <li>Setup</li> <li>Product</li> <li>Systems</li> </ul> 2. \ud83d\udd22 Data <ul> <li>Preparation</li> <li>Exploration</li> <li>Preprocessing</li> <li>Distributed</li> </ul> 3. \ud83e\udd16 Model <ul> <li>Training</li> <li>Tracking</li> <li>Tuning</li> <li>Evaluation</li> <li>Serving</li> </ul> 4. \ud83d\udcbb Develop <ul> <li>Scripting</li> <li>Command-line</li> </ul> 5. \ud83d\udce6 Utilities <ul> <li>Logging</li> <li>Documentation</li> <li>Styling</li> <li>Pre-commit</li> </ul> 6. \ud83e\uddea Test <ul> <li>Code</li> <li>Data</li> <li>Models</li> </ul> 7. \u267b\ufe0f Reproducibility <ul> <li>Versioning</li> </ul> 8. \ud83d\ude80 Production <ul> <li>Jobs &amp; Services</li> <li>CI/CD workflows</li> <li>Monitoring</li> <li>Data engineering</li> </ul> <p>Live cohort</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.          </p>  Learn more <p></p> \u00a0 While the specific task in this course involves fine-tuning an LLM for a supervised task, everything we learn easily extends to all applications (NLP, CV, time-series, etc.), models (regression \u2192 LLMs), data modalities (tabular, text, etc.), cloud platforms (AWS, GCP) and scale (local laptop \u2192 distributed cluster).  <p></p> First principles                   Before we jump straight into the code, we develop a first principles understanding for every machine learning concept.          Best practices                   Implement software engineering best practices as we develop and deploy our machine learning models.          Scale                   Easily scale ML workloads (data, train, tune, serve) in Python without having to learn completely new languages.          MLOps                   Connect MLOps components (tracking, testing, serving, orchestration, etc.) as we build an end-to-end machine learning system.          Dev to Prod                   Learn how to quickly and reliably go from development to production without any changes to our code or infra management.          CI/CD                   Learn how to create mature CI/CD workflows to continuously train and deploy better models in a modular way that integrates with any stack.          <p>Machine learning is not a separate industry, instead, it's a powerful way of thinking about data that's not reserved for any one type of person.</p>              \ud83d\udc69\u200d\ud83d\udcbb\u00a0 All developers                       Whether software/infra engineer or data scientist, ML is increasingly becoming a key part of the products that you'll be developing.                       \ud83d\udc69\u200d\ud83c\udf93\u00a0 College graduates                       Learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.                       \ud83d\udc69\u200d\ud83d\udcbc\u00a0 Product/Leadership                       who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning."},{"location":"#instructor","title":"Meet your instructor","text":"Hi, I'm Goku Mohandas <p>     I've spent my career developing ML applications across all scales and industries. Specifically over the last four years (through Made With ML), I\u2019ve had the opportunity to help dozens of F500 companies + startups build out their ML platforms and launch high-impact ML applications on top of them. I started Made With ML to address the gaps in education and share the best practices on how to deliver value with ML in production. </p> <p>     While this was an amazing experience, it was also a humbling one because there were obstacles around scale, integrations and productionization that I didn\u2019t have great solutions for. So, I decided to join a team that has been addressing these precise obstacles with some of the best ML teams in the world and has an even bigger vision I could stand behind. So I'm excited to announce that Made With ML is now part of Anyscale to accelerate the path towards production ML. </p>  \ud83c\udf89\u00a0 Made With ML is now part of Anyscale, read more about it here!"},{"location":"#wall-of-love","title":"\u2764\ufe0f Wall of LoveFrequently Asked Questions (FAQ)","text":"<p>See what the community has to say about Made With ML.</p> Sherry Wang Senior ML Engineer - Cars.com <p>\"Made with ML is one of the best courses I\u2019ve ever taken. The material covered is very practical; I get to apply some of them to my job right away.\"</p> Deepak Jayakumaran Lead Data Scientist - Grab <p>\"This course has given me the know-how to make optimal choices around design &amp; implementation of ML engineering for a variety of real-world use-cases.\"</p> Jeremy Jordan Senior ML Engineer - Duo Security <p>\"This will be a great journey for those interested in deploying machine learning models which lead to a positive impact on the product.\"</p> Clara Matos Head of AI Eng - Sword Health <p>\"This course really mimics the production ML thought process by providing alternative options with different levels of complexity &amp; weighing on the pros/cons.\"</p> Ritchie Ng PyTorch Keynote Speaker <p>\"For production ML, I cannot possibly think of a better resource out there ... this resource is the gold standard.\"</p> Greg Coquillo AI Product - Amazon <p>\"One of the best places where you can learn the fundamentals of ML, then practice MLOps by building production grade products and delivering value!\"</p> Kavin Veerapandian Senior Analyst - Citi <p>\"Coming from academia with purely model-specific knowledge, Made With ML set the expectations right when it comes to how ML is being applied in the industry.\"</p> Daniel Bourke Founder - Mrdbourke <p>\"Built some machine learning models? Want to take them to the next level? I do. And I\u2019m using @madewithml to learn how. Outstanding MLOps lessons!\"</p> Dmitry Petrov Co-Founder, CEO - DVC <p>\"This is not a usual ML class, it covers productionalization part of ML projects - the most important part from a business point of view.\"</p> Lawrence Okegbemi ML Engineer - Enterscale <p>\"Following all through, it's really amazing to see how you demonstrated best practices in building an ML driven application.\"</p> Laxman Tomar ML Engineer - Robofied <p>\"The best MLOps resource that I've come across on the web. Goes over whys, hows, tradeoffs, tools &amp; their alternatives via high-quality explanations and code.\"</p> Satyabrata Pal Julia Community <p>\"Completely sold out on the clean code and detailed writeup. This is one of the few ML courses which doesn't stop on just training a model but goes beyond.\"</p> \u2039 \u203a <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> Who is this course for?              Machine learning is not a separate industry, instead, it's a powerful way of thinking about data that's not reserved for any one type of person.                 <ul> <li>All developers Whether software engineer or data scientist, ML is increaingly becoming a key part of the products that you'll be developing.</li> <li>College graduates Learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.</li> <li>Product / Leadesrhip who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning.</li> </ul> What are the prerequisites? <p>You should know how to code in Python and the basics of machine learning.</p> <ul> <li>currently working with ML in industry or academia</li> <li>Python (basics, NumPy, Pandas)</li> <li>ML / deep learning basics (logistic regression, PyTorch, etc.)</li> </ul> Why should I take this course now?                  Machine learning is increasingly becoming a key part of many products and so companies are looking for people with deeper knowledge on not only modeling, but how to operationalize it (MLOps). It's a major advantage to understand the fundamentals of this field at this nascent stage so you can responsibly design, develop, deploy and iterate on production ML applications as a foundational developer in your respective industry.              What is the time commitment?                      You can go through the lessons at your pace or sign up for our upcoming live cohort where we'll provide live lessons, QA, compute (GPUs) and community to learn everything in one day.                  What happens after the course?                  After the course, you'll have access to our private community where you can connect with alumni and meet future cohort members as well. You can continue to ask questions about the topics (especially as new tools enter the market), get feedback on your work, etc.              Is this course fully remote?                  When you sign up for the course, you'll have the choice of attending remotely or at one of our in-person weekend sessions near you.              \u00a0 If you have additional questions, send us an email and we'll get back to you very soon.  <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Home - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"about/","title":"Our Mission","text":""},{"location":"about/#instructor","title":"Meet your instructor","text":"Hi, I'm Goku Mohandas <p>     I've spent my career developing ML applications across all scales and industries. Specifically over the last four years (through Made With ML), I\u2019ve had the opportunity to help dozens of F500 companies + startups build out their ML platforms and launch high-impact ML applications on top of them. I started Made With ML to address the gaps in education and share the best practices on how to deliver value with ML in production. </p> <p>     While this was an amazing experience, it was also a humbling one because there were obstacles around scale, integrations and productionization that I didn\u2019t have great solutions for. So, I decided to join a team that has been addressing these precise obstacles with some of the best ML teams in the world and has an even bigger vision I could stand behind. So I'm excited to announce that Made With ML is now part of Anyscale to accelerate the path towards production ML. </p>  \ud83c\udf89\u00a0 Made With ML is now part of Anyscale, read more about it here!  <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p>"},{"location":"courses/foundations/","title":"Foundations","text":"1. \ud83d\udee0 Toolkit <ul> <li>Notebooks</li> <li>Python</li> <li>NumPy</li> <li>Pandas</li> <li>PyTorch</li> </ul> 2. \ud83d\udd25 Machine Learning <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Neural Networks</li> <li>Data Quality</li> <li>Utilities</li> </ul> 3. \ud83e\udd16 Deep Learning <ul> <li>CNNs</li> <li>Embeddings</li> <li>RNNs</li> <li>Attention</li> <li>Transformers</li> </ul> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Foundations - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/","title":"MLOps Course","text":"1. \ud83c\udfa8 Design <ul> <li>Setup</li> <li>Product</li> <li>Systems</li> </ul> 2. \ud83d\udd22 Data <ul> <li>Preparation</li> <li>Exploration</li> <li>Preprocessing</li> <li>Distributed</li> </ul> 3. \ud83e\udd16 Model <ul> <li>Training</li> <li>Tracking</li> <li>Tuning</li> <li>Evaluation</li> <li>Serving</li> </ul> 4. \ud83d\udcbb Develop <ul> <li>Scripting</li> <li>Command-line</li> </ul> 5. \ud83d\udce6 Utilities <ul> <li>Logging</li> <li>Documentation</li> <li>Styling</li> <li>Pre-commit</li> </ul> 6. \ud83e\uddea Test <ul> <li>Code</li> <li>Data</li> <li>Models</li> </ul> 7. \u267b\ufe0f Reproducibility <ul> <li>Versioning</li> </ul> 8. \ud83d\ude80 Production <ul> <li>Jobs &amp; Services</li> <li>CI/CD workflows</li> <li>Monitoring</li> <li>Data engineering</li> </ul> <p>Live cohort</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.          </p>  Learn more <p></p> \u00a0 While the specific task in this course involves fine-tuning an LLM for a supervised task, everything we learn easily extends to all applications (NLP, CV, time-series, etc.), models (regression \u2192 LLMs), data modalities (tabular, text, etc.), cloud platforms (AWS, GCP) and scale (local laptop \u2192 distributed cluster).  <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { MLOps Course - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/attention/","title":"Attention","text":""},{"location":"courses/foundations/attention/#overview","title":"Overview","text":"<p>In the RNN lesson, we were constrained to using the representation at the very end but what if we could give contextual weight to each encoded input (\\(h_i\\)) when making our prediction? This is also preferred because it can help mitigate the vanishing gradient issue which stems from processing very long sequences. Below is attention applied to the outputs from an RNN. In theory, the outputs can come from anywhere where we want to learn how to weight amongst them but since we're working with the context of an RNN from the previous lesson , we'll continue with that.</p> \\[ \\alpha = softmax(W_{attn}h) \\] \\[ c_t = \\sum_{i=1}^{n} \\alpha_{t,i}h_i \\] <p> Variable Description \\(N\\) batch size \\(M\\) max sequence length in the batch \\(H\\) hidden dim, model dim, etc. \\(h\\) RNN outputs (or any group of outputs you want to attend to) \\(\\in \\mathbb{R}^{NXMXH}\\) \\(\\alpha_{t,i}\\) alignment function context vector \\(c_t\\) (attention in our case) $ \\(W_{attn}\\) attention weights to learn \\(\\in \\mathbb{R}^{HX1}\\) \\(c_t\\) context vector that accounts for the different inputs with attention <p></p> <ul> <li>Objective:<ul> <li>At it's core, attention is about learning how to weigh a group of encoded representations to produce a context-aware representation to use for downstream tasks. This is done by learning a set of attention weights and then using softmax to create attention values that sum to 1.</li> </ul> </li> <li>Advantages:<ul> <li>Learn how to account for the appropriate encoded representations regardless of position.</li> </ul> </li> <li>Disadvantages:<ul> <li>Another compute step that involves learning weights.</li> </ul> </li> <li>Miscellaneous:<ul> <li>Several state-of-the-art approaches extend on basic attention to deliver highly context-aware representations (ex. self-attention).</li> </ul> </li> </ul>"},{"location":"courses/foundations/attention/#set-up","title":"Set up","text":"<p>Let's set our seed and device for our main task. <pre><code>import numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code>def set_seeds(seed=1234):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # multi-GPU\n</code></pre> <pre><code># Set seeds for reproducibility\nset_seeds(seed=SEED)\n</code></pre> <pre><code># Set device\ncuda = True\ndevice = torch.device(\"cuda\" if (\n    torch.cuda.is_available() and cuda) else \"cpu\")\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\nif device.type == \"cuda\":\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\nprint (device)\n</code></pre></p> <pre>\ncuda\n</pre>"},{"location":"courses/foundations/attention/#load-data","title":"Load data","text":"<p>We will download the AG News dataset, which consists of 120K text samples from 4 unique classes (<code>Business</code>, <code>Sci/Tech</code>, <code>Sports</code>, <code>World</code>) <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/news.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> title category 0 Sharon Accepts Plan to Reduce Gaza Army Operation... World 1 Internet Key Battleground in Wildlife Crime Fight Sci/Tech 2 July Durable Good Orders Rise 1.7 Percent Business 3 Growing Signs of a Slowing on Wall Street Business 4 The New Faces of Reality TV World"},{"location":"courses/foundations/attention/#preprocessing","title":"Preprocessing","text":"<p>We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc. <pre><code>import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n</code></pre> <pre><code>nltk.download(\"stopwords\")\nSTOPWORDS = stopwords.words(\"english\")\nprint (STOPWORDS[:5])\nporter = PorterStemmer()\n</code></pre></p> <pre>\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n['i', 'me', 'my', 'myself', 'we']\n</pre> <p><pre><code>def preprocess(text, stopwords=STOPWORDS):\n\"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Remove stopwords\n    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n    text = pattern.sub(\"\", text)\n\n    # Remove words in parenthesis\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n\n    # Spacing and filters\n    text = re.sub(r\"([-;;.,!?&lt;=&gt;])\", r\" \\1 \", text)\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()\n\n    return text\n</code></pre> <pre><code># Sample\ntext = \"Great week for the NYSE!\"\npreprocess(text=text)\n</code></pre></p> <pre>\ngreat week nyse\n</pre> <pre><code># Apply to dataframe\npreprocessed_df = df.copy()\npreprocessed_df.title = preprocessed_df.title.apply(preprocess)\nprint (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")\n</code></pre> <pre>\nSharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n\nsharon accepts plan reduce gaza army operation haaretz says\n</pre> <p>Warning</p> <p>If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before applying those operations. This is because we cannot apply any knowledge gained from the test set accidentally (data leak) during preprocessing/training. However for global preprocessing steps like the function above where we aren't learning anything from the data itself, we can perform before splitting the data.</p>"},{"location":"courses/foundations/attention/#split-data","title":"Split data","text":"<p><pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Data\nX = preprocessed_df[\"title\"].values\ny = preprocessed_df[\"category\"].values\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (84000,), y_train: (84000,)\nX_val: (18000,), y_val: (18000,)\nX_test: (18000,), y_test: (18000,)\nSample point: china battles north korea nuclear talks \u2192 World\n</pre>"},{"location":"courses/foundations/attention/#label-encoding","title":"Label encoding","text":"<p>Next we'll define a <code>LabelEncoder</code> to encode our text labels into unique indices <pre><code>import itertools\n</code></pre> <pre><code>class LabelEncoder(object):\n\"\"\"Label encoder for tag labels.\"\"\"\n    def __init__(self, class_to_index={}):\n        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n\n    def __len__(self):\n        return len(self.class_to_index)\n\n    def __str__(self):\n        return f\"&lt;LabelEncoder(num_classes={len(self)})&gt;\"\n\n    def fit(self, y):\n        classes = np.unique(y)\n        for i, class_ in enumerate(classes):\n            self.class_to_index[class_] = i\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n        return self\n\n    def encode(self, y):\n        encoded = np.zeros((len(y)), dtype=int)\n        for i, item in enumerate(y):\n            encoded[i] = self.class_to_index[item]\n        return encoded\n\n    def decode(self, y):\n        classes = []\n        for i, item in enumerate(y):\n            classes.append(self.index_to_class[item])\n        return classes\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {'class_to_index': self.class_to_index}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Encode\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nNUM_CLASSES = len(label_encoder)\nlabel_encoder.class_to_index\n</code></pre></p> <pre>\n{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.encode(y_train)\ny_val = label_encoder.encode(y_val)\ny_test = label_encoder.encode(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\n</code></pre> <pre>\ny_train[0]: World\ny_train[0]: 3\n</pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [21000 21000 21000 21000]\nweights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n</pre>"},{"location":"courses/foundations/attention/#tokenizer","title":"Tokenizer","text":"<p>We'll define a <code>Tokenizer</code> to convert our text input data into token indices.</p> <p><pre><code>import json\nfrom collections import Counter\nfrom more_itertools import take\n</code></pre> <pre><code>class Tokenizer(object):\n    def __init__(self, char_level, num_tokens=None,\n                 pad_token=\"&lt;PAD&gt;\", oov_token=\"&lt;UNK&gt;\",\n                 token_to_index=None):\n        self.char_level = char_level\n        self.separator = \"\" if self.char_level else \" \"\n        if num_tokens: num_tokens -= 2 # pad + unk tokens\n        self.num_tokens = num_tokens\n        self.pad_token = pad_token\n        self.oov_token = oov_token\n        if not token_to_index:\n            token_to_index = {pad_token: 0, oov_token: 1}\n        self.token_to_index = token_to_index\n        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n\n    def __len__(self):\n        return len(self.token_to_index)\n\n    def __str__(self):\n        return f\"&lt;Tokenizer(num_tokens={len(self)})&gt;\"\n\n    def fit_on_texts(self, texts):\n        if not self.char_level:\n            texts = [text.split(\" \") for text in texts]\n        all_tokens = [token for text in texts for token in text]\n        counts = Counter(all_tokens).most_common(self.num_tokens)\n        self.min_token_freq = counts[-1][1]\n        for token, count in counts:\n            index = len(self)\n            self.token_to_index[token] = index\n            self.index_to_token[index] = token\n        return self\n\n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            if not self.char_level:\n                text = text.split(\" \")\n            sequence = []\n            for token in text:\n                sequence.append(self.token_to_index.get(\n                    token, self.token_to_index[self.oov_token]))\n            sequences.append(np.asarray(sequence))\n        return sequences\n\n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = []\n            for index in sequence:\n                text.append(self.index_to_token.get(index, self.oov_token))\n            texts.append(self.separator.join([token for token in text]))\n        return texts\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {\n                \"char_level\": self.char_level,\n                \"oov_token\": self.oov_token,\n                \"token_to_index\": self.token_to_index\n            }\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre></p> <p>Warning</p> <p>It's important that we only fit using our train data split because during inference, our model will not always know every token so it's important to replicate that scenario with our validation and test splits as well.</p> <pre><code># Tokenize\ntokenizer = Tokenizer(char_level=False, num_tokens=5000)\ntokenizer.fit_on_texts(texts=X_train)\nVOCAB_SIZE = len(tokenizer)\nprint (tokenizer)\n</code></pre> <pre>\n&lt;Tokenizer(num_tokens=5000)&gt;\n\n</pre> <pre><code># Sample of tokens\nprint (take(5, tokenizer.token_to_index.items()))\nprint (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens\n</code></pre> <pre>\n[('&lt;PAD&gt;', 0), ('&lt;UNK&gt;', 1), ('39', 2), ('b', 3), ('gt', 4)]\nleast freq token's freq: 14\n</pre> <pre><code># Convert texts to sequences of indices\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(X_test)\npreprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\nprint (\"Text to indices:\\n\"\n    f\"  (preprocessed) \u2192 {preprocessed_text}\\n\"\n    f\"  (tokenized) \u2192 {X_train[0]}\")\n</code></pre> <pre>\nText to indices:\n  (preprocessed) \u2192 china battles north korea nuclear talks\n  (tokenized) \u2192 [  16 1491  285  142  114   24]\n</pre>"},{"location":"courses/foundations/attention/#padding","title":"Padding","text":"<p>We'll need to do 2D padding to our tokenized text. <pre><code>def pad_sequences(sequences, max_seq_len=0):\n\"\"\"Pad sequences to max length in sequence.\"\"\"\n    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n    padded_sequences = np.zeros((len(sequences), max_seq_len))\n    for i, sequence in enumerate(sequences):\n        padded_sequences[i][:len(sequence)] = sequence\n    return padded_sequences\n</code></pre> <pre><code># 2D sequences\npadded = pad_sequences(X_train[0:3])\nprint (padded.shape)\nprint (padded)\n</code></pre></p> <pre>\n(3, 6)\n[[1.600e+01 1.491e+03 2.850e+02 1.420e+02 1.140e+02 2.400e+01]\n [1.445e+03 2.300e+01 6.560e+02 2.197e+03 1.000e+00 0.000e+00]\n [1.200e+02 1.400e+01 1.955e+03 1.005e+03 1.529e+03 4.014e+03]]\n</pre>"},{"location":"courses/foundations/attention/#datasets","title":"Datasets","text":"<p>We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits.</p> <p><pre><code>class Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __str__(self):\n        return f\"&lt;Dataset(N={len(self)})&gt;\"\n\n    def __getitem__(self, index):\n        X = self.X[index]\n        y = self.y[index]\n        return [X, len(X), y]\n\n    def collate_fn(self, batch):\n\"\"\"Processing on a batch.\"\"\"\n        # Get inputs\n        batch = np.array(batch)\n        X = batch[:, 0]\n        seq_lens = batch[:, 1]\n        y = batch[:, 2]\n\n        # Pad inputs\n        X = pad_sequences(sequences=X)\n\n        # Cast\n        X = torch.LongTensor(X.astype(np.int32))\n        seq_lens = torch.LongTensor(seq_lens.astype(np.int32))\n        y = torch.LongTensor(y.astype(np.int32))\n\n        return X, seq_lens, y\n\n    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n        return torch.utils.data.DataLoader(\n            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n            shuffle=shuffle, drop_last=drop_last, pin_memory=True)\n</code></pre> <pre><code># Create datasets\ntrain_dataset = Dataset(X=X_train, y=y_train)\nval_dataset = Dataset(X=X_val, y=y_val)\ntest_dataset = Dataset(X=X_test, y=y_test)\nprint (\"Datasets:\\n\"\n    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {train_dataset[0][0]}\\n\"\n    f\"  seq_len: {train_dataset[0][1]}\\n\"\n    f\"  y: {train_dataset[0][2]}\")\n</code></pre></p> <pre>\nDatasets:\n  Train dataset: &lt;Dataset(N=84000)&gt;\n  Val dataset: &lt;Dataset(N=18000)&gt;\n  Test dataset: &lt;Dataset(N=18000)&gt;\nSample point:\n  X: [  16 1491  285  142  114   24]\n  seq_len: 6\n  y: 3\n</pre> <pre><code># Create dataloaders\nbatch_size = 64\ntrain_dataloader = train_dataset.create_dataloader(\n    batch_size=batch_size)\nval_dataloader = val_dataset.create_dataloader(\n    batch_size=batch_size)\ntest_dataloader = test_dataset.create_dataloader(\n    batch_size=batch_size)\nbatch_X, batch_seq_lens, batch_y = next(iter(train_dataloader))\nprint (\"Sample batch:\\n\"\n    f\"  X: {list(batch_X.size())}\\n\"\n    f\"  seq_lens: {list(batch_seq_lens.size())}\\n\"\n    f\"  y: {list(batch_y.size())}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {batch_X[0]}\\n\"\n    f\" seq_len: {batch_seq_lens[0]}\\n\"\n    f\"  y: {batch_y[0]}\")\n</code></pre> <pre>\nSample batch:\n  X: [64, 14]\n  seq_lens: [64]\n  y: [64]\nSample point:\n  X: tensor([  16, 1491,  285,  142,  114,   24,    0,    0,    0,    0,    0,    0,\n           0,    0])\n seq_len: 6\n  y: 3\n</pre>"},{"location":"courses/foundations/attention/#trainer","title":"Trainer","text":"<p>Let's create the <code>Trainer</code> class that we'll use to facilitate training for our experiments.</p> <pre><code>class Trainer(object):\n    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n\n        # Set params\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train_step(self, dataloader):\n\"\"\"Train step.\"\"\"\n        # Set model to train mode\n        self.model.train()\n        loss = 0.0\n\n        # Iterate over train batches\n        for i, batch in enumerate(dataloader):\n\n            # Step\n            batch = [item.to(self.device) for item in batch]  # Set device\n            inputs, targets = batch[:-1], batch[-1]\n            self.optimizer.zero_grad()  # Reset gradients\n            z = self.model(inputs)  # Forward pass\n            J = self.loss_fn(z, targets)  # Define loss\n            J.backward()  # Backward pass\n            self.optimizer.step()  # Update weights\n\n            # Cumulative Metrics\n            loss += (J.detach().item() - loss) / (i + 1)\n\n        return loss\n\n    def eval_step(self, dataloader):\n\"\"\"Validation or test step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        loss = 0.0\n        y_trues, y_probs = [], []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Step\n                batch = [item.to(self.device) for item in batch]  # Set device\n                inputs, y_true = batch[:-1], batch[-1]\n                z = self.model(inputs)  # Forward pass\n                J = self.loss_fn(z, y_true).item()\n\n                # Cumulative Metrics\n                loss += (J - loss) / (i + 1)\n\n                # Store outputs\n                y_prob = torch.sigmoid(z).cpu().numpy()\n                y_probs.extend(y_prob)\n                y_trues.extend(y_true.cpu().numpy())\n\n        return loss, np.vstack(y_trues), np.vstack(y_probs)\n\n    def predict_step(self, dataloader):\n\"\"\"Prediction step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        y_probs = []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Forward pass w/ inputs\n                inputs, targets = batch[:-1], batch[-1]\n                y_prob = F.softmax(model(inputs), dim=1)\n\n                # Store outputs\n                y_probs.extend(y_prob)\n\n        return np.vstack(y_probs)\n\n    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n        best_val_loss = np.inf\n        for epoch in range(num_epochs):\n            # Steps\n            train_loss = self.train_step(dataloader=train_dataloader)\n            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n            self.scheduler.step(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = self.model\n                _patience = patience  # reset _patience\n            else:\n                _patience -= 1\n            if not _patience:  # 0\n                print(\"Stopping early!\")\n                break\n\n            # Logging\n            print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.5f}, \"\n                f\"val_loss: {val_loss:.5f}, \"\n                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n                f\"_patience: {_patience}\"\n            )\n        return best_model\n</code></pre>"},{"location":"courses/foundations/attention/#attention","title":"Attention","text":"<p>Attention applied to the outputs from an RNN. In theory, the outputs can come from anywhere where we want to learn how to weight amongst them but since we're working with the context of an RNN from the previous lesson , we'll continue with that.</p> \\[ \\alpha = softmax(W_{attn}h) \\] \\[ c_t = \\sum_{i=1}^{n} \\alpha_{t,i}h_i \\] <p> Variable Description \\(N\\) batch size \\(M\\) max sequence length in the batch \\(H\\) hidden dim, model dim, etc. \\(h\\) RNN outputs (or any group of outputs you want to attend to) \\(\\in \\mathbb{R}^{NXMXH}\\) \\(\\alpha_{t,i}\\) alignment function context vector \\(c_t\\) (attention in our case) $ \\(W_{attn}\\) attention weights to learn \\(\\in \\mathbb{R}^{HX1}\\) \\(c_t\\) context vector that accounts for the different inputs with attention <p></p> <pre><code>import torch.nn.functional as F\n</code></pre> <p>The RNN will create an encoded representation for each word in our input resulting in a stacked vector that has dimensions \\(NXMXH\\), where N is the # of samples in the batch, M is the max sequence length in the batch, and H is the number of hidden units in the RNN.</p> <pre><code>BATCH_SIZE = 64\nSEQ_LEN = 8\nEMBEDDING_DIM = 100\nRNN_HIDDEN_DIM = 128\n</code></pre> <pre><code># Embed\nx = torch.rand((BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM))\n</code></pre> <pre><code># Encode\nrnn = nn.RNN(EMBEDDING_DIM, RNN_HIDDEN_DIM, batch_first=True)\nout, h_n = rnn(x) # h_n is the last hidden state\nprint (\"out: \", out.shape)\nprint (\"h_n: \", h_n.shape)\n</code></pre> <pre>\nout:  torch.Size([64, 8, 128])\nh_n:  torch.Size([1, 64, 128])\n</pre> <pre><code># Attend\nattn = nn.Linear(RNN_HIDDEN_DIM, 1)\ne = attn(out)\nattn_vals = F.softmax(e.squeeze(2), dim=1)\nc = torch.bmm(attn_vals.unsqueeze(1), out).squeeze(1)\nprint (\"e: \", e.shape)\nprint (\"attn_vals: \", attn_vals.shape)\nprint (\"attn_vals[0]: \", attn_vals[0])\nprint (\"sum(attn_vals[0]): \", sum(attn_vals[0]))\nprint (\"c: \", c.shape)\n</code></pre> <pre>\ne:  torch.Size([64, 8, 1])\nattn_vals:  torch.Size([64, 8])\nattn_vals[0]:  tensor([0.1131, 0.1161, 0.1438, 0.1181, 0.1244, 0.1234, 0.1351, 0.1261],\n       grad_fn=)\nsum(attn_vals[0]):  tensor(1.0000, grad_fn=)\nc:  torch.Size([64, 128])\n\n\n<pre><code># Predict\nfc1 = nn.Linear(RNN_HIDDEN_DIM, NUM_CLASSES)\noutput = F.softmax(fc1(c), dim=1)\nprint (\"output: \", output.shape)\n</code></pre>\n<pre>\noutput:  torch.Size([64, 4])\n</pre>\n\n<p>In a many-to-many task such as machine translation, our attentional interface will also account for the encoded representation of token in the output as well (via concatenation) so we can know which encoded inputs to attend to based on the encoded output we're focusing on. For more on this, be sure to explore Bahdanau's attention paper.</p>"},{"location":"courses/foundations/attention/#model","title":"Model","text":"<p>Now let's create our RNN based model but with the addition of the attention layer on top of the RNN's outputs.</p>\n<pre><code>RNN_HIDDEN_DIM = 128\nDROPOUT_P = 0.1\nHIDDEN_DIM = 100\n</code></pre>\n<pre><code>class RNN(nn.Module):\n    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n        super(RNN, self).__init__()\n\n        # Initialize embeddings\n        self.embeddings = nn.Embedding(\n            embedding_dim=embedding_dim, num_embeddings=vocab_size,\n            padding_idx=padding_idx)\n\n        # RNN\n        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n\n        # Attention\n        self.attn = nn.Linear(rnn_hidden_dim, 1)\n\n        # FC weights\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs):\n        # Embed\n        x_in, seq_lens = inputs\n        x_in = self.embeddings(x_in)\n\n        # Encode\n        out, h_n = self.rnn(x_in)\n\n        # Attend\n        e = self.attn(out)\n        attn_vals = F.softmax(e.squeeze(2), dim=1)\n        c = torch.bmm(attn_vals.unsqueeze(1), out).squeeze(1)\n\n        # Predict\n        z = self.fc1(c)\n        z = self.dropout(z)\n        z = self.fc2(z)\n\n        return z\n</code></pre>\n<pre><code># Simple RNN cell\nmodel = RNN(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre>\n<pre>\n&lt;bound method Module.named_parameters of RNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (rnn): RNN(100, 128, batch_first=True)\n  (attn): Linear(in_features=128, out_features=1, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=128, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/attention/#training","title":"Training","text":"<p><pre><code>from torch.optim import Adam\n</code></pre>\n<pre><code>NUM_LAYERS = 1\nLEARNING_RATE = 1e-4\nPATIENCE = 10\nNUM_EPOCHS = 50\n</code></pre>\n<pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre>\n<pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre>\n<pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre>\n<pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p>\n<pre>\nEpoch: 1 | train_loss: 1.21680, val_loss: 1.08622, lr: 1.00E-04, _patience: 10\nEpoch: 2 | train_loss: 1.00379, val_loss: 0.93546, lr: 1.00E-04, _patience: 10\nEpoch: 3 | train_loss: 0.87091, val_loss: 0.83399, lr: 1.00E-04, _patience: 10\n...\nEpoch: 48 | train_loss: 0.35045, val_loss: 0.54718, lr: 1.00E-08, _patience: 10\nEpoch: 49 | train_loss: 0.35055, val_loss: 0.54718, lr: 1.00E-08, _patience: 10\nEpoch: 50 | train_loss: 0.35086, val_loss: 0.54717, lr: 1.00E-08, _patience: 10\nStopping early!\n</pre>"},{"location":"courses/foundations/attention/#evaluation","title":"Evaluation","text":"<p><pre><code>import json\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre>\n<pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre>\n<pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre>\n<pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p>\n<pre>\n{\n  \"precision\": 0.8133385428975775,\n  \"recall\": 0.8137222222222222,\n  \"f1\": 0.8133454847232977,\n  \"num_samples\": 18000.0\n}\n</pre>"},{"location":"courses/foundations/attention/#inference","title":"Inference","text":"<pre><code>def get_probability_distribution(y_prob, classes):\n\"\"\"Create a dict of class probabilities from an array.\"\"\"\n    results = {}\n    for i, class_ in enumerate(classes):\n        results[class_] = np.float64(y_prob[i])\n    sorted_results = {k: v for k, v in sorted(\n        results.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_results\n</code></pre>\n<pre><code># Load artifacts\ndevice = torch.device(\"cpu\")\nlabel_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\ntokenizer = Tokenizer.load(fp=Path(dir, 'tokenizer.json'))\nmodel = GRU(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\nmodel.to(device)\n</code></pre>\n<pre>\nRNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (rnn): RNN(100, 128, batch_first=True)\n  (attn): Linear(in_features=128, out_features=1, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=128, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)\n</pre>\n\n<pre><code># Initialize trainer\ntrainer = Trainer(model=model, device=device)\n</code></pre>\n<pre><code># Dataloader\ntext = \"The final tennis tournament starts next week.\"\nX = tokenizer.texts_to_sequences([preprocess(text)])\nprint (tokenizer.sequences_to_texts(X))\ny_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\ndataset = Dataset(X=X, y=y_filler)\ndataloader = dataset.create_dataloader(batch_size=batch_size)\n</code></pre>\n<pre>\n['final tennis tournament starts next week']\n</pre>\n<pre><code># Inference\ny_prob = trainer.predict_step(dataloader)\ny_pred = np.argmax(y_prob, axis=1)\nlabel_encoder.decode(y_pred)\n</code></pre>\n<pre>\n['Sports']\n</pre>\n<pre><code># Class distributions\nprob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\nprint (json.dumps(prob_dist, indent=2))\n</code></pre>\n<pre>\n{\n  \"Sports\": 0.9651875495910645,\n  \"World\": 0.03468644618988037,\n  \"Sci/Tech\": 8.490968320984393e-05,\n  \"Business\": 4.112234091735445e-05\n}\n</pre>"},{"location":"courses/foundations/attention/#interpretability","title":"Interpretability","text":"<p>Let's use the attention values to see which encoded tokens were most useful in predicting the appropriate label.</p>\n<pre><code>import collections\nimport seaborn as sns\n</code></pre>\n<pre><code>class InterpretAttn(nn.Module):\n    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n        super(InterpretAttn, self).__init__()\n\n        # Initialize embeddings\n        self.embeddings = nn.Embedding(\n            embedding_dim=embedding_dim, num_embeddings=vocab_size,\n            padding_idx=padding_idx)\n\n        # RNN\n        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n\n        # Attention\n        self.attn = nn.Linear(rnn_hidden_dim, 1)\n\n        # FC weights\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs):\n        # Embed\n        x_in, seq_lens = inputs\n        x_in = self.embeddings(x_in)\n\n        # Encode\n        out, h_n = self.rnn(x_in)\n\n        # Attend\n        e = self.attn(out)  # could add optional activation function (ex. tanh)\n        attn_vals = F.softmax(e.squeeze(2), dim=1)\n\n        return attn_vals\n</code></pre>\n<pre><code># Initialize model\ninterpretable_model = InterpretAttn(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\ninterpretable_model.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\ninterpretable_model.to(device)\n</code></pre>\n<pre>\nInterpretAttn(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (rnn): RNN(100, 128, batch_first=True)\n  (attn): Linear(in_features=128, out_features=1, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=128, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)\n</pre>\n\n<pre><code># Initialize trainer\ninterpretable_trainer = Trainer(model=interpretable_model, device=device)\n</code></pre>\n<pre><code># Get attention values\nattn_vals  = interpretable_trainer.predict_step(dataloader)\nprint (attn_vals.shape) # (N, max_seq_len)\n</code></pre>\n<pre><code># Visualize a bi-gram filter's outputs\nsns.set(rc={\"figure.figsize\":(10, 1)})\ntokens = tokenizer.sequences_to_texts(X)[0].split(\" \")\nsns.heatmap(attn_vals, xticklabels=tokens)\n</code></pre>\n<p>The word <code>tennis</code> was attended to the most to result in the <code>Sports</code> label.</p>"},{"location":"courses/foundations/attention/#types-of-attention","title":"Types of attention","text":"<p>We'll briefly look at the different types of attention and when to use each them.</p>"},{"location":"courses/foundations/attention/#soft-global-attention","title":"Soft (global) attention","text":"<p>Soft attention the type of attention we've implemented so far, where we attend to all encoded inputs when creating our context vector.</p>\n<ul>\n<li>advantages: we always have the ability to attend to all inputs in case something we saw much earlier/ see later are crucial for determining the output.</li>\n<li>disadvantages: if our input sequence is very long, this can lead to expensive compute.</li>\n</ul>"},{"location":"courses/foundations/attention/#hard-attention","title":"Hard attention","text":"<p>Hard attention is focusing on a specific set of the encoded inputs at each time step.</p>\n<ul>\n<li>advantages: we can save a lot of compute on long sequences by only focusing on a local patch each time.</li>\n<li>disadvantages: non-differentiable and so we need to use more complex techniques (variance reduction, reinforcement learning, etc.) to train.</li>\n</ul>\nShow, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"location":"courses/foundations/attention/#local-attention","title":"Local attention","text":"<p>Local attention blends the advantages of soft and hard attention. It involves learning an aligned position vector and empirically determining a local window of encoded inputs to attend to.</p>\n<ul>\n<li>advantages: apply attention to a local patch of inputs yet remain differentiable.</li>\n<li>disadvantages: need to determine the alignment vector for each output but it's a worthwhile trade off to determine the right window of inputs to attend to in order to avoid attending to all of them.</li>\n</ul>\nEffective Approaches to Attention-based Neural Machine Translation"},{"location":"courses/foundations/attention/#self-attention","title":"Self-attention","text":"<p>We can also use attention within the encoded input sequence to create a weighted representation that based on the similarity between input pairs. This will allow us to create rich representations of the input sequence that are aware of the relationships between each other. For example, in the image below you can see that when composing the representation of the token \"its\", this specific attention head will be incorporating signal from the token \"Law\" (it's learned that \"its\" is referring to the \"Law\").</p>\nAttention Is All You Need\n<p>In the next lesson, we'll implement Transformers that leverage self-attention to create contextual representations of our inputs for downstream applications.</p>\n<p>To cite this content, please use:</p>\n<pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Attention - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/convolutional-neural-networks/","title":"Convolutional Neural Networks (CNN)","text":""},{"location":"courses/foundations/convolutional-neural-networks/#overview","title":"Overview","text":"<p>At the core of CNNs are filters (aka weights, kernels, etc.) which convolve (slide) across our input to extract relevant features. The filters are initialized randomly but learn to act as feature extractors via parameter sharing.</p> <ul> <li>Objective:<ul> <li>Extract meaningful spatial substructure from encoded data.</li> </ul> </li> <li>Advantages:<ul> <li>Small number of weights (shared)</li> <li>Parallelizable</li> <li>Detects spatial substrcutures (feature extractors)</li> <li>Interpretability via filters</li> <li>Can be used for processing in images, text, time-series, etc.</li> </ul> </li> <li>Disadvantages:<ul> <li>Many hyperparameters (kernel size, strides, etc.) to tune.</li> </ul> </li> <li>Miscellaneous:<ul> <li>Lot's of deep CNN architectures constantly updated for SOTA performance.</li> <li>Very popular feature extractor that acts as a foundation for many architectures.</li> </ul> </li> </ul>"},{"location":"courses/foundations/convolutional-neural-networks/#set-up","title":"Set up","text":"<p>Let's set our seed and device. <pre><code>import numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code>def set_seeds(seed=1234):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # multi-GPU\n</code></pre> <pre><code># Set seeds for reproducibility\nset_seeds(seed=SEED)\n</code></pre> <pre><code># Set device\ncuda = True\ndevice = torch.device(\"cuda\" if (\n    torch.cuda.is_available() and cuda) else \"cpu\")\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\nif device.type == \"cuda\":\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\nprint (device)\n</code></pre></p> <pre>\ncuda\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#load-data","title":"Load data","text":"<p>We will download the AG News dataset, which consists of 120K text samples from 4 unique classes (<code>Business</code>, <code>Sci/Tech</code>, <code>Sports</code>, <code>World</code>) <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/news.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> title category 0 Sharon Accepts Plan to Reduce Gaza Army Operation... World 1 Internet Key Battleground in Wildlife Crime Fight Sci/Tech 2 July Durable Good Orders Rise 1.7 Percent Business 3 Growing Signs of a Slowing on Wall Street Business 4 The New Faces of Reality TV World"},{"location":"courses/foundations/convolutional-neural-networks/#preprocessing","title":"Preprocessing","text":"<p>We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc. <pre><code>import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n</code></pre> <pre><code>nltk.download(\"stopwords\")\nSTOPWORDS = stopwords.words(\"english\")\nprint (STOPWORDS[:5])\nporter = PorterStemmer()\n</code></pre></p> <pre>\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n['i', 'me', 'my', 'myself', 'we']\n</pre> <p><pre><code>def preprocess(text, stopwords=STOPWORDS):\n\"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Remove stopwords\n    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n    text = pattern.sub(\"\", text)\n\n    # Remove words in parenthesis\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n\n    # Spacing and filters\n    text = re.sub(r\"([-;;.,!?&lt;=&gt;])\", r\" \\1 \", text)  # separate punctuation tied to words\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()\n\n    return text\n</code></pre> <pre><code># Sample\ntext = \"Great week for the NYSE!\"\npreprocess(text=text)\n</code></pre></p> <pre>\ngreat week nyse\n</pre> <pre><code># Apply to dataframe\npreprocessed_df = df.copy()\npreprocessed_df.title = preprocessed_df.title.apply(preprocess)\nprint (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")\n</code></pre> <pre>\nSharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n\nsharon accepts plan reduce gaza army operation haaretz says\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#split-data","title":"Split data","text":"<p><pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Data\nX = preprocessed_df[\"title\"].values\ny = preprocessed_df[\"category\"].values\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (84000,), y_train: (84000,)\nX_val: (18000,), y_val: (18000,)\nX_test: (18000,), y_test: (18000,)\nSample point: china battles north korea nuclear talks \u2192 World\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#label-encoding","title":"Label encoding","text":"<p>Next we'll define a <code>LabelEncoder</code> to encode our text labels into unique indices <pre><code>import itertools\n</code></pre> <pre><code>class LabelEncoder(object):\n\"\"\"Label encoder for tag labels.\"\"\"\n    def __init__(self, class_to_index={}):\n        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n\n    def __len__(self):\n        return len(self.class_to_index)\n\n    def __str__(self):\n        return f\"&lt;LabelEncoder(num_classes={len(self)})&gt;\"\n\n    def fit(self, y):\n        classes = np.unique(y)\n        for i, class_ in enumerate(classes):\n            self.class_to_index[class_] = i\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n        return self\n\n    def encode(self, y):\n        encoded = np.zeros((len(y)), dtype=int)\n        for i, item in enumerate(y):\n            encoded[i] = self.class_to_index[item]\n        return encoded\n\n    def decode(self, y):\n        classes = []\n        for i, item in enumerate(y):\n            classes.append(self.index_to_class[item])\n        return classes\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {'class_to_index': self.class_to_index}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Encode\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nNUM_CLASSES = len(label_encoder)\nlabel_encoder.class_to_index\n</code></pre></p> <pre>\n{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.encode(y_train)\ny_val = label_encoder.encode(y_val)\ny_test = label_encoder.encode(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\n</code></pre> <pre>\ny_train[0]: World\ny_train[0]: 3\n</pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [21000 21000 21000 21000]\nweights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#tokenizer","title":"Tokenizer","text":"<p>Our input data is text and we can't feed it directly to our models. So, we'll define a <code>Tokenizer</code> to convert our text input data into token indices. This means that every token (we can decide what a token is char, word, sub-word, etc.) is mapped to a unique index which allows us to represent our text as an array of indices.</p> <p><pre><code>import json\nfrom collections import Counter\nfrom more_itertools import take\n</code></pre> <pre><code>class Tokenizer(object):\n    def __init__(self, char_level, num_tokens=None,\n                 pad_token=\"&lt;PAD&gt;\", oov_token=\"&lt;UNK&gt;\",\n                 token_to_index=None):\n        self.char_level = char_level\n        self.separator = \"\" if self.char_level else \" \"\n        if num_tokens: num_tokens -= 2 # pad + unk tokens\n        self.num_tokens = num_tokens\n        self.pad_token = pad_token\n        self.oov_token = oov_token\n        if not token_to_index:\n            token_to_index = {pad_token: 0, oov_token: 1}\n        self.token_to_index = token_to_index\n        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n\n    def __len__(self):\n        return len(self.token_to_index)\n\n    def __str__(self):\n        return f\"&lt;Tokenizer(num_tokens={len(self)})&gt;\"\n\n    def fit_on_texts(self, texts):\n        if not self.char_level:\n            texts = [text.split(\" \") for text in texts]\n        all_tokens = [token for text in texts for token in text]\n        counts = Counter(all_tokens).most_common(self.num_tokens)\n        self.min_token_freq = counts[-1][1]\n        for token, count in counts:\n            index = len(self)\n            self.token_to_index[token] = index\n            self.index_to_token[index] = token\n        return self\n\n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            if not self.char_level:\n                text = text.split(\" \")\n            sequence = []\n            for token in text:\n                sequence.append(self.token_to_index.get(\n                    token, self.token_to_index[self.oov_token]))\n            sequences.append(np.asarray(sequence))\n        return sequences\n\n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = []\n            for index in sequence:\n                text.append(self.index_to_token.get(index, self.oov_token))\n            texts.append(self.separator.join([token for token in text]))\n        return texts\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {\n                \"char_level\": self.char_level,\n                \"oov_token\": self.oov_token,\n                \"token_to_index\": self.token_to_index\n            }\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> We're going to restrict the number of tokens in our <code>Tokenizer</code> to the top 500 most frequent tokens (stop words already removed) because the full vocabulary size (~35K) is too large to run on Google Colab notebooks.</p> <pre><code># Tokenize\ntokenizer = Tokenizer(char_level=False, num_tokens=500)\ntokenizer.fit_on_texts(texts=X_train)\nVOCAB_SIZE = len(tokenizer)\nprint (tokenizer)\n</code></pre> <pre>\nTokenizer(num_tokens=500)\n</pre> <pre><code># Sample of tokens\nprint (take(5, tokenizer.token_to_index.items()))\nprint (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens\n</code></pre> <pre>\n[('&lt;PAD&gt;', 0), ('&lt;UNK&gt;', 1), ('39', 2), ('b', 3), ('gt', 4)]\nleast freq token's freq: 166\n</pre> <pre><code># Convert texts to sequences of indices\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(X_test)\npreprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\nprint (\"Text to indices:\\n\"\n    f\"  (preprocessed) \u2192 {preprocessed_text}\\n\"\n    f\"  (tokenized) \u2192 {X_train[0]}\")\n</code></pre> <pre>\nText to indices:\n  (preprocessed) \u2192 china &lt;UNK&gt; north korea nuclear talks\n  (tokenized) \u2192 [ 16   1 285 142 114  24]\n</pre> <p>Did we need to split the data first?</p> <p>How come we applied the preprocessing functions to the entire dataset but tokenization after splitting the dataset? Does it matter?</p> Show answer <p>If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before applying those operations. This is because we cannot apply any knowledge gained from the test set accidentally (data leak) during preprocessing/training. So for the tokenization process, it's important that we only fit using our train data split because during inference, our model will not always know every token so it's important to replicate that scenario with our validation and test splits as well. However for global preprocessing steps, like the preprocessing function where we aren't learning anything from the data itself, we can perform before splitting the data.</p>"},{"location":"courses/foundations/convolutional-neural-networks/#one-hot-encoding","title":"One-hot encoding","text":"<p>One-hot encoding creates a binary column for each unique value for the feature we're trying to map.  All of the values in each token's array will be 0 except at the index that this specific token is represented by.</p> <p>There are 5 words in the vocabulary: <pre><code>{\n\"a\": 0,\n\"e\": 1,\n\"i\": 2,\n\"o\": 3,\n\"u\": 4\n}\n</code></pre></p> <p>Then the text <code>aou</code> would be represented by: <pre><code>[[1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n</code></pre></p> <p>One-hot encoding allows us to represent our data in a way that our models can process the data and isn't biased by the actual value of the token (ex. if your labels were actual numbers).</p> <p>We have already applied one-hot encoding in the previous lessons when we encoded our labels. Each label was represented by a unique index but when determining loss, we effectively use it's one hot representation and compared it to the predicted probability distribution. We never explicitly wrote this out since all of our previous tasks were multi-class which means every input had just one output class, so the 0s didn't affect the loss (though it did matter during back propagation).</p> <p><pre><code>def to_categorical(seq, num_classes):\n\"\"\"One-hot encode a sequence of tokens.\"\"\"\n    one_hot = np.zeros((len(seq), num_classes))\n    for i, item in enumerate(seq):\n        one_hot[i, item] = 1.\n    return one_hot\n</code></pre> <pre><code># One-hot encoding\nprint (X_train[0])\nprint (len(X_train[0]))\ncat = to_categorical(seq=X_train[0], num_classes=len(tokenizer))\nprint (cat)\nprint (cat.shape)\n</code></pre></p> <pre>\n[ 16   1 285 142 114  24]\n6\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n(6, 500)\n</pre> <pre><code># Convert tokens to one-hot\nvocab_size = len(tokenizer)\nX_train = [to_categorical(seq, num_classes=vocab_size) for seq in X_train]\nX_val = [to_categorical(seq, num_classes=vocab_size) for seq in X_val]\nX_test = [to_categorical(seq, num_classes=vocab_size) for seq in X_test]\n</code></pre>"},{"location":"courses/foundations/convolutional-neural-networks/#padding","title":"Padding","text":"<p>Our inputs are all of varying length but we need each batch to be uniformly shaped. Therefore, we will use padding to make all the inputs in the batch the same length. Our padding index will be 0 (note that this is consistent with the <code>&lt;PAD&gt;</code> token defined in our <code>Tokenizer</code>).</p> <p>One-hot encoding creates a batch of shape (<code>N</code>, <code>max_seq_len</code>, <code>vocab_size</code>) so we'll need to be able to pad 3D sequences.</p> <p><pre><code>def pad_sequences(sequences, max_seq_len=0):\n\"\"\"Pad sequences to max length in sequence.\"\"\"\n    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n    num_classes = sequences[0].shape[-1]\n    padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))\n    for i, sequence in enumerate(sequences):\n        padded_sequences[i][:len(sequence)] = sequence\n    return padded_sequences\n</code></pre> <pre><code># 3D sequences\nprint (X_train[0].shape, X_train[1].shape, X_train[2].shape)\npadded = pad_sequences(X_train[0:3])\nprint (padded.shape)\n</code></pre></p> <pre>\n(6, 500) (5, 500) (6, 500)\n(3, 6, 500)\n</pre> <p>Is our <code>pad_sequences</code> function properly created?</p> <p>Notice any assumptions that could lead to hidden bugs?</p> Show answer <p>By using <code>np.zeros()</code> to create our padded sequences, we're assuming that our pad token's index is 0. While this is the case for our project, someone could choose to use a different index and this can cause an error. Worst of all, this would be a silent error because all downstream operations would still run normally but our performance will suffer and it may not always be intuitive that this was the cause of issue!</p>"},{"location":"courses/foundations/convolutional-neural-networks/#dataset","title":"Dataset","text":"<p>We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits.</p> <p><pre><code>FILTER_SIZE = 1 # unigram\n</code></pre> <pre><code>class Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y, max_filter_size):\n        self.X = X\n        self.y = y\n        self.max_filter_size = max_filter_size\n\n    def __len__(self):\n        return len(self.y)\n\n    def __str__(self):\n        return f\"&lt;Dataset(N={len(self)})&gt;\"\n\n    def __getitem__(self, index):\n        X = self.X[index]\n        y = self.y[index]\n        return [X, y]\n\n    def collate_fn(self, batch):\n\"\"\"Processing on a batch.\"\"\"\n        # Get inputs\n        batch = np.array(batch)\n        X = batch[:, 0]\n        y = batch[:, 1]\n\n        # Pad sequences\n        X = pad_sequences(X, max_seq_len=self.max_filter_size)\n\n        # Cast\n        X = torch.FloatTensor(X.astype(np.int32))\n        y = torch.LongTensor(y.astype(np.int32))\n\n        return X, y\n\n    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n        return torch.utils.data.DataLoader(\n            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n            shuffle=shuffle, drop_last=drop_last, pin_memory=True)\n</code></pre> <pre><code># Create datasets for embedding\ntrain_dataset = Dataset(X=X_train, y=y_train, max_filter_size=FILTER_SIZE)\nval_dataset = Dataset(X=X_val, y=y_val, max_filter_size=FILTER_SIZE)\ntest_dataset = Dataset(X=X_test, y=y_test, max_filter_size=FILTER_SIZE)\nprint (\"Datasets:\\n\"\n    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {test_dataset[0][0]}\\n\"\n    f\"  y: {test_dataset[0][1]}\")\n</code></pre></p> <pre>\nDatasets:\n  Train dataset: &lt;Dataset(N=84000)&gt;\n  Val dataset: &lt;Dataset(N=18000)&gt;\n  Test dataset: &lt;Dataset(N=18000)&gt;\nSample point:\n  X: [[0. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]]\n  y: 1\n</pre> <pre><code># Create dataloaders\nbatch_size = 64\ntrain_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\nval_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\ntest_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\nbatch_X, batch_y = next(iter(test_dataloader))\nprint (\"Sample batch:\\n\"\n    f\"  X: {list(batch_X.size())}\\n\"\n    f\"  y: {list(batch_y.size())}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {batch_X[0]}\\n\"\n    f\"  y: {batch_y[0]}\")\n</code></pre> <pre>\nSample batch:\n  X: [64, 14, 500]\n  y: [64]\nSample point:\n  X: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], device=\"cpu\")\n  y: 1\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#cnn","title":"CNN","text":"<p>We're going to learn about CNNs by applying them on 1D text data.</p>"},{"location":"courses/foundations/convolutional-neural-networks/#inputs","title":"Inputs","text":"<p>In the dummy example below, our inputs are composed of character tokens that are one-hot encoded. We have a batch of N samples, where each sample has 8 characters and each character is represented by an array of 10 values (<code>vocab size=10</code>). This gives our inputs the size <code>(N, 8, 10)</code>.</p> <p>With PyTorch, when dealing with convolution, our inputs (X) need to have the channels as the second dimension, so our inputs will be <code>(N, 10, 8)</code>.</p> <p><pre><code>import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n</code></pre> <pre><code># Assume all our inputs are padded to have the same # of words\nbatch_size = 64\nmax_seq_len = 8 # words per input\nvocab_size = 10 # one hot size\nx = torch.randn(batch_size, max_seq_len, vocab_size)\nprint(f\"X: {x.shape}\")\nx = x.transpose(1, 2)\nprint(f\"X: {x.shape}\")\n</code></pre></p> <pre>\nX: torch.Size([64, 8, 10])\nX: torch.Size([64, 10, 8])\n</pre> This diagram above is for char-level tokens but extends to any level of tokenization."},{"location":"courses/foundations/convolutional-neural-networks/#filters","title":"Filters","text":"<p>At the core of CNNs are filters (aka weights, kernels, etc.) which convolve (slide) across our input to extract relevant features. The filters are initialized randomly but learn to act as feature extractors via parameter sharing.</p> <p>We can see convolution in the diagram below where we simplified the filters and inputs to be 2D for ease of visualization. Also note that the values are 0/1s but in reality they can be any floating point value.</p> <p>Now let's return to our actual inputs <code>x</code>, which is of shape (8, 10) [<code>max_seq_len</code>, <code>vocab_size</code>] and we want to convolve on this input using filters. We will use 50 filters that are of size (1, 3) and has the same depth as the number of channels (<code>num_channels</code> = <code>vocab_size</code> = <code>one_hot_size</code> = 10). This gives our filter a shape of (3, 10, 50) [<code>kernel_size</code>, <code>vocab_size</code>, <code>num_filters</code>]</p> <ul> <li><code>stride</code>: amount the filters move from one convolution operation to the next.</li> <li><code>padding</code>: values (typically zero) padded to the input, typically to create a volume with whole number dimensions.</li> </ul> <p>So far we've used a <code>stride</code> of 1 and <code>VALID</code> padding (no padding) but let's look at an example with a higher stride and difference between different padding approaches.</p> <p>Padding types:</p> <ul> <li><code>VALID</code>: no padding, the filters only use the \"valid\" values in the input. If the filter cannot reach all the input values (filters go left to right), the extra values on the right are dropped.</li> <li><code>SAME</code>: adds padding evenly to the right (preferred) and left sides of the input so that all values in the input are processed.</li> </ul> <p>We're going to use the Conv1d layer to process our inputs.</p> <pre><code># Convolutional filters (VALID padding)\nvocab_size = 10 # one hot size\nnum_filters = 50 # num filters\nfilter_size = 3 # filters are 3X3\nstride = 1\npadding = 0 # valid padding (no padding)\nconv1 = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,\n                  kernel_size=filter_size, stride=stride,\n                  padding=padding, padding_mode=\"zeros\")\nprint(\"conv: {}\".format(conv1.weight.shape))\n</code></pre> <pre>\nconv: torch.Size([50, 10, 3])\n</pre> <pre><code># Forward pass\nz = conv1(x)\nprint (f\"z: {z.shape}\")\n</code></pre> <pre>\nz: torch.Size([64, 50, 6])\n</pre> <p>When we apply these filter on our inputs, we receive an output of shape (N, 6, 50). We get 50 for the output channel dim because we used 50 filters and 6 for the conv outputs because:</p> \\[ W_1 = \\frac{W_2 - F + 2P}{S} + 1 = \\frac{8 - 3 + 2(0)}{1} + 1 = 6 \\] \\[ H_1 = \\frac{H_2 - F + 2P}{S} + 1 = \\frac{1 - 1 + 2(0)}{1} + 1 = 1 \\] \\[ D_2 = D_1 \\] <p> Variable Description \\(W\\) width of each input = 8 \\(H\\) height of each input = 1 \\(D\\) depth (# of channels) \\(F\\) filter size = 3 \\(P\\) padding = 0 \\(S\\) stride = 1 <p></p> <p>Now we'll add padding so that the convolutional outputs are the same shape as our inputs. The amount of padding for the <code>SAME</code> padding can be determined using the same equation. We want out output to have the same width as our input, so we solve for P:</p> \\[ \\frac{W-F+2P}{S} + 1 = W \\] \\[ P = \\frac{S(W-1) - W + F}{2} \\] <p>If \\(P\\) is not a whole number, we round up (using <code>math.ceil</code>) and place the extra padding on the right side.</p> <pre><code># Convolutional filters (SAME padding)\nvocab_size = 10 # one hot size\nnum_filters = 50 # num filters\nfilter_size = 3 # filters are 3X3\nstride = 1\nconv = nn.Conv1d(in_channels=vocab_size, out_channels=num_filters,\n                 kernel_size=filter_size, stride=stride)\nprint(\"conv: {}\".format(conv.weight.shape))\n</code></pre> <pre>\nconv: torch.Size([50, 10, 3])\n</pre> <pre><code># `SAME` padding\npadding_left = int((conv.stride[0]*(max_seq_len-1) - max_seq_len + filter_size)/2)\npadding_right = int(math.ceil((conv.stride[0]*(max_seq_len-1) - max_seq_len + filter_size)/2))\nprint (f\"padding: {(padding_left, padding_right)}\")\n</code></pre> <pre>\npadding: (1, 1)\n</pre> <pre><code># Forward pass\nz = conv(F.pad(x, (padding_left, padding_right)))\nprint (f\"z: {z.shape}\")\n</code></pre> <pre>\nz: torch.Size([64, 50, 8])\n</pre> <p>We will explore larger dimensional convolution layers in subsequent lessons. For example, Conv2D is used with 3D inputs (images, char-level text, etc.) and Conv3D is used for 4D inputs (videos, time-series, etc.).</p>"},{"location":"courses/foundations/convolutional-neural-networks/#pooling","title":"Pooling","text":"<p>The result of convolving filters on an input is a feature map. Due to the nature of convolution and overlaps, our feature map will have lots of redundant information. Pooling is a way to summarize a high-dimensional feature map into a lower dimensional one for simplified downstream computation. The pooling operation can be the max value, average, etc. in a certain receptive field. Below is an example of pooling where the outputs from a conv layer are <code>4X4</code> and we're going to apply max pool filters of size <code>2X2</code>.</p> \\[ W_2 = \\frac{W_1 - F}{S} + 1 = \\frac{4 - 2}{2} + 1 = 2 \\] \\[ H_2 = \\frac{H_1 - F}{S} + 1 = \\frac{4 - 2}{2} + 1 = 2 \\] \\[ D_2 = D_1 \\] <p> Variable Description \\(W\\) width of each input = 4 \\(H\\) height of each input = 4 \\(D\\) depth (# of channels) \\(F\\) filter size = 2 \\(S\\) stride = 2 <p></p> <p>In our use case, we want to just take the one max value so we will use the MaxPool1D layer, so our max-pool filter size will be max_seq_len. <pre><code># Max pooling\npool_output = F.max_pool1d(z, z.size(2))\nprint(\"Size: {}\".format(pool_output.shape))\n</code></pre></p> <pre>\nSize: torch.Size([64, 50, 1])\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#batch-normalization","title":"Batch normalization","text":"<p>The last topic we'll cover before constructing our model is batch normalization. It's an operation that will standardize (mean=0, std=1) the activations from the previous layer. Recall that we used to standardize our inputs in previous notebooks so our model can optimize quickly with larger learning rates. It's the same concept here but we continue to maintain standardized values throughout the repeated forward passes to further aid optimization.</p> <pre><code># Batch normalization\nbatch_norm = nn.BatchNorm1d(num_features=num_filters)\nz = batch_norm(conv(x)) # applied to activations (after conv layer &amp; before pooling)\nprint (f\"z: {z.shape}\")\n</code></pre> <pre>\nz: torch.Size([64, 50, 6])\n</pre> <pre><code># Mean and std before batchnorm\nprint (f\"mean: {torch.mean(conv(x)):.2f}, std: {torch.std(conv(x)):.2f}\")\n</code></pre> <pre>\nmean: -0.00, std: 0.57\n</pre> <pre><code># Mean and std after batchnorm\nprint (f\"mean: {torch.mean(z):.2f}, std: {torch.std(z):.2f}\")\n</code></pre> <pre>\nmean: 0.00, std: 1.00\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#modeling","title":"Modeling","text":""},{"location":"courses/foundations/convolutional-neural-networks/#model","title":"Model","text":"<p>Let's visualize the model's forward pass.</p> <ol> <li>We'll first tokenize our inputs (<code>batch_size</code>, <code>max_seq_len</code>).</li> <li>Then we'll one-hot encode our tokenized inputs (<code>batch_size</code>, <code>max_seq_len</code>, <code>vocab_size</code>).</li> <li>We'll apply convolution via filters (<code>filter_size</code>, <code>vocab_size</code>, <code>num_filters</code>) followed by batch normalization. Our filters act as character level n-gram detectors.</li> <li>We'll apply 1D global max pooling which will extract the most relevant information from the feature maps for making the decision.</li> <li>We feed the pool outputs to a fully-connected (FC) layer (with dropout).</li> <li>We use one more FC layer with softmax to derive class probabilities.</li> </ol> <p><pre><code>NUM_FILTERS = 50\nHIDDEN_DIM = 100\nDROPOUT_P = 0.1\n</code></pre> <pre><code>class CNN(nn.Module):\n    def __init__(self, vocab_size, num_filters, filter_size,\n                 hidden_dim, dropout_p, num_classes):\n        super(CNN, self).__init__()\n\n        # Convolutional filters\n        self.filter_size = filter_size\n        self.conv = nn.Conv1d(\n            in_channels=vocab_size, out_channels=num_filters,\n            kernel_size=filter_size, stride=1, padding=0, padding_mode=\"zeros\")\n        self.batch_norm = nn.BatchNorm1d(num_features=num_filters)\n\n        # FC layers\n        self.fc1 = nn.Linear(num_filters, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs, channel_first=False,):\n\n        # Rearrange input so num_channels is in dim 1 (N, C, L)\n        x_in, = inputs\n        if not channel_first:\n            x_in = x_in.transpose(1, 2)\n\n        # Padding for `SAME` padding\n        max_seq_len = x_in.shape[2]\n        padding_left = int((self.conv.stride[0]*(max_seq_len-1) - max_seq_len + self.filter_size)/2)\n        padding_right = int(math.ceil((self.conv.stride[0]*(max_seq_len-1) - max_seq_len + self.filter_size)/2))\n\n        # Conv outputs\n        z = self.conv(F.pad(x_in, (padding_left, padding_right)))\n        z = F.max_pool1d(z, z.size(2)).squeeze(2)\n\n        # FC layer\n        z = self.fc1(z)\n        z = self.dropout(z)\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Initialize model\nmodel = CNN(vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,\n            hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of CNN(\n  (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))\n  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc1): Linear(in_features=50, out_features=100, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)&gt;\n</pre> <p>We used <code>SAME</code> padding (w/ stride=1) which means that the conv outputs will have the same width (<code>max_seq_len</code>) as our inputs. The amount of padding differs for each batch based on the <code>max_seq_len</code> but you can calculate it by solving for P in the equation below.</p> \\[ \\frac{W_1 - F + 2P}{S} + 1 = W_2 \\] \\[ \\frac{\\text{max_seq_len } - \\text{ filter_size } + 2P}{\\text{stride}} + 1 = \\text{max_seq_len} \\] \\[ P = \\frac{\\text{stride}(\\text{max_seq_len}-1) - \\text{max_seq_len} + \\text{filter_size}}{2} \\] <p>If \\(P\\) is not a whole number, we round up (using <code>math.ceil</code>) and place the extra padding on the right side.</p>"},{"location":"courses/foundations/convolutional-neural-networks/#training","title":"Training","text":"<p>Let's create the <code>Trainer</code> class that we'll use to facilitate training for our experiments. Notice that we're now moving the <code>train</code> function inside this class.</p> <p><pre><code>from torch.optim import Adam\n</code></pre> <pre><code>LEARNING_RATE = 1e-3\nPATIENCE = 5\nNUM_EPOCHS = 10\n</code></pre> <pre><code>class Trainer(object):\n    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n\n        # Set params\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train_step(self, dataloader):\n\"\"\"Train step.\"\"\"\n        # Set model to train mode\n        self.model.train()\n        loss = 0.0\n\n        # Iterate over train batches\n        for i, batch in enumerate(dataloader):\n\n            # Step\n            batch = [item.to(self.device) for item in batch]  # Set device\n            inputs, targets = batch[:-1], batch[-1]\n            self.optimizer.zero_grad()  # Reset gradients\n            z = self.model(inputs)  # Forward pass\n            J = self.loss_fn(z, targets)  # Define loss\n            J.backward()  # Backward pass\n            self.optimizer.step()  # Update weights\n\n            # Cumulative Metrics\n            loss += (J.detach().item() - loss) / (i + 1)\n\n        return loss\n\n    def eval_step(self, dataloader):\n\"\"\"Validation or test step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        loss = 0.0\n        y_trues, y_probs = [], []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Step\n                batch = [item.to(self.device) for item in batch]  # Set device\n                inputs, y_true = batch[:-1], batch[-1]\n                z = self.model(inputs)  # Forward pass\n                J = self.loss_fn(z, y_true).item()\n\n                # Cumulative Metrics\n                loss += (J - loss) / (i + 1)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n                y_trues.extend(y_true.cpu().numpy())\n\n        return loss, np.vstack(y_trues), np.vstack(y_probs)\n\n    def predict_step(self, dataloader):\n\"\"\"Prediction step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        y_probs = []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Forward pass w/ inputs\n                inputs, targets = batch[:-1], batch[-1]\n                z = self.model(inputs)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n\n        return np.vstack(y_probs)\n\n    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n        best_val_loss = np.inf\n        for epoch in range(num_epochs):\n            # Steps\n            train_loss = self.train_step(dataloader=train_dataloader)\n            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n            self.scheduler.step(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = self.model\n                _patience = patience  # reset _patience\n            else:\n                _patience -= 1\n            if not _patience:  # 0\n                print(\"Stopping early!\")\n                break\n\n            # Logging\n            print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.5f}, \"\n                f\"val_loss: {val_loss:.5f}, \"\n                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n                f\"_patience: {_patience}\"\n            )\n        return best_model\n</code></pre> <pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p> <pre>\nEpoch: 1 | train_loss: 0.87388, val_loss: 0.79013, lr: 1.00E-03, _patience: 3\nEpoch: 2 | train_loss: 0.78354, val_loss: 0.78657, lr: 1.00E-03, _patience: 3\nEpoch: 3 | train_loss: 0.77743, val_loss: 0.78433, lr: 1.00E-03, _patience: 3\nEpoch: 4 | train_loss: 0.77242, val_loss: 0.78260, lr: 1.00E-03, _patience: 3\nEpoch: 5 | train_loss: 0.76900, val_loss: 0.78169, lr: 1.00E-03, _patience: 3\nEpoch: 6 | train_loss: 0.76613, val_loss: 0.78064, lr: 1.00E-03, _patience: 3\nEpoch: 7 | train_loss: 0.76413, val_loss: 0.78019, lr: 1.00E-03, _patience: 3\nEpoch: 8 | train_loss: 0.76215, val_loss: 0.78016, lr: 1.00E-03, _patience: 3\nEpoch: 9 | train_loss: 0.76034, val_loss: 0.77974, lr: 1.00E-03, _patience: 3\nEpoch: 10 | train_loss: 0.75859, val_loss: 0.77978, lr: 1.00E-03, _patience: 2\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#evaluation","title":"Evaluation","text":"<p><pre><code>import json\nfrom pathlib import Path\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre> <pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.7120047175492572,\n  \"recall\": 0.6935,\n  \"f1\": 0.6931471439737603,\n  \"num_samples\": 18000.0\n}\n</pre> <pre><code># Save artifacts\ndir = Path(\"cnn\")\ndir.mkdir(parents=True, exist_ok=True)\nlabel_encoder.save(fp=Path(dir, \"label_encoder.json\"))\ntokenizer.save(fp=Path(dir, 'tokenizer.json'))\ntorch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\nwith open(Path(dir, 'performance.json'), \"w\") as fp:\n    json.dump(performance, indent=2, sort_keys=False, fp=fp)\n</code></pre>"},{"location":"courses/foundations/convolutional-neural-networks/#inference","title":"Inference","text":"<p><pre><code>def get_probability_distribution(y_prob, classes):\n\"\"\"Create a dict of class probabilities from an array.\"\"\"\n    results = {}\n    for i, class_ in enumerate(classes):\n        results[class_] = np.float64(y_prob[i])\n    sorted_results = {k: v for k, v in sorted(\n        results.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_results\n</code></pre> <pre><code># Load artifacts\ndevice = torch.device(\"cpu\")\nlabel_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\ntokenizer = Tokenizer.load(fp=Path(dir, 'tokenizer.json'))\nmodel = CNN(\n    vocab_size=VOCAB_SIZE, num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,\n    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\nmodel.to(device)\n</code></pre></p> <pre>\nCNN(\n  (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))\n  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc1): Linear(in_features=50, out_features=100, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)\n</pre> <p><pre><code># Initialize trainer\ntrainer = Trainer(model=model, device=device)\n</code></pre> <pre><code># Dataloader\ntext = \"What a day for the new york stock market to go bust!\"\nsequences = tokenizer.texts_to_sequences([preprocess(text)])\nprint (tokenizer.sequences_to_texts(sequences))\nX = [to_categorical(seq, num_classes=len(tokenizer)) for seq in sequences]\ny_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\ndataset = Dataset(X=X, y=y_filler, max_filter_size=FILTER_SIZE)\ndataloader = dataset.create_dataloader(batch_size=batch_size)\n</code></pre></p> <pre>\n['day new &lt;UNK&gt; stock market go &lt;UNK&gt;']\n</pre> <pre><code># Inference\ny_prob = trainer.predict_step(dataloader)\ny_pred = np.argmax(y_prob, axis=1)\nlabel_encoder.decode(y_pred)\n</code></pre> <pre>\n['Business']\n</pre> <pre><code># Class distributions\nprob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\nprint (json.dumps(prob_dist, indent=2))\n</code></pre> <pre>\n{\n  \"Business\": 0.8670833110809326,\n  \"Sci/Tech\": 0.10699427127838135,\n  \"World\": 0.021050667390227318,\n  \"Sports\": 0.004871787969022989\n}\n</pre>"},{"location":"courses/foundations/convolutional-neural-networks/#interpretability","title":"Interpretability","text":"<p>We went through all the trouble of padding our inputs before convolution to result in outputs of the same shape as our inputs so we can try to get some interpretability. Since every token is mapped to a convolutional output on which we apply max pooling, we can see which token's output was most influential towards the prediction. We first need to get the conv outputs from our model:</p> <p><pre><code>import collections\nimport seaborn as sns\n</code></pre> <pre><code>class InterpretableCNN(nn.Module):\n    def __init__(self, vocab_size, num_filters, filter_size,\n                 hidden_dim, dropout_p, num_classes):\n        super(InterpretableCNN, self).__init__()\n\n        # Convolutional filters\n        self.filter_size = filter_size\n        self.conv = nn.Conv1d(\n            in_channels=vocab_size, out_channels=num_filters,\n            kernel_size=filter_size, stride=1, padding=0, padding_mode=\"zeros\")\n        self.batch_norm = nn.BatchNorm1d(num_features=num_filters)\n\n        # FC layers\n        self.fc1 = nn.Linear(num_filters, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs, channel_first=False):\n\n        # Rearrange input so num_channels is in dim 1 (N, C, L)\n        x_in, = inputs\n        if not channel_first:\n            x_in = x_in.transpose(1, 2)\n\n        # Padding for `SAME` padding\n        max_seq_len = x_in.shape[2]\n        padding_left = int((self.conv.stride[0]*(max_seq_len-1) - max_seq_len + self.filter_size)/2)\n        padding_right = int(math.ceil((self.conv.stride[0]*(max_seq_len-1) - max_seq_len + self.filter_size)/2))\n\n        # Conv outputs\n        z = self.conv(F.pad(x_in, (padding_left, padding_right)))\n        return z\n</code></pre> <pre><code># Initialize\ninterpretable_model = InterpretableCNN(\n    vocab_size=len(tokenizer), num_filters=NUM_FILTERS, filter_size=FILTER_SIZE,\n    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n</code></pre> <pre><code># Load weights (same architecture)\ninterpretable_model.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\ninterpretable_model.to(device)\n</code></pre></p> <pre>\nInterpretableCNN(\n  (conv): Conv1d(500, 50, kernel_size=(1,), stride=(1,))\n  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc1): Linear(in_features=50, out_features=100, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)\n</pre> <p><pre><code># Initialize trainer\ninterpretable_trainer = Trainer(model=interpretable_model, device=device)\n</code></pre> <pre><code># Get conv outputs\nconv_outputs = interpretable_trainer.predict_step(dataloader)\nprint (conv_outputs.shape) # (num_filters, max_seq_len)\n</code></pre></p> <pre>\n(50, 7)\n</pre> <pre><code># Visualize a bi-gram filter's outputs\ntokens = tokenizer.sequences_to_texts(sequences)[0].split(\" \")\nsns.heatmap(conv_outputs, xticklabels=tokens)\n</code></pre> <p>The filters have high values for the words <code>stock</code> and <code>market</code> which influenced the <code>Business</code> category classification.</p> <p>Warning</p> <p>This is a crude technique loosely based off of more elaborate interpretability methods.</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { CNNs - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/data-quality/","title":"Data Quality for Machine Learning","text":""},{"location":"courses/foundations/data-quality/#overview","title":"Overview","text":"<p>In a nutshell, a machine learning model consumes input data and produces predictions. The quality of the predictions directly corresponds to the quality of data you train the model with; garbage in, garbage out. Check out this article on where it makes sense to use AI and how to properly apply it.</p> <p>We're going to go through all the concepts with concrete code examples and some synthesized data to train our models on. The task is to determine whether a tumor will be benign (harmless) or malignant (harmful) based on leukocyte (white blood cells) count and blood pressure. This is a synthetic dataset that we created and has no clinical relevance.</p>"},{"location":"courses/foundations/data-quality/#set-up","title":"Set up","text":"<p>We'll set our seeds for reproducibility. <pre><code>import numpy as np\nimport random\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(SEED)\nrandom.seed(SEED)\n</code></pre></p>"},{"location":"courses/foundations/data-quality/#full-dataset","title":"Full dataset","text":"<p>We'll first train a model with the entire dataset. Later we'll remove a subset of the dataset and see the effect it has on our model.</p>"},{"location":"courses/foundations/data-quality/#load-data","title":"Load data","text":"<p><pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\n</code></pre> <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tumors.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> leukocyte_count blood_pressure tumor_class 0 15.335860 14.637535 benign 1 9.857535 14.518942 malignant 2 17.632579 15.869585 benign 3 18.369174 14.774547 benign 4 14.509367 15.892224 malignant <pre><code># Define X and y\nX = df[[\"leukocyte_count\", \"blood_pressure\"]].values\ny = df[\"tumor_class\"].values\nprint (\"X: \", np.shape(X))\nprint (\"y: \", np.shape(y))\n</code></pre> <pre>\nX:  (1000, 2)\ny:  (1000,)\n</pre> <pre><code># Plot data\ncolors = {\"benign\": \"red\", \"malignant\": \"blue\"}\nplt.scatter(X[:, 0], X[:, 1], c=[colors[_y] for _y in y], s=25, edgecolors=\"k\")\nplt.xlabel(\"leukocyte count\")\nplt.ylabel(\"blood pressure\")\nplt.legend([\"malignant\", \"benign\"], loc=\"upper right\")\nplt.show()\n</code></pre> <p>We want to choose features that have strong predictive signal for our task. If you want to improve performance, you need to continuously do feature engineering by collecting and adding new signals. So you may run into a new feature that has high correlation (orthogonal signal) with your existing features but it may still possess some unique signal to boost your predictive performance. <pre><code># Correlation matrix\nscatter_matrix(df, figsize=(5, 5));\ndf.corr()\n</code></pre></p> leukocyte_count blood_pressure leukocyte_count 1.000000 -0.162875 blood_pressure -0.162875 1.000000"},{"location":"courses/foundations/data-quality/#split-data","title":"Split data","text":"<p><pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.70\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (700, 2), y_train: (700,)\nX_val: (150, 2), y_val: (150,)\nX_test: (150, 2), y_test: (150,)\nSample point: [11.5066204  15.98030799] \u2192 malignant\n</pre>"},{"location":"courses/foundations/data-quality/#label-encoding","title":"Label encoding","text":"<p><pre><code>from sklearn.preprocessing import LabelEncoder\n</code></pre> <pre><code># Output vectorizer\nlabel_encoder = LabelEncoder()\n</code></pre> <pre><code># Fit on train data\nlabel_encoder = label_encoder.fit(y_train)\nclasses = list(label_encoder.classes_)\nprint (f\"classes: {classes}\")\n</code></pre></p> <pre>\nclasses: [\"benign\", \"malignant\"]\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.transform(y_train)\ny_val = label_encoder.transform(y_val)\ny_test = label_encoder.transform(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\n</code></pre> <pre>\ny_train[0]: malignant\ny_train[0]: 1\n</pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [272 428]\nweights: {0: 0.003676470588235294, 1: 0.002336448598130841}\n</pre>"},{"location":"courses/foundations/data-quality/#standardize-data","title":"Standardize data","text":"<p><pre><code>from sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code># Standardize the data (mean=0, std=1) using training data\nX_scaler = StandardScaler().fit(X_train)\n</code></pre> <pre><code># Apply scaler on training and test data (don't standardize outputs for classification)\nX_train = X_scaler.transform(X_train)\nX_val = X_scaler.transform(X_val)\nX_test = X_scaler.transform(X_test)\n</code></pre> <pre><code># Check (means should be ~0 and std should be ~1)\nprint (f\"X_test[0]: mean: {np.mean(X_test[:, 0], axis=0):.1f}, std: {np.std(X_test[:, 0], axis=0):.1f}\")\nprint (f\"X_test[1]: mean: {np.mean(X_test[:, 1], axis=0):.1f}, std: {np.std(X_test[:, 1], axis=0):.1f}\")\n</code></pre></p> <pre>\nX_test[0]: mean: 0.0, std: 1.0\nX_test[1]: mean: 0.0, std: 1.0\n</pre>"},{"location":"courses/foundations/data-quality/#model","title":"Model","text":"<p><pre><code>import torch\nfrom torch import nn\nimport torch.nn.functional as F\n</code></pre> <pre><code># Set seed for reproducibility\ntorch.manual_seed(SEED)\n</code></pre> <pre><code>INPUT_DIM = 2 # X is 2-dimensional\nHIDDEN_DIM = 100\nNUM_CLASSES = 2\n</code></pre> <pre><code>class MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x_in):\n        z = F.relu(self.fc1(x_in)) # ReLU activation function added!\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Initialize model\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of MLP(\n  (fc1): Linear(in_features=2, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=2, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/data-quality/#training","title":"Training","text":"<p><pre><code>from torch.optim import Adam\n</code></pre> <pre><code>LEARNING_RATE = 1e-3\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\n</code></pre> <pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values()))\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Accuracy\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\n</code></pre> <pre><code># Optimizer\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n</code></pre> <pre><code># Convert data to tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\n</code></pre> <pre><code># Training\nfor epoch in range(NUM_EPOCHS*10):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%10==0:\n        predictions = y_pred.max(dim=1)[1] # class\n        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\n</code></pre></p> <pre>\nEpoch: 0 | loss: 0.70, accuracy: 49.6\nEpoch: 10 | loss: 0.54, accuracy: 93.7\nEpoch: 20 | loss: 0.43, accuracy: 97.1\nEpoch: 30 | loss: 0.35, accuracy: 97.0\nEpoch: 40 | loss: 0.30, accuracy: 97.4\n</pre>"},{"location":"courses/foundations/data-quality/#evaluation","title":"Evaluation","text":"<p><pre><code>import json\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre> <pre><code># Predictions\ny_prob = F.softmax(model(X_test), dim=1)\ny_pred = y_prob.max(dim=1)[1]\n</code></pre> <pre><code># # Performance\nperformance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)\nprint (json.dumps(performance, indent=2))\n</code></pre></p> <pre>\n{\n  \"overall\": {\n    \"precision\": 0.9461538461538461,\n    \"recall\": 0.9619565217391304,\n    \"f1\": 0.9517707041477195,\n    \"num_samples\": 150.0\n  },\n  \"class\": {\n    \"benign\": {\n      \"precision\": 0.8923076923076924,\n      \"recall\": 1.0,\n      \"f1\": 0.9430894308943091,\n      \"num_samples\": 58.0\n    },\n    \"malignant\": {\n      \"precision\": 1.0,\n      \"recall\": 0.9239130434782609,\n      \"f1\": 0.96045197740113,\n      \"num_samples\": 92.0\n    }\n  }\n}\n</pre>"},{"location":"courses/foundations/data-quality/#inference","title":"Inference","text":"<p>We're going to plot a point, which we know belongs to the malignant tumor class. Our well trained model here would accurately predict that it is indeed a malignant tumor! <pre><code>def plot_multiclass_decision_boundary(model, X, y):\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n    cmap = plt.cm.Spectral\n\n    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n    y_pred = F.softmax(model(X_test), dim=1)\n    _, y_pred = y_pred.max(dim=1)\n    y_pred = y_pred.reshape(xx.shape)\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n</code></pre> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(8,5))\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n\n# Sample point near the decision boundary\nmean_leukocyte_count, mean_blood_pressure = X_scaler.transform(\n    [[np.mean(df.leukocyte_count), np.mean(df.blood_pressure)]])[0]\nplt.scatter(mean_leukocyte_count+0.05, mean_blood_pressure-0.05, s=200,\n            c=\"b\", edgecolor=\"w\", linewidth=2)\n\n# Annotate\nplt.annotate(\"true: malignant,\\npred: malignant\",\n             color=\"white\",\n             xy=(mean_leukocyte_count, mean_blood_pressure),\n             xytext=(0.4, 0.65),\n             textcoords=\"figure fraction\",\n             fontsize=16,\n             arrowprops=dict(facecolor=\"white\", shrink=0.1))\nplt.show()\n</code></pre></p> <p>Great! We received great performances on both our train and test data splits. We're going to use this dataset to show the importance of data quality.</p>"},{"location":"courses/foundations/data-quality/#reduced-dataset","title":"Reduced dataset","text":"<p>Let's remove some training data near the decision boundary and see how robust the model is now.</p>"},{"location":"courses/foundations/data-quality/#load-data_1","title":"Load data","text":"<pre><code># Raw reduced data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tumors_reduced.csv\"\ndf_reduced = pd.read_csv(url, header=0) # load\ndf_reduced = df_reduced.sample(frac=1).reset_index(drop=True) # shuffle\ndf_reduced.head()\n</code></pre> leukocyte_count blood_pressure tumor_class 0 16.795186 14.434741 benign 1 13.472969 15.250393 malignant 2 9.840450 16.434717 malignant 3 16.390730 14.419258 benign 4 13.367974 15.741790 malignant <pre><code># Define X and y\nX = df_reduced[[\"leukocyte_count\", \"blood_pressure\"]].values\ny = df_reduced[\"tumor_class\"].values\nprint (\"X: \", np.shape(X))\nprint (\"y: \", np.shape(y))\n</code></pre> <pre>\nX:  (720, 2)\ny:  (720,)\n</pre> <pre><code># Plot data\ncolors = {\"benign\": \"red\", \"malignant\": \"blue\"}\nplt.scatter(X[:, 0], X[:, 1], c=[colors[_y] for _y in y], s=25, edgecolors=\"k\")\nplt.xlabel(\"leukocyte count\")\nplt.ylabel(\"blood pressure\")\nplt.legend([\"malignant\", \"benign\"], loc=\"upper right\")\nplt.show()\n</code></pre>"},{"location":"courses/foundations/data-quality/#split-data_1","title":"Split data","text":"<pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre> <pre>\nX_train: (503, 2), y_train: (503,)\nX_val: (108, 2), y_val: (108,)\nX_test: (109, 2), y_test: (109,)\nSample point: [19.66235758 15.65939541] \u2192 benign\n</pre>"},{"location":"courses/foundations/data-quality/#label-encoding_1","title":"Label encoding","text":"<p><pre><code># Encode class labels\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y_train)\nnum_classes = len(label_encoder.classes_)\ny_train = label_encoder.transform(y_train)\ny_val = label_encoder.transform(y_val)\ny_test = label_encoder.transform(y_test)\n</code></pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre></p> <pre>\ncounts: [272 231]\nweights: {0: 0.003676470588235294, 1: 0.004329004329004329}\n</pre>"},{"location":"courses/foundations/data-quality/#standardize-data_1","title":"Standardize data","text":"<pre><code># Standardize inputs using training data\nX_scaler = StandardScaler().fit(X_train)\nX_train = X_scaler.transform(X_train)\nX_val = X_scaler.transform(X_val)\nX_test = X_scaler.transform(X_test)\n</code></pre>"},{"location":"courses/foundations/data-quality/#model_1","title":"Model","text":"<pre><code># Initialize model\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\n</code></pre>"},{"location":"courses/foundations/data-quality/#training_1","title":"Training","text":"<p><pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values()))\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Optimizer\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n</code></pre> <pre><code># Convert data to tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\n</code></pre> <pre><code># Training\nfor epoch in range(NUM_EPOCHS*10):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%10==0:\n        predictions = y_pred.max(dim=1)[1] # class\n        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\n</code></pre></p> <pre>\nEpoch: 0 | loss: 0.68, accuracy: 69.8\nEpoch: 10 | loss: 0.53, accuracy: 99.6\nEpoch: 20 | loss: 0.42, accuracy: 99.6\nEpoch: 30 | loss: 0.33, accuracy: 99.6\nEpoch: 40 | loss: 0.27, accuracy: 99.8\n</pre>"},{"location":"courses/foundations/data-quality/#evaluation_1","title":"Evaluation","text":"<p><pre><code># Predictions\ny_prob = F.softmax(model(X_test), dim=1)\ny_pred = y_prob.max(dim=1)[1]\n</code></pre> <pre><code># # Performance\nperformance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)\nprint (json.dumps(performance, indent=2))\n</code></pre></p> <pre>\n{\n  \"overall\": {\n    \"precision\": 1.0,\n    \"recall\": 1.0,\n    \"f1\": 1.0,\n    \"num_samples\": 109.0\n  },\n  \"class\": {\n    \"benign\": {\n      \"precision\": 1.0,\n      \"recall\": 1.0,\n      \"f1\": 1.0,\n      \"num_samples\": 59.0\n    },\n    \"malignant\": {\n      \"precision\": 1.0,\n      \"recall\": 1.0,\n      \"f1\": 1.0,\n      \"num_samples\": 50.0\n    }\n  }\n}\n</pre>"},{"location":"courses/foundations/data-quality/#inference_1","title":"Inference","text":"<p>Now let's see how the same inference point from earlier performs now on the model trained on the reduced dataset.</p> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(8,5))\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n\n# Sample point near the decision boundary (same point as before)\nplt.scatter(mean_leukocyte_count+0.05, mean_blood_pressure-0.05, s=200,\n            c=\"b\", edgecolor=\"w\", linewidth=2)\n\n# Annotate\nplt.annotate(\"true: malignant,\\npred: benign\",\n             color=\"white\",\n             xy=(mean_leukocyte_count, mean_blood_pressure),\n             xytext=(0.45, 0.60),\n             textcoords=\"figure fraction\",\n             fontsize=16,\n             arrowprops=dict(facecolor=\"white\", shrink=0.1))\nplt.show()\n</code></pre> <p>This is a very fragile but highly realistic scenario. Based on our reduced synthetic dataset, we have achieved a model that generalized really well on the test data. But when we ask for the prediction for the same point tested earlier (which we known is malignant), the prediction is now a benign tumor. We would have completely missed the tumor. To mitigate this, we can:</p> <ol> <li>Get more data around the space we are concerned about</li> <li>Consume predictions with caution when they are close to the decision boundary</li> </ol>"},{"location":"courses/foundations/data-quality/#takeaway","title":"Takeaway","text":"<p>Models are not crystal balls. So it's important that before any machine learning, we really look at our data and ask ourselves if it is truly representative for the task we want to solve. The model itself may fit really well and generalize well on your data but if the data is of poor quality to begin with, the model cannot be trusted.</p> <p>Once you are confident that your data is of good quality, you can finally start thinking about modeling. The type of model you choose depends on many factors, including the task, type of data, complexity required, etc.</p> <p>So once you figure out what type of model your task needs, start with simple models and then slowly add complexity. You don\u2019t want to start with neural networks right away because that may not be right model for your data and task. Striking this balance in model complexity is one of the key tasks of your data scientists. simple models \u2192 complex models</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Data quality - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/embeddings/","title":"Embeddings","text":""},{"location":"courses/foundations/embeddings/#overview","title":"Overview","text":"<p>While one-hot encoding allows us to preserve the structural information, it does poses two major disadvantages.</p> <ul> <li>linearly dependent on the number of unique tokens in our vocabulary, which is a problem if we're dealing with a large corpus.</li> <li>representation for each token does not preserve any relationship with respect to other tokens.</li> </ul> <p>In this notebook, we're going to motivate the need for embeddings and how they address all the shortcomings of one-hot encoding. The main idea of embeddings is to have fixed length representations for the tokens in a text regardless of the number of tokens in the vocabulary. With one-hot encoding, each token is represented by an array of size <code>vocab_size</code>, but with embeddings, each token now has the shape <code>embed_dim</code>. The values in the representation will are not fixed binary values but rather, changing floating points allowing for fine-grained learned representations.</p> <ul> <li>Objectives:<ul> <li>Represent tokens in text that capture the intrinsic semantic relationships.</li> </ul> </li> <li>Advantages:<ul> <li>Low-dimensionality while capturing relationships.</li> <li>Interpretable token representations</li> </ul> </li> <li>Disadvantages:<ul> <li>Can be computationally intensive to precompute.</li> </ul> </li> <li>Miscellaneous:<ul> <li>There are lot's of pretrained embeddings to choose from but you can also train your own from scratch.</li> </ul> </li> </ul>"},{"location":"courses/foundations/embeddings/#learning-embeddings","title":"Learning embeddings","text":"<p>We can learn embeddings by creating our models in PyTorch but first, we're going to use a library that specializes in embeddings and topic modeling called Gensim.</p> <pre><code>import nltk\nnltk.download(\"punkt\");\nimport numpy as np\nimport re\nimport urllib\n</code></pre> <pre>\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n</pre> <p><pre><code>SEED = 1234\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(SEED)\n</code></pre> <pre><code># Split text into sentences\ntokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\nbook = urllib.request.urlopen(url=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/harrypotter.txt\")\nsentences = tokenizer.tokenize(str(book.read()))\nprint (f\"{len(sentences)} sentences\")\n</code></pre></p> <pre>\n12443 sentences\n</pre> <p><pre><code>def preprocess(text):\n\"\"\"Conditional preprocessing on our text.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Spacing and filters\n    text = re.sub(r\"([-;;.,!?&lt;=&gt;])\", r\" \\1 \", text)\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()\n\n    # Separate into word tokens\n    text = text.split(\" \")\n\n    return text\n</code></pre> <pre><code># Preprocess sentences\nprint (sentences[11])\nsentences = [preprocess(sentence) for sentence in sentences]\nprint (sentences[11])\n</code></pre></p> <pre>\nSnape nodded, but did not elaborate.\n['snape', 'nodded', 'but', 'did', 'not', 'elaborate']\n</pre> <p>But how do we learn the embeddings the first place? The intuition behind embeddings is that the definition of a token doesn't depend on the token itself but on its context. There are several different ways of doing this:</p> <ol> <li>Given the word in the context, predict the target word (CBOW - continuous bag of words).</li> <li>Given the target word, predict the context word (skip-gram).</li> <li>Given a sequence of words, predict the next word (LM - language modeling).</li> </ol> <p>All of these approaches involve create data to train our model on. Every word in a sentence becomes the target word and the context words are determines by a window. In the image below (skip-gram), the window size is 2 (2 words to the left and right of the target word). We repeat this for every sentence in our corpus and this results in our training data for the unsupervised task. This in an unsupervised learning technique since we don't have official labels for contexts. The idea is that similar target words will appear with similar contexts and we can learn this relationship by repeatedly training our mode with (context, target) pairs.</p> <p>We can learn embeddings using any of these approaches above and some work better than others. You can inspect the learned embeddings but the best way to choose an approach is to empirically validate the performance on a supervised task.</p>"},{"location":"courses/foundations/embeddings/#word2vec","title":"Word2Vec","text":"<p>When we have large vocabularies to learn embeddings for, things can get complex very quickly. Recall that the backpropagation with softmax updates both the correct and incorrect class weights. This becomes a massive computation for every backwards pass we do so a workaround is to use negative sampling which only updates the correct class and a few arbitrary incorrect classes (<code>NEGATIVE_SAMPLING</code>=20). We're able to do this because of the large amount of training data where we'll see the same word as the target class multiple times.</p> <p><pre><code>import gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\n</code></pre> <pre><code>EMBEDDING_DIM = 100\nWINDOW = 5\nMIN_COUNT = 3 # Ignores all words with total frequency lower than this\nSKIP_GRAM = 1 # 0 = CBOW\nNEGATIVE_SAMPLING = 20\n</code></pre> <pre><code># Super fast because of optimized C code under the hood\nw2v = Word2Vec(\n    sentences=sentences, size=EMBEDDING_DIM,\n    window=WINDOW, min_count=MIN_COUNT,\n    sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\nprint (w2v)\n</code></pre></p> <pre>\nWord2Vec(vocab=4937, size=100, alpha=0.025)\n</pre> <pre><code># Vector for each word\nw2v.wv.get_vector(\"potter\")\n</code></pre> <pre>\narray([-0.11787166, -0.2702948 ,  0.24332453,  0.07497228, -0.5299148 ,\n        0.17751476, -0.30183575,  0.17060578, -0.0342238 , -0.331856  ,\n       -0.06467848,  0.02454215,  0.4524056 , -0.18918884, -0.22446074,\n        0.04246538,  0.5784022 ,  0.12316586,  0.03419832,  0.12895502,\n       -0.36260423,  0.06671549, -0.28563526, -0.06784113, -0.0838319 ,\n        0.16225453,  0.24313857,  0.04139925,  0.06982274,  0.59947336,\n        0.14201492, -0.00841052, -0.14700615, -0.51149386, -0.20590985,\n        0.00435914,  0.04931103,  0.3382509 , -0.06798466,  0.23954925,\n       -0.07505646, -0.50945646, -0.44729665,  0.16253233,  0.11114362,\n        0.05604156,  0.26727834,  0.43738437, -0.2606872 ,  0.16259147,\n       -0.28841105, -0.02349186,  0.00743417,  0.08558545, -0.0844396 ,\n       -0.44747537, -0.30635086, -0.04186366,  0.11142804,  0.03187608,\n        0.38674814, -0.2663519 ,  0.35415238,  0.094676  , -0.13586426,\n       -0.35296437, -0.31428036, -0.02917303,  0.02518964, -0.59744245,\n       -0.11500382,  0.15761602,  0.30535367, -0.06207089,  0.21460988,\n        0.17566076,  0.46426776,  0.15573359,  0.3675553 , -0.09043553,\n        0.2774392 ,  0.16967005,  0.32909656,  0.01422888,  0.4131812 ,\n        0.20034142,  0.13722987,  0.10324971,  0.14308734,  0.23772323,\n        0.2513108 ,  0.23396717, -0.10305202, -0.03343603,  0.14360961,\n       -0.01891198,  0.11430877,  0.30017182, -0.09570111, -0.10692801],\n      dtype=float32)\n</pre> <pre><code># Get nearest neighbors (excluding itself)\nw2v.wv.most_similar(positive=\"scar\", topn=5)\n</code></pre> <pre>\n[('pain', 0.9274871349334717),\n ('forehead', 0.9020695686340332),\n ('heart', 0.8953317999839783),\n ('mouth', 0.8939940929412842),\n ('throat', 0.8922691345214844)]\n</pre> <pre><code># Saving and loading\nw2v.wv.save_word2vec_format(\"model.bin\", binary=True)\nw2v = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True)\n</code></pre>"},{"location":"courses/foundations/embeddings/#fasttext","title":"FastText","text":"<p>What happens when a word doesn't exist in our vocabulary? We could assign an UNK token which is used for all OOV (out of vocabulary) words or we could use FastText, which uses character-level n-grams to embed a word. This helps embed rare words, misspelled words, and also words that don't exist in our corpus but are similar to words in our corpus. <pre><code>from gensim.models import FastText\n</code></pre> <pre><code># Super fast because of optimized C code under the hood\nft = FastText(sentences=sentences, size=EMBEDDING_DIM,\n              window=WINDOW, min_count=MIN_COUNT,\n              sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\nprint (ft)\n</code></pre></p> <pre>\nFastText(vocab=4937, size=100, alpha=0.025)\n</pre> <p><pre><code># This word doesn't exist so the word2vec model will error out\nw2v.wv.most_similar(positive=\"scarring\", topn=5)\n</code></pre> <pre><code># FastText will use n-grams to embed an OOV word\nft.wv.most_similar(positive=\"scarring\", topn=5)\n</code></pre></p> <pre>\n[('sparkling', 0.9785991907119751),\n ('coiling', 0.9770463705062866),\n ('watering', 0.9759057760238647),\n ('glittering', 0.9756022095680237),\n ('dazzling', 0.9755154848098755)]\n</pre> <pre><code># Save and loading\nft.wv.save(\"model.bin\")\nft = KeyedVectors.load(\"model.bin\")\n</code></pre>"},{"location":"courses/foundations/embeddings/#pretrained-embeddings","title":"Pretrained embeddings","text":"<p>We can learn embeddings from scratch using one of the approaches above but we can also leverage pretrained embeddings that have been trained on millions of documents. Popular ones include Word2Vec (skip-gram) or GloVe (global word-word co-occurrence). We can validate that these embeddings captured meaningful semantic relationships by confirming them.</p> <p><pre><code>from gensim.scripts.glove2word2vec import glove2word2vec\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\n</code></pre> <pre><code># Arguments\nEMBEDDING_DIM = 100\n</code></pre> <pre><code>def plot_embeddings(words, embeddings, pca_results):\n    for word in words:\n        index = embeddings.index2word.index(word)\n        plt.scatter(pca_results[index, 0], pca_results[index, 1])\n        plt.annotate(word, xy=(pca_results[index, 0], pca_results[index, 1]))\n    plt.show()\n</code></pre> <pre><code># Unzip the file (may take ~3-5 minutes)\nresp = urlopen(\"http://nlp.stanford.edu/data/glove.6B.zip\")\nzipfile = ZipFile(BytesIO(resp.read()))\nzipfile.namelist()\n</code></pre></p> <pre>\n['glove.6B.50d.txt',\n 'glove.6B.100d.txt',\n 'glove.6B.200d.txt',\n 'glove.6B.300d.txt']\n</pre> <pre><code># Write embeddings to file\nembeddings_file = \"glove.6B.{0}d.txt\".format(EMBEDDING_DIM)\nzipfile.extract(embeddings_file)\n</code></pre> <pre>\n/content/glove.6B.100d.txt\n</pre> <pre><code># Preview of the GloVe embeddings file\nwith open(embeddings_file, \"r\") as fp:\n    line = next(fp)\n    values = line.split()\n    word = values[0]\n    embedding = np.asarray(values[1:], dtype='float32')\n    print (f\"word: {word}\")\n    print (f\"embedding:\\n{embedding}\")\n    print (f\"embedding dim: {len(embedding)}\")\n</code></pre> <pre>\nword: the\nembedding:\n[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n  0.8278    0.27062 ]\nembedding dim: 100\n</pre> <pre><code># Save GloVe embeddings to local directory in word2vec format\nword2vec_output_file = \"{0}.word2vec\".format(embeddings_file)\nglove2word2vec(embeddings_file, word2vec_output_file)\n</code></pre> <pre>\n(400000, 100)\n</pre> <p><pre><code># Load embeddings (may take a minute)\nglove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n</code></pre> <pre><code># (king - man) + woman = ?\n# king - man = ? -  woman\nglove.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=5)\n</code></pre></p> <pre>\n[('queen', 0.7698541283607483),\n ('monarch', 0.6843380928039551),\n ('throne', 0.6755735874176025),\n ('daughter', 0.6594556570053101),\n ('princess', 0.6520534753799438)]\n</pre> <pre><code># Get nearest neighbors (excluding itself)\nglove.wv.most_similar(positive=\"goku\", topn=5)\n</code></pre> <pre>\n[('gohan', 0.7246542572975159),\n ('bulma', 0.6497020125389099),\n ('raistlin', 0.6443604230880737),\n ('skaar', 0.6316742897033691),\n ('guybrush', 0.6231324672698975)]\n</pre> <p><pre><code># Reduce dimensionality for plotting\nX = glove[glove.wv.vocab]\npca = PCA(n_components=2)\npca_results = pca.fit_transform(X)\n</code></pre> <pre><code># Visualize\nplot_embeddings(\n    words=[\"king\", \"queen\", \"man\", \"woman\"], embeddings=glove,\n    pca_results=pca_results)\n</code></pre></p> <pre><code># Bias in embeddings\nglove.most_similar(positive=[\"woman\", \"doctor\"], negative=[\"man\"], topn=5)\n</code></pre> <pre>\n[('nurse', 0.7735227346420288),\n ('physician', 0.7189429998397827),\n ('doctors', 0.6824328303337097),\n ('patient', 0.6750682592391968),\n ('dentist', 0.6726033687591553)]\n</pre>"},{"location":"courses/foundations/embeddings/#setup","title":"Set up","text":"<ul> <li>Load data</li> <li>preprocessing</li> <li>Split data</li> <li>Label encoding</li> <li>Tokenizer</li> </ul> <p>Let's set our seed and device for our main task. <pre><code>import numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code>def set_seeds(seed=1234):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # multi-GPU\n</code></pre> <pre><code># Set seeds for reproducibility\nset_seeds(seed=SEED)\n</code></pre> <pre><code># Set device\ncuda = True\ndevice = torch.device(\"cuda\" if (\n    torch.cuda.is_available() and cuda) else \"cpu\")\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\nif device.type == \"cuda\":\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\nprint (device)\n</code></pre></p> <pre>\ncuda\n</pre>"},{"location":"courses/foundations/embeddings/#load-data","title":"Load data","text":"<p>We will download the AG News dataset, which consists of 120K text samples from 4 unique classes (<code>Business</code>, <code>Sci/Tech</code>, <code>Sports</code>, <code>World</code>) <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/news.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> title category 0 Sharon Accepts Plan to Reduce Gaza Army Operation... World 1 Internet Key Battleground in Wildlife Crime Fight Sci/Tech 2 July Durable Good Orders Rise 1.7 Percent Business 3 Growing Signs of a Slowing on Wall Street Business 4 The New Faces of Reality TV World"},{"location":"courses/foundations/embeddings/#preprocessing","title":"Preprocessing","text":"<p>We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc. <pre><code>import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n</code></pre> <pre><code>nltk.download(\"stopwords\")\nSTOPWORDS = stopwords.words(\"english\")\nprint (STOPWORDS[:5])\nporter = PorterStemmer()\n</code></pre></p> <pre>\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n['i', 'me', 'my', 'myself', 'we']\n</pre> <p><pre><code>def preprocess(text, stopwords=STOPWORDS):\n\"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Remove stopwords\n    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n    text = pattern.sub(\"\", text)\n\n    # Remove words in parenthesis\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n\n    # Spacing and filters\n    text = re.sub(r\"([-;;.,!?&lt;=&gt;])\", r\" \\1 \", text)\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()\n\n    return text\n</code></pre> <pre><code># Sample\ntext = \"Great week for the NYSE!\"\npreprocess(text=text)\n</code></pre></p> <pre>\ngreat week nyse\n</pre> <pre><code># Apply to dataframe\npreprocessed_df = df.copy()\npreprocessed_df.title = preprocessed_df.title.apply(preprocess)\nprint (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")\n</code></pre> <pre>\nSharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n\nsharon accepts plan reduce gaza army operation haaretz says\n</pre> <p>Warning</p> <p>If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before applying those operations. This is because we cannot apply any knowledge gained from the test set accidentally (data leak) during preprocessing/training. However for global preprocessing steps like the function above where we aren't learning anything from the data itself, we can perform before splitting the data.</p>"},{"location":"courses/foundations/embeddings/#split-data","title":"Split data","text":"<p><pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Data\nX = preprocessed_df[\"title\"].values\ny = preprocessed_df[\"category\"].values\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (84000,), y_train: (84000,)\nX_val: (18000,), y_val: (18000,)\nX_test: (18000,), y_test: (18000,)\nSample point: china battles north korea nuclear talks \u2192 World\n</pre>"},{"location":"courses/foundations/embeddings/#label-encoding","title":"Label encoding","text":"<p>Next we'll define a <code>LabelEncoder</code> to encode our text labels into unique indices <pre><code>import itertools\n</code></pre> <pre><code>class LabelEncoder(object):\n\"\"\"Label encoder for tag labels.\"\"\"\n    def __init__(self, class_to_index={}):\n        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n\n    def __len__(self):\n        return len(self.class_to_index)\n\n    def __str__(self):\n        return f\"&lt;LabelEncoder(num_classes={len(self)})&gt;\"\n\n    def fit(self, y):\n        classes = np.unique(y)\n        for i, class_ in enumerate(classes):\n            self.class_to_index[class_] = i\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n        return self\n\n    def encode(self, y):\n        encoded = np.zeros((len(y)), dtype=int)\n        for i, item in enumerate(y):\n            encoded[i] = self.class_to_index[item]\n        return encoded\n\n    def decode(self, y):\n        classes = []\n        for i, item in enumerate(y):\n            classes.append(self.index_to_class[item])\n        return classes\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {'class_to_index': self.class_to_index}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Encode\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nNUM_CLASSES = len(label_encoder)\nlabel_encoder.class_to_index\n</code></pre></p> <pre>\n{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.encode(y_train)\ny_val = label_encoder.encode(y_val)\ny_test = label_encoder.encode(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\n</code></pre> <pre>\ny_train[0]: World\ny_train[0]: 3\n</pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [21000 21000 21000 21000]\nweights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n</pre>"},{"location":"courses/foundations/embeddings/#tokenizer","title":"Tokenizer","text":"<p>We'll define a <code>Tokenizer</code> to convert our text input data into token indices.</p> <p><pre><code>import json\nfrom collections import Counter\nfrom more_itertools import take\n</code></pre> <pre><code>class Tokenizer(object):\n    def __init__(self, char_level, num_tokens=None,\n                 pad_token=\"&lt;PAD&gt;\", oov_token=\"&lt;UNK&gt;\",\n                 token_to_index=None):\n        self.char_level = char_level\n        self.separator = \"\" if self.char_level else \" \"\n        if num_tokens: num_tokens -= 2 # pad + unk tokens\n        self.num_tokens = num_tokens\n        self.pad_token = pad_token\n        self.oov_token = oov_token\n        if not token_to_index:\n            token_to_index = {pad_token: 0, oov_token: 1}\n        self.token_to_index = token_to_index\n        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n\n    def __len__(self):\n        return len(self.token_to_index)\n\n    def __str__(self):\n        return f\"&lt;Tokenizer(num_tokens={len(self)})&gt;\"\n\n    def fit_on_texts(self, texts):\n        if not self.char_level:\n            texts = [text.split(\" \") for text in texts]\n        all_tokens = [token for text in texts for token in text]\n        counts = Counter(all_tokens).most_common(self.num_tokens)\n        self.min_token_freq = counts[-1][1]\n        for token, count in counts:\n            index = len(self)\n            self.token_to_index[token] = index\n            self.index_to_token[index] = token\n        return self\n\n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            if not self.char_level:\n                text = text.split(\" \")\n            sequence = []\n            for token in text:\n                sequence.append(self.token_to_index.get(\n                    token, self.token_to_index[self.oov_token]))\n            sequences.append(np.asarray(sequence))\n        return sequences\n\n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = []\n            for index in sequence:\n                text.append(self.index_to_token.get(index, self.oov_token))\n            texts.append(self.separator.join([token for token in text]))\n        return texts\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {\n                \"char_level\": self.char_level,\n                \"oov_token\": self.oov_token,\n                \"token_to_index\": self.token_to_index\n            }\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre></p> <p>Warning</p> <p>It's important that we only fit using our train data split because during inference, our model will not always know every token so it's important to replicate that scenario with our validation and test splits as well.</p> <pre><code># Tokenize\ntokenizer = Tokenizer(char_level=False, num_tokens=5000)\ntokenizer.fit_on_texts(texts=X_train)\nVOCAB_SIZE = len(tokenizer)\nprint (tokenizer)\n</code></pre> <pre>\n&lt;Tokenizer(num_tokens=5000)&gt;\n\n</pre> <pre><code># Sample of tokens\nprint (take(5, tokenizer.token_to_index.items()))\nprint (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens\n</code></pre> <pre>\n[('&lt;PAD&gt;', 0), ('&lt;UNK&gt;', 1), ('39', 2), ('b', 3), ('gt', 4)]\nleast freq token's freq: 14\n</pre> <pre><code># Convert texts to sequences of indices\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(X_test)\npreprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\nprint (\"Text to indices:\\n\"\n    f\"  (preprocessed) \u2192 {preprocessed_text}\\n\"\n    f\"  (tokenized) \u2192 {X_train[0]}\")\n</code></pre> <pre>\nText to indices:\n  (preprocessed) \u2192 nba wrap neal &lt;UNK&gt; 40 heat &lt;UNK&gt; wizards\n  (tokenized) \u2192 [ 299  359 3869    1 1648  734    1 2021]\n</pre>"},{"location":"courses/foundations/embeddings/#embedding-layer","title":"Embedding layer","text":"<p>We can embed our inputs using PyTorch's embedding layer.</p> <pre><code># Input\nvocab_size = 10\nx = torch.randint(high=vocab_size, size=(1,5))\nprint (x)\nprint (x.shape)\n</code></pre> <pre>\ntensor([[2, 6, 5, 2, 6]])\ntorch.Size([1, 5])\n</pre> <pre><code># Embedding layer\nembeddings = nn.Embedding(embedding_dim=100, num_embeddings=vocab_size)\nprint (embeddings.weight.shape)\n</code></pre> <pre>\ntorch.Size([10, 100])\n</pre> <pre><code># Embed the input\nembeddings(x).shape\n</code></pre> <pre>\ntorch.Size([1, 5, 100])\n</pre> <p>Each token in the input is represented via embeddings (all out-of-vocabulary (OOV) tokens are given the embedding for <code>UNK</code> token.) In the model below, we'll see how to set these embeddings to be pretrained GloVe embeddings and how to choose whether to freeze (fixed embedding weights) those embeddings or not during training.</p>"},{"location":"courses/foundations/embeddings/#padding","title":"Padding","text":"<p>Our inputs are all of varying length but we need each batch to be uniformly shaped. Therefore, we will use padding to make all the inputs in the batch the same length. Our padding index will be 0 (note that this is consistent with the <code>&lt;PAD&gt;</code> token defined in our <code>Tokenizer</code>).</p> <p>While embedding our input tokens will create a batch of shape (<code>N</code>, <code>max_seq_len</code>, <code>embed_dim</code>) we only need to provide a 2D matrix (<code>N</code>, <code>max_seq_len</code>) for using embeddings with PyTorch.</p> <p><pre><code>def pad_sequences(sequences, max_seq_len=0):\n\"\"\"Pad sequences to max length in sequence.\"\"\"\n    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n    padded_sequences = np.zeros((len(sequences), max_seq_len))\n    for i, sequence in enumerate(sequences):\n        padded_sequences[i][:len(sequence)] = sequence\n    return padded_sequences\n</code></pre> <pre><code># 2D sequences\npadded = pad_sequences(X_train[0:3])\nprint (padded.shape)\nprint (padded)\n</code></pre></p> <pre>\n(3, 8)\n[[2.990e+02 3.590e+02 3.869e+03 1.000e+00 1.648e+03 7.340e+02 1.000e+00\n  2.021e+03]\n [4.977e+03 1.000e+00 8.070e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00]\n [5.900e+01 1.213e+03 1.160e+02 4.042e+03 2.040e+02 4.190e+02 1.000e+00\n  0.000e+00]]\n</pre>"},{"location":"courses/foundations/embeddings/#dataset","title":"Dataset","text":"<p>We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits.</p> <p><pre><code>FILTER_SIZES = list(range(1, 4)) # uni, bi and tri grams\n</code></pre> <pre><code>class Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y, max_filter_size):\n        self.X = X\n        self.y = y\n        self.max_filter_size = max_filter_size\n\n    def __len__(self):\n        return len(self.y)\n\n    def __str__(self):\n        return f\"&lt;Dataset(N={len(self)})&gt;\"\n\n    def __getitem__(self, index):\n        X = self.X[index]\n        y = self.y[index]\n        return [X, y]\n\n    def collate_fn(self, batch):\n\"\"\"Processing on a batch.\"\"\"\n        # Get inputs\n        batch = np.array(batch)\n        X = batch[:, 0]\n        y = batch[:, 1]\n\n        # Pad sequences\n        X = pad_sequences(X)\n\n        # Cast\n        X = torch.LongTensor(X.astype(np.int32))\n        y = torch.LongTensor(y.astype(np.int32))\n\n        return X, y\n\n    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n        return torch.utils.data.DataLoader(\n            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n            shuffle=shuffle, drop_last=drop_last, pin_memory=True)\n</code></pre> <pre><code># Create datasets\nmax_filter_size = max(FILTER_SIZES)\ntrain_dataset = Dataset(X=X_train, y=y_train, max_filter_size=max_filter_size)\nval_dataset = Dataset(X=X_val, y=y_val, max_filter_size=max_filter_size)\ntest_dataset = Dataset(X=X_test, y=y_test, max_filter_size=max_filter_size)\nprint (\"Datasets:\\n\"\n    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {train_dataset[0][0]}\\n\"\n    f\"  y: {train_dataset[0][1]}\")\n</code></pre></p> <pre>\nDatasets:\n  Train dataset: &lt;Dataset(N=84000)&gt;\n  Val dataset: &lt;Dataset(N=18000)&gt;\n  Test dataset: &lt;Dataset(N=18000)&gt;\nSample point:\n  X: [ 299  359 3869    1 1648  734    1 2021]\n  y: 2\n</pre> <pre><code># Create dataloaders\nbatch_size = 64\ntrain_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\nval_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\ntest_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\nbatch_X, batch_y = next(iter(train_dataloader))\nprint (\"Sample batch:\\n\"\n    f\"  X: {list(batch_X.size())}\\n\"\n    f\"  y: {list(batch_y.size())}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {batch_X[0]}\\n\"\n    f\"  y: {batch_y[0]}\")\n</code></pre> <pre>\nSample batch:\n  X: [64, 9]\n  y: [64]\nSample point:\n  X: tensor([ 299,  359, 3869,    1, 1648,  734,    1, 2021,    0], device=\"cpu\")\n  y: 2\n</pre>"},{"location":"courses/foundations/embeddings/#model","title":"Model","text":"<p>We'll be using a convolutional neural network on top of our embedded tokens to extract meaningful spatial signal. This time, we'll be using many filter widths to act as n-gram feature extractors.</p> <p>Let's visualize the model's forward pass.</p> <ol> <li>We'll first tokenize our inputs (<code>batch_size</code>, <code>max_seq_len</code>).</li> <li>Then we'll embed our tokenized inputs (<code>batch_size</code>, <code>max_seq_len</code>, <code>embedding_dim</code>).</li> <li>We'll apply convolution via filters (<code>filter_size</code>, <code>embedding_dim</code>, <code>num_filters</code>) followed by batch normalization. Our filters act as character level n-gram detectors. We have three different filter sizes (2, 3 and 4) and they will act as bi-gram, tri-gram and 4-gram feature extractors, respectively.</li> <li>We'll apply 1D global max pooling which will extract the most relevant information from the feature maps for making the decision.</li> <li>We feed the pool outputs to a fully-connected (FC) layer (with dropout).</li> <li>We use one more FC layer with softmax to derive class probabilities.</li> </ol> <p><pre><code>import math\nimport torch.nn.functional as F\n</code></pre> <pre><code>EMBEDDING_DIM = 100\nHIDDEN_DIM = 100\nDROPOUT_P = 0.1\n</code></pre> <pre><code>class CNN(nn.Module):\n    def __init__(self, embedding_dim, vocab_size, num_filters,\n                 filter_sizes, hidden_dim, dropout_p, num_classes,\n                 pretrained_embeddings=None, freeze_embeddings=False,\n                 padding_idx=0):\n        super(CNN, self).__init__()\n\n        # Filter sizes\n        self.filter_sizes = filter_sizes\n\n        # Initialize embeddings\n        if pretrained_embeddings is None:\n            self.embeddings = nn.Embedding(\n                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n                padding_idx=padding_idx)\n        else:\n            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n            self.embeddings = nn.Embedding(\n                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n                padding_idx=padding_idx, _weight=pretrained_embeddings)\n\n        # Freeze embeddings or not\n        if freeze_embeddings:\n            self.embeddings.weight.requires_grad = False\n\n        # Conv weights\n        self.conv = nn.ModuleList(\n            [nn.Conv1d(in_channels=embedding_dim,\n                       out_channels=num_filters,\n                       kernel_size=f) for f in filter_sizes])\n\n        # FC weights\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs, channel_first=False):\n\n        # Embed\n        x_in, = inputs\n        x_in = self.embeddings(x_in)\n\n        # Rearrange input so num_channels is in dim 1 (N, C, L)\n        if not channel_first:\n            x_in = x_in.transpose(1, 2)\n\n        # Conv outputs\n        z = []\n        max_seq_len = x_in.shape[2]\n        for i, f in enumerate(self.filter_sizes):\n            # `SAME` padding\n            padding_left = int((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\n            padding_right = int(math.ceil((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\n\n            # Conv + pool\n            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\n            _z = F.max_pool1d(_z, _z.size(2)).squeeze(2)\n            z.append(_z)\n\n        # Concat conv outputs\n        z = torch.cat(z, 1)\n\n        # FC layers\n        z = self.fc1(z)\n        z = self.dropout(z)\n        z = self.fc2(z)\n        return z\n</code></pre></p>"},{"location":"courses/foundations/embeddings/#using-glove","title":"Using GloVe","text":"<p>We're going create some utility functions to be able to load the pretrained GloVe embeddings into our Embeddings layer.</p> <p><pre><code>def load_glove_embeddings(embeddings_file):\n\"\"\"Load embeddings from a file.\"\"\"\n    embeddings = {}\n    with open(embeddings_file, \"r\") as fp:\n        for index, line in enumerate(fp):\n            values = line.split()\n            word = values[0]\n            embedding = np.asarray(values[1:], dtype='float32')\n            embeddings[word] = embedding\n    return embeddings\n</code></pre> <pre><code>def make_embeddings_matrix(embeddings, word_index, embedding_dim):\n\"\"\"Create embeddings matrix to use in Embedding layer.\"\"\"\n    embedding_matrix = np.zeros((len(word_index), embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n</code></pre> <pre><code># Create embeddings\nembeddings_file = 'glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\nglove_embeddings = load_glove_embeddings(embeddings_file=embeddings_file)\nembedding_matrix = make_embeddings_matrix(\n    embeddings=glove_embeddings, word_index=tokenizer.token_to_index,\n    embedding_dim=EMBEDDING_DIM)\nprint (f\"&lt;Embeddings(words={embedding_matrix.shape[0]}, dim={embedding_matrix.shape[1]})&gt;\")\n</code></pre></p> <pre>\n&lt;Embeddings(words=5000, dim=100)&gt;\n</pre>"},{"location":"courses/foundations/embeddings/#experiments","title":"Experiments","text":"<p>We have first have to decide whether to use pretrained embeddings randomly initialized ones. Then, we can choose to freeze our embeddings or continue to train them using the supervised data (this could lead to overfitting). Here are the three experiments we're going to conduct:</p> <ul> <li>randomly initialized embeddings (fine-tuned)</li> <li>GloVe embeddings (frozen)</li> <li>GloVe embeddings (fine-tuned)</li> </ul> <p><pre><code>import json\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom torch.optim import Adam\n</code></pre> <pre><code>NUM_FILTERS = 50\nLEARNING_RATE = 1e-3\nPATIENCE = 5\nNUM_EPOCHS = 10\n</code></pre> <pre><code>class Trainer(object):\n    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n\n        # Set params\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train_step(self, dataloader):\n\"\"\"Train step.\"\"\"\n        # Set model to train mode\n        self.model.train()\n        loss = 0.0\n\n        # Iterate over train batches\n        for i, batch in enumerate(dataloader):\n\n            # Step\n            batch = [item.to(self.device) for item in batch]  # Set device\n            inputs, targets = batch[:-1], batch[-1]\n            self.optimizer.zero_grad()  # Reset gradients\n            z = self.model(inputs)  # Forward pass\n            J = self.loss_fn(z, targets)  # Define loss\n            J.backward()  # Backward pass\n            self.optimizer.step()  # Update weights\n\n            # Cumulative Metrics\n            loss += (J.detach().item() - loss) / (i + 1)\n\n        return loss\n\n    def eval_step(self, dataloader):\n\"\"\"Validation or test step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        loss = 0.0\n        y_trues, y_probs = [], []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Step\n                batch = [item.to(self.device) for item in batch]  # Set device\n                inputs, y_true = batch[:-1], batch[-1]\n                z = self.model(inputs)  # Forward pass\n                J = self.loss_fn(z, y_true).item()\n\n                # Cumulative Metrics\n                loss += (J - loss) / (i + 1)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n                y_trues.extend(y_true.cpu().numpy())\n\n        return loss, np.vstack(y_trues), np.vstack(y_probs)\n\n    def predict_step(self, dataloader):\n\"\"\"Prediction step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        y_probs = []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Forward pass w/ inputs\n                inputs, targets = batch[:-1], batch[-1]\n                z = self.model(inputs)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n\n        return np.vstack(y_probs)\n\n    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n        best_val_loss = np.inf\n        for epoch in range(num_epochs):\n            # Steps\n            train_loss = self.train_step(dataloader=train_dataloader)\n            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n            self.scheduler.step(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = self.model\n                _patience = patience  # reset _patience\n            else:\n                _patience -= 1\n            if not _patience:  # 0\n                print(\"Stopping early!\")\n                break\n\n            # Logging\n            print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.5f}, \"\n                f\"val_loss: {val_loss:.5f}, \"\n                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n                f\"_patience: {_patience}\"\n            )\n        return best_model\n</code></pre> <pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre></p>"},{"location":"courses/foundations/embeddings/#random-initialization","title":"Random initialization","text":"<p><pre><code>PRETRAINED_EMBEDDINGS = None\nFREEZE_EMBEDDINGS = False\n</code></pre> <pre><code># Initialize model\nmodel = CNN(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of CNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (conv): ModuleList(\n    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=150, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)&gt;\n</pre> <p><pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p> <pre>\nEpoch: 1 | train_loss: 0.77038, val_loss: 0.59683, lr: 1.00E-03, _patience: 3\nEpoch: 2 | train_loss: 0.49571, val_loss: 0.54363, lr: 1.00E-03, _patience: 3\nEpoch: 3 | train_loss: 0.40796, val_loss: 0.54551, lr: 1.00E-03, _patience: 2\nEpoch: 4 | train_loss: 0.34797, val_loss: 0.57950, lr: 1.00E-03, _patience: 1\nStopping early!\n</pre> <p><pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.8070310520771562,\n  \"recall\": 0.7999444444444445,\n  \"f1\": 0.8012357147662316,\n  \"num_samples\": 18000.0\n}\n</pre>"},{"location":"courses/foundations/embeddings/#glove-frozen","title":"Glove (frozen)","text":"<p><pre><code>PRETRAINED_EMBEDDINGS = embedding_matrix\nFREEZE_EMBEDDINGS = True\n</code></pre> <pre><code># Initialize model\nmodel = CNN(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of CNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (conv): ModuleList(\n    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=150, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)&gt;\n</pre> <p><pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p> <pre>\nEpoch: 1 | train_loss: 0.51510, val_loss: 0.47643, lr: 1.00E-03, _patience: 3\nEpoch: 2 | train_loss: 0.44220, val_loss: 0.46124, lr: 1.00E-03, _patience: 3\nEpoch: 3 | train_loss: 0.41204, val_loss: 0.46231, lr: 1.00E-03, _patience: 2\nEpoch: 4 | train_loss: 0.38733, val_loss: 0.46606, lr: 1.00E-03, _patience: 1\nStopping early!\n</pre> <p><pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.8304874226557859,\n  \"recall\": 0.8281111111111111,\n  \"f1\": 0.828556487688813,\n  \"num_samples\": 18000.0\n}\n</pre>"},{"location":"courses/foundations/embeddings/#glove-fine-tuned","title":"Glove (fine-tuned)","text":"<p><pre><code>PRETRAINED_EMBEDDINGS = embedding_matrix\nFREEZE_EMBEDDINGS = False\n</code></pre> <pre><code># Initialize model\nmodel = CNN(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of CNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (conv): ModuleList(\n    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=150, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)&gt;\n</pre> <p><pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p> <pre>\nEpoch: 1 | train_loss: 0.48908, val_loss: 0.44320, lr: 1.00E-03, _patience: 3\nEpoch: 2 | train_loss: 0.38986, val_loss: 0.43616, lr: 1.00E-03, _patience: 3\nEpoch: 3 | train_loss: 0.34403, val_loss: 0.45240, lr: 1.00E-03, _patience: 2\nEpoch: 4 | train_loss: 0.30224, val_loss: 0.49063, lr: 1.00E-03, _patience: 1\nStopping early!\n</pre> <p><pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.8297157849772082,\n  \"recall\": 0.8263333333333334,\n  \"f1\": 0.8266579939871359,\n  \"num_samples\": 18000.0\n}\n</pre> <pre><code># Save artifacts\nfrom pathlib import Path\ndir = Path(\"cnn\")\ndir.mkdir(parents=True, exist_ok=True)\nlabel_encoder.save(fp=Path(dir, \"label_encoder.json\"))\ntokenizer.save(fp=Path(dir, \"tokenizer.json\"))\ntorch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\nwith open(Path(dir, \"performance.json\"), \"w\") as fp:\n    json.dump(performance, indent=2, sort_keys=False, fp=fp)\n</code></pre>"},{"location":"courses/foundations/embeddings/#inference","title":"Inference","text":"<p><pre><code>def get_probability_distribution(y_prob, classes):\n\"\"\"Create a dict of class probabilities from an array.\"\"\"\n    results = {}\n    for i, class_ in enumerate(classes):\n        results[class_] = np.float64(y_prob[i])\n    sorted_results = {k: v for k, v in sorted(\n        results.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_results\n</code></pre> <pre><code># Load artifacts\ndevice = torch.device(\"cpu\")\nlabel_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\ntokenizer = Tokenizer.load(fp=Path(dir, \"tokenizer.json\"))\nmodel = CNN(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\nmodel.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\nmodel.to(device)\n</code></pre></p> <pre>\nCNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (conv): ModuleList(\n    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=150, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)\n</pre> <p><pre><code># Initialize trainer\ntrainer = Trainer(model=model, device=device)\n</code></pre> <pre><code># Dataloader\ntext = \"The final tennis tournament starts next week.\"\nX = tokenizer.texts_to_sequences([preprocess(text)])\nprint (tokenizer.sequences_to_texts(X))\ny_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\ndataset = Dataset(X=X, y=y_filler, max_filter_size=max_filter_size)\ndataloader = dataset.create_dataloader(batch_size=batch_size)\n</code></pre></p> <pre>\n['final tennis tournament starts next week']\n</pre> <pre><code># Inference\ny_prob = trainer.predict_step(dataloader)\ny_pred = np.argmax(y_prob, axis=1)\nlabel_encoder.decode(y_pred)\n</code></pre> <pre>\n['Sports']\n</pre> <pre><code># Class distributions\nprob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\nprint (json.dumps(prob_dist, indent=2))\n</code></pre> <pre>\n{\n  \"Sports\": 0.9999998807907104,\n  \"World\": 6.336378532978415e-08,\n  \"Sci/Tech\": 2.107449992294619e-09,\n  \"Business\": 3.706519813295728e-10\n}\n</pre>"},{"location":"courses/foundations/embeddings/#interpretability","title":"Interpretability","text":"<p>We went through all the trouble of padding our inputs before convolution to result is outputs of the same shape as our inputs so we can try to get some interpretability. Since every token is mapped to a convolutional output on which we apply max pooling, we can see which token's output was most influential towards the prediction. We first need to get the conv outputs from our model:</p> <p><pre><code>import collections\nimport seaborn as sns\n</code></pre> <pre><code>class InterpretableCNN(nn.Module):\n    def __init__(self, embedding_dim, vocab_size, num_filters,\n                 filter_sizes, hidden_dim, dropout_p, num_classes,\n                 pretrained_embeddings=None, freeze_embeddings=False,\n                 padding_idx=0):\n        super(InterpretableCNN, self).__init__()\n\n        # Filter sizes\n        self.filter_sizes = filter_sizes\n\n        # Initialize embeddings\n        if pretrained_embeddings is None:\n            self.embeddings = nn.Embedding(\n                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n                padding_idx=padding_idx)\n        else:\n            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n            self.embeddings = nn.Embedding(\n                embedding_dim=embedding_dim, num_embeddings=vocab_size,\n                padding_idx=padding_idx, _weight=pretrained_embeddings)\n\n        # Freeze embeddings or not\n        if freeze_embeddings:\n            self.embeddings.weight.requires_grad = False\n\n        # Conv weights\n        self.conv = nn.ModuleList(\n            [nn.Conv1d(in_channels=embedding_dim,\n                       out_channels=num_filters,\n                       kernel_size=f) for f in filter_sizes])\n\n        # FC weights\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs, channel_first=False):\n\n        # Embed\n        x_in, = inputs\n        x_in = self.embeddings(x_in)\n\n        # Rearrange input so num_channels is in dim 1 (N, C, L)\n        if not channel_first:\n            x_in = x_in.transpose(1, 2)\n\n        # Conv outputs\n        z = []\n        max_seq_len = x_in.shape[2]\n        for i, f in enumerate(self.filter_sizes):\n            # `SAME` padding\n            padding_left = int((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\n            padding_right = int(math.ceil((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\n\n            # Conv + pool\n            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\n            z.append(_z.cpu().numpy())\n\n        return z\n</code></pre> <pre><code>PRETRAINED_EMBEDDINGS = embedding_matrix\nFREEZE_EMBEDDINGS = False\n</code></pre> <pre><code># Initialize model\ninterpretable_model = InterpretableCNN(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    num_filters=NUM_FILTERS, filter_sizes=FILTER_SIZES,\n    hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P, num_classes=NUM_CLASSES,\n    pretrained_embeddings=PRETRAINED_EMBEDDINGS, freeze_embeddings=FREEZE_EMBEDDINGS)\ninterpretable_model.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\ninterpretable_model.to(device)\n</code></pre></p> <pre>\nInterpretableCNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (conv): ModuleList(\n    (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,))\n    (1): Conv1d(100, 50, kernel_size=(2,), stride=(1,))\n    (2): Conv1d(100, 50, kernel_size=(3,), stride=(1,))\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=150, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)\n</pre> <pre><code># Get conv outputs\ninterpretable_model.eval()\nconv_outputs = []\nwith torch.inference_mode():\n    for i, batch in enumerate(dataloader):\n\n        # Forward pass w/ inputs\n        inputs, targets = batch[:-1], batch[-1]\n        z = interpretable_model(inputs)\n\n        # Store conv outputs\n        conv_outputs.extend(z)\n\nconv_outputs = np.vstack(conv_outputs)\nprint (conv_outputs.shape) # (len(filter_sizes), num_filters, max_seq_len)\n</code></pre> <pre>\n(3, 50, 6)\n</pre> <pre><code># Visualize a bi-gram filter's outputs\ntokens = tokenizer.sequences_to_texts(X)[0].split(\" \")\nfilter_size = 2\nsns.heatmap(conv_outputs[filter_size-1][:, len(tokens)], xticklabels=tokens)\n</code></pre> <p>1D global max-pooling would extract the highest value from each of our <code>num_filters</code> for each <code>filter_size</code>. We could also follow this same approach to figure out which n-gram is most relevant but notice in the heatmap above that many filters don't have much variance. To mitigate this, this paper uses threshold values to determine which filters to use for interpretability. But to keep things simple, let's extract which tokens' filter outputs were extracted via max-pooling the most frequently.</p> <pre><code>sample_index = 0\nprint (f\"Original text:\\n{text}\")\nprint (f\"\\nPreprocessed text:\\n{tokenizer.sequences_to_texts(X)[0]}\")\nprint (\"\\nMost important n-grams:\")\n# Process conv outputs for each unique filter size\nfor i, filter_size in enumerate(FILTER_SIZES):\n\n    # Identify most important n-gram (excluding last token)\n    popular_indices = collections.Counter([np.argmax(conv_output) \\\n            for conv_output in conv_outputs[i]])\n\n    # Get corresponding text\n    start = popular_indices.most_common(1)[-1][0]\n    n_gram = \" \".join([token for token in tokens[start:start+filter_size]])\n    print (f\"[{filter_size}-gram]: {n_gram}\")\n</code></pre> <pre>\nOriginal text:\nThe final tennis tournament starts next week.\n\nPreprocessed text:\nfinal tennis tournament starts next week\n\nMost important n-grams:\n[1-gram]: tennis\n[2-gram]: tennis tournament\n[3-gram]: final tennis tournament\n</pre> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Embeddings - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/linear-regression/","title":"Linear Regression","text":""},{"location":"courses/foundations/linear-regression/#overview","title":"Overview","text":"<p>Our goal is to learn a linear model \\(\\hat{y}\\) that models \\(y\\) given \\(X\\) using weights \\(W\\) and bias \\(b\\):</p> \\[ \\hat{y} = XW + b \\] <p> Variable Description \\(N\\) total numbers of samples \\(\\hat{y}\\) predictions \\(\\in \\mathbb{R}^{NX1}\\) \\(X\\) inputs \\(\\in \\mathbb{R}^{NXD}\\) \\(W\\) weights \\(\\in \\mathbb{R}^{DX1}\\) \\(b\\) bias \\(\\in \\mathbb{R}^{1}\\) <p></p> <ul> <li>Objective:<ul> <li>Use inputs \\(X\\) to predict the output \\(\\hat{y}\\) using a linear model. The model will be a line of best fit that minimizes the distance between the predicted (model's output) and target (ground truth) values. Training data \\((X, y)\\) is used to train the model and learn the weights \\(W\\) using gradient descent.</li> </ul> </li> <li>Advantages:<ul> <li>Computationally simple.</li> <li>Highly interpretable.</li> <li>Can account for continuous and categorical features.</li> </ul> </li> <li>Disadvantages:<ul> <li>The model will perform well only when the data is linearly separable (for classification).</li> </ul> </li> <li>Miscellaneous:<ul> <li>You can also use linear regression for binary classification tasks where if the predicted continuous value is above a threshold, it belongs to a certain class. But we will cover better techniques for classification in future lessons and will focus on linear regression for continuous regression tasks only.</li> </ul> </li> </ul>"},{"location":"courses/foundations/linear-regression/#generate-data","title":"Generate data","text":"<p>We're going to generate some simple dummy data to apply linear regression on. It's going to create roughly linear data (<code>y = 3.5X + noise</code>); the random noise is added to create realistic data that doesn't perfectly align in a line. Our goal is to have the model converge to a similar linear equation (there will be slight variance since we added some noise). <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>SEED = 1234\nNUM_SAMPLES = 50\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(SEED)\n</code></pre> <pre><code># Generate synthetic data\ndef generate_data(num_samples):\n\"\"\"Generate dummy data for linear regression.\"\"\"\n    X = np.array(range(num_samples))\n    random_noise = np.random.uniform(-10, 20, size=num_samples)\n    y = 3.5*X + random_noise # add some noise\n    return X, y\n</code></pre> <pre><code># Generate random (linear) data\nX, y = generate_data(num_samples=NUM_SAMPLES)\ndata = np.vstack([X, y]).T\nprint (data[:5])\n</code></pre></p> <pre>\n[[ 0.         -4.25441649]\n [ 1.         12.16326313]\n [ 2.         10.13183217]\n [ 3.         24.06075751]\n [ 4.         27.39927424]]\n</pre> <pre><code># Load into a Pandas DataFrame\ndf = pd.DataFrame(data, columns=[\"X\", \"y\"])\nX = df[[\"X\"]].values\ny = df[[\"y\"]].values\ndf.head()\n</code></pre> X y 0 0.0 -4.254416 1 1.0 12.163263 2 2.0 10.131832 3 3.0 24.060758 4 4.0 27.399274 <pre><code># Scatter plot\nplt.title(\"Generated data\")\nplt.scatter(x=df[\"X\"], y=df[\"y\"])\nplt.show()\n</code></pre>"},{"location":"courses/foundations/linear-regression/#numpy","title":"NumPy","text":"<p>Now that we have our data prepared, we'll first implement linear regression using just NumPy. This will let us really understand the underlying operations.</p>"},{"location":"courses/foundations/linear-regression/#split-data","title":"Split data","text":"<p>Since our task is a regression task, we will randomly split our dataset into three sets: train, validation and test data splits.</p> <ul> <li><code>train</code>: used to train our model.</li> <li><code>val</code> : used to validate our model's performance during training.</li> <li><code>test</code>: used to do an evaluation of our fully trained model.</li> </ul> <p>Be sure to check out our entire lesson focused on properly splitting data in our MLOps course.</p> <p><pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code># Shuffle data\nindices = list(range(NUM_SAMPLES))\nnp.random.shuffle(indices)\nX = X[indices]\ny = y[indices]\n</code></pre></p> <pre></pre> <p>Warning</p> <p>Be careful not to shuffle \\(X\\) and \\(y\\) separately because then the inputs won't correspond to the outputs!</p> <p><pre><code># Split indices\ntrain_start = 0\ntrain_end = int(0.7*NUM_SAMPLES)\nval_start = train_end\nval_end = int((TRAIN_SIZE+VAL_SIZE)*NUM_SAMPLES)\ntest_start = val_end\n</code></pre> <pre><code># Split data\nX_train = X[train_start:train_end]\ny_train = y[train_start:train_end]\nX_val = X[val_start:val_end]\ny_val = y[val_start:val_end]\nX_test = X[test_start:]\ny_test = y[test_start:]\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_test: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n</code></pre></p> <pre>\nX_train: (35, 1), y_train: (35, 1)\nX_val: (7, 1), y_test: (7, 1)\nX_test: (8, 1), y_test: (8, 1)\n</pre>"},{"location":"courses/foundations/linear-regression/#standardize-data","title":"Standardize data","text":"<p>We need to standardize our data (zero mean and unit variance) so a specific feature's magnitude doesn't affect how the model learns its weights.</p> \\[  z = \\frac{x_i - \\mu}{\\sigma} \\] <p> Variable Description \\(z\\) standardized value \\(x_i\\) inputs \\(\\mu\\) mean \\(\\sigma\\) standard deviation <p></p> <p><pre><code>def standardize_data(data, mean, std):\n    return (data - mean)/std\n</code></pre> <pre><code># Determine means and stds\nX_mean = np.mean(X_train)\nX_std = np.std(X_train)\ny_mean = np.mean(y_train)\ny_std = np.std(y_train)\n</code></pre></p> <pre></pre> <p>We need to treat the validation and test sets as if they were hidden datasets. So we only use the train set to determine the mean and std to avoid biasing our training process.</p> <p><pre><code># Standardize\nX_train = standardize_data(X_train, X_mean, X_std)\ny_train = standardize_data(y_train, y_mean, y_std)\nX_val = standardize_data(X_val, X_mean, X_std)\ny_val = standardize_data(y_val, y_mean, y_std)\nX_test = standardize_data(X_test, X_mean, X_std)\ny_test = standardize_data(y_test, y_mean, y_std)\n</code></pre> <pre><code># Check (means should be ~0 and std should be ~1)\n# Check (means should be ~0 and std should be ~1)\nprint (f\"mean: {np.mean(X_test, axis=0)[0]:.1f}, std: {np.std(X_test, axis=0)[0]:.1f}\")\nprint (f\"mean: {np.mean(y_test, axis=0)[0]:.1f}, std: {np.std(y_test, axis=0)[0]:.1f}\")\n</code></pre></p> <pre>\nmean: -0.4, std: 0.9\nmean: -0.3, std: 1.0\n</pre>"},{"location":"courses/foundations/linear-regression/#weights","title":"Weights","text":"<p>Our goal is to learn a linear model \\(\\hat{y}\\) that models \\(y\\) given \\(X\\) using weights \\(W\\) and bias \\(b\\) \u2192 \\(\\hat{y} = XW + b\\)</p> <p><code>Step 1</code>: Randomly initialize the model's weights \\(W\\). <pre><code>INPUT_DIM = X_train.shape[1] # X is 1-dimensional\nOUTPUT_DIM = y_train.shape[1] # y is 1-dimensional\n</code></pre> <pre><code># Initialize random weights\nW = 0.01 * np.random.randn(INPUT_DIM, OUTPUT_DIM)\nb = np.zeros((1, 1))\nprint (f\"W: {W.shape}\")\nprint (f\"b: {b.shape}\")\n</code></pre></p> <pre>\nW: (1, 1)\nb: (1, 1)\n</pre>"},{"location":"courses/foundations/linear-regression/#model","title":"Model","text":"<p><code>Step 2</code>: Feed inputs \\(X\\) into the model to receive the predictions \\(\\hat{y}\\) <pre><code># Forward pass [NX1] \u00b7 [1X1] = [NX1]\ny_pred = np.dot(X_train, W) + b\nprint (f\"y_pred: {y_pred.shape}\")\n</code></pre></p> <pre>\ny_pred: (35, 1)\n</pre>"},{"location":"courses/foundations/linear-regression/#loss","title":"Loss","text":"<p><code>Step 3</code>: Compare the predictions \\(\\hat{y}\\) with the actual target values \\(y\\) using the objective (cost) function to determine the loss \\(J\\). A common objective function for linear regression is mean squared error (MSE). This function calculates the difference between the predicted and target values and squares it.</p> \\[ J(\\theta) = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2  = \\frac{1}{N}\\sum_i (y_i - X_iW)^2 \\] <p>bias term (\\(b\\)) excluded to avoid crowding the notations</p> <pre><code># Loss\nN = len(y_train)\nloss = (1/N) * np.sum((y_train - y_pred)**2)\nprint (f\"loss: {loss:.2f}\")\n</code></pre> <pre>\nloss: 0.99\n</pre>"},{"location":"courses/foundations/linear-regression/#gradients","title":"Gradients","text":"<p><code>Step 4</code>: Calculate the gradient of loss \\(J(\\theta)\\) w.r.t to the model weights.</p> \\[ \u2192 \\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW) X_i = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i) X_i \\] \\[ \u2192 \\frac{\\partial{J}}{\\partial{b}} = -\\frac{2}{N} \\sum_i (y_i - X_iW)1 = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i)1 \\] <pre><code># Backpropagation\ndW = -(2/N) * np.sum((y_train - y_pred) * X_train)\ndb = -(2/N) * np.sum((y_train - y_pred) * 1)\n</code></pre> <p>The gradient is the derivative, or the rate of change of a function. It's a vector that points in the direction of greatest increase of a function. For example the gradient of our loss function (\\(J\\)) with respect to our weights (\\(W\\)) will tell us how to change \\(W\\) so we can maximize \\(J\\). However, we want to minimize our loss so we subtract the gradient from \\(W\\).</p>"},{"location":"courses/foundations/linear-regression/#update-weights","title":"Update weights","text":"<p><code>Step 5</code>: Update the weights \\(W\\) using a small learning rate \\(\\alpha\\).</p> \\[ W = W - \\alpha\\frac{\\partial{J}}{\\partial{W}} \\] \\[ b = b - \\alpha\\frac{\\partial{J}}{\\partial{b}} \\] <p><pre><code>LEARNING_RATE = 1e-1\n</code></pre> <pre><code># Update weights\nW += -LEARNING_RATE * dW\nb += -LEARNING_RATE * db\n</code></pre></p> <p>The learning rate \\(\\alpha\\) is a way to control how much we update the weights by. If we choose a small learning rate, it may take a long time for our model to train. However, if we choose a large learning rate, we may overshoot and our training will never converge. The specific learning rate depends on our data and the type of models we use but it's typically good to explore in the range of \\([1e^{-8}, 1e^{-1}]\\). We'll explore learning rate update strategies in later lessons.</p>"},{"location":"courses/foundations/linear-regression/#training","title":"Training","text":"<p><code>Step 6</code>: Repeat steps 2 - 5 to minimize the loss and train the model. <pre><code>NUM_EPOCHS = 100\n</code></pre> <pre><code># Initialize random weights\nW = 0.01 * np.random.randn(INPUT_DIM, OUTPUT_DIM)\nb = np.zeros((1, ))\n\n# Training loop\nfor epoch_num in range(NUM_EPOCHS):\n\n    # Forward pass [NX1] \u00b7 [1X1] = [NX1]\n    y_pred = np.dot(X_train, W) + b\n\n    # Loss\n    loss = (1/len(y_train)) * np.sum((y_train - y_pred)**2)\n\n    # Show progress\n    if epoch_num%10 == 0:\n        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}\")\n\n    # Backpropagation\n    dW = -(2/N) * np.sum((y_train - y_pred) * X_train)\n    db = -(2/N) * np.sum((y_train - y_pred) * 1)\n\n    # Update weights\n    W += -LEARNING_RATE * dW\n    b += -LEARNING_RATE * db\n</code></pre></p> <pre>\nEpoch: 0, loss: 0.990\nEpoch: 10, loss: 0.039\nEpoch: 20, loss: 0.028\nEpoch: 30, loss: 0.028\nEpoch: 40, loss: 0.028\nEpoch: 50, loss: 0.028\nEpoch: 60, loss: 0.028\nEpoch: 70, loss: 0.028\nEpoch: 80, loss: 0.028\nEpoch: 90, loss: 0.028\n</pre> <p>To keep the code simple, we're not calculating and displaying the validation loss after each epoch here. But in later lessons, the performance on the validation set will be crucial in influencing the learning process (learning rate, when to stop training, etc.).</p>"},{"location":"courses/foundations/linear-regression/#evaluation","title":"Evaluation","text":"<p>Now we're ready to see how well our trained model will perform on our test (hold-out) data split. This will be our best measure on how well the model would perform on the real world, given that our dataset's distribution is close to unseen data. <pre><code># Predictions\npred_train = W*X_train + b\npred_test = W*X_test + b\n</code></pre> <pre><code># Train and test MSE\ntrain_mse = np.mean((y_train - pred_train) ** 2)\ntest_mse = np.mean((y_test - pred_test) ** 2)\nprint (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")\n</code></pre></p> <pre>\ntrain_MSE: 0.03, test_MSE: 0.01\n</pre> <pre><code># Figure size\nplt.figure(figsize=(15,5))\n\n# Plot train data\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplt.scatter(X_train, y_train, label=\"y_train\")\nplt.plot(X_train, pred_train, color=\"red\", linewidth=1, linestyle=\"-\", label=\"model\")\nplt.legend(loc=\"lower right\")\n\n# Plot test data\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplt.scatter(X_test, y_test, label='y_test')\nplt.plot(X_test, pred_test, color=\"red\", linewidth=1, linestyle=\"-\", label=\"model\")\nplt.legend(loc=\"lower right\")\n\n# Show plots\nplt.show()\n</code></pre>"},{"location":"courses/foundations/linear-regression/#interpretability","title":"Interpretability","text":"<p>Since we standardized our inputs and outputs, our weights were fit to those standardized values. So we need to unstandardize our weights so we can compare it to our true weight (3.5).</p> <p>Note that both \\(X\\) and \\(y\\) were standardized.</p> \\[ \\hat{y}_{scaled} = b_{scaled} + \\sum_{j=1}^{k}{W_{scaled}}_j{x_{scaled}}_j \\] <p> Variable Description \\(y_{scaled}\\) \\(\\frac{\\hat{y} - \\bar{y}}{\\sigma_y}\\) \\(x_{scaled}\\) \\(\\frac{x_j - \\bar{x}_j}{\\sigma_j}\\) <p></p> \\[ \\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = b_{scaled} + \\sum_{j=1}^{k}{W_{scaled}}_j\\frac{x_j - \\bar{x}_j}{\\sigma_j} \\] \\[ \\hat{y}_{scaled} = \\frac{\\hat{y}_{unscaled} - \\bar{y}}{\\sigma_y} = {b_{scaled}} + \\sum_{j=1}^{k} {W_{scaled}}_j (\\frac{x_j - \\bar{x}_j}{\\sigma_j}) \\] \\[ \\hat{y}_{unscaled} = b_{scaled}\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} {W_{scaled}}_j(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j + \\sum_{j=1}^{k}{W_{scaled}}_j(\\frac{\\sigma_y}{\\sigma_j})x_j \\] <p>In the expression above, we can see the expression:</p> \\[ \\hat{y}_{unscaled} = b_{unscaled} + W_{unscaled}x \\] <p> Variable Description \\(W_{unscaled}\\) \\({W}_j(\\frac{\\sigma_y}{\\sigma_j})\\) \\(b_{unscaled}\\) \\(b_{scaled}\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} {W}_j(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j\\) <p></p> <p>By substituting \\(W_{unscaled}\\) in \\(b_{unscaled}\\), it now becomes:</p> \\[ b_{unscaled} = b_{scaled}\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} W_{unscaled}\\bar{x}_j \\] <pre><code># Unscaled weights\nW_unscaled = W * (y_std/X_std)\nb_unscaled = b * y_std + y_mean - np.sum(W_unscaled*X_mean)\nprint (\"[actual] y = 3.5X + noise\")\nprint (f\"[model] y_hat = {W_unscaled[0][0]:.1f}X + {b_unscaled[0]:.1f}\")\n</code></pre> <pre>\n[actual] y = 3.5X + noise\n[model] y_hat = 3.4X + 7.8\n</pre>"},{"location":"courses/foundations/linear-regression/#pytorch","title":"PyTorch","text":"<p>Now that we've implemented linear regression with Numpy, let's do the same with PyTorch. <pre><code>import torch\n</code></pre> <pre><code># Set seed for reproducibility\ntorch.manual_seed(SEED)\n</code></pre></p> <pre>"},{"location":"courses/foundations/linear-regression/#split-data_1","title":"Split data","text":"<p>This time, instead of splitting data using indices, let's use scikit-learn's built in <code>train_test_split</code> function.\n<pre><code>from sklearn.model_selection import train_test_split\n</code></pre>\n<pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre>\n<pre><code># Split (train)\nX_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE)\n</code></pre>\n<pre><code>print (f\"train: {len(X_train)} ({(len(X_train) / len(X)):.2f})\\n\"\n       f\"remaining: {len(X_)} ({(len(X_) / len(X)):.2f})\")\n</code></pre></p>\n<pre>\ntrain: 35 (0.70)\nremaining: 15 (0.30)\n</pre>\n<p><pre><code># Split (test)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_, y_, train_size=0.5)\n</code></pre>\n<pre><code>print(f\"train: {len(X_train)} ({len(X_train)/len(X):.2f})\\n\"\n      f\"val: {len(X_val)} ({len(X_val)/len(X):.2f})\\n\"\n      f\"test: {len(X_test)} ({len(X_test)/len(X):.2f})\")\n</code></pre></p>\n<pre>\ntrain: 35 (0.70)\nval: 7 (0.14)\ntest: 8 (0.16)\n</pre>"},{"location":"courses/foundations/linear-regression/#standardize-data_1","title":"Standardize data","text":"<p>This time we'll use scikit-learn's <code>StandardScaler</code> to standardize our data.</p>\n<p><pre><code>from sklearn.preprocessing import StandardScaler\n</code></pre>\n<pre><code># Standardize the data (mean=0, std=1) using training data\nX_scaler = StandardScaler().fit(X_train)\ny_scaler = StandardScaler().fit(y_train)\n</code></pre>\n<pre><code># Apply scaler on training and test data\nX_train = X_scaler.transform(X_train)\ny_train = y_scaler.transform(y_train).ravel().reshape(-1, 1)\nX_val = X_scaler.transform(X_val)\ny_val = y_scaler.transform(y_val).ravel().reshape(-1, 1)\nX_test = X_scaler.transform(X_test)\ny_test = y_scaler.transform(y_test).ravel().reshape(-1, 1)\n</code></pre>\n<pre><code># Check (means should be ~0 and std should be ~1)\nprint (f\"mean: {np.mean(X_test, axis=0)[0]:.1f}, std: {np.std(X_test, axis=0)[0]:.1f}\")\nprint (f\"mean: {np.mean(y_test, axis=0)[0]:.1f}, std: {np.std(y_test, axis=0)[0]:.1f}\")\n</code></pre></p>\n<pre>\nmean: -0.3, std: 0.7\nmean: -0.3, std: 0.6\n</pre>"},{"location":"courses/foundations/linear-regression/#weights_1","title":"Weights","text":"<p>We will be using PyTorch's Linear layers in our MLP implementation. These layers will act as out weights (and biases).</p>\n\\[ z = XW \\]\n<p><pre><code>from torch import nn\n</code></pre>\n<pre><code># Inputs\nN = 3 # num samples\nx = torch.randn(N, INPUT_DIM)\nprint (x.shape)\nprint (x.numpy())\n</code></pre></p>\n<pre>\ntorch.Size([3, 1])\n[[ 0.04613046]\n [ 0.40240282]\n [-1.0115291 ]]\n</pre>\n<pre><code># Weights\nm = nn.Linear(INPUT_DIM, OUTPUT_DIM)\nprint (m)\nprint (f\"weights ({m.weight.shape}): {m.weight[0][0]}\")\nprint (f\"bias ({m.bias.shape}): {m.bias[0]}\")\n</code></pre>\n<pre>\nLinear(in_features=1, out_features=1, bias=True)\nweights (torch.Size([1, 1])): 0.35\nbias (torch.Size([1])): -0.34\n</pre>\n<pre><code># Forward pass\nz = m(x)\nprint (z.shape)\nprint (z.detach().numpy())\n</code></pre>\n<pre>\ntorch.Size([3, 1])\n[[-0.32104054]\n [-0.19719592]\n [-0.68869597]]\n</pre>"},{"location":"courses/foundations/linear-regression/#model_1","title":"Model","text":"\\[ \\hat{y} = XW + b \\]\n<p><pre><code>class LinearRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegression, self).__init__()\n        self.fc1 = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x_in):\n        y_pred = self.fc1(x_in)\n        return y_pred\n</code></pre>\n<pre><code># Initialize model\nmodel = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\nprint (model.named_parameters)\n</code></pre></p>\n<pre>\nModel:\n&lt;bound method Module.named_parameters of LinearRegression(\n  (fc1): Linear(in_features=1, out_features=1, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/linear-regression/#loss_1","title":"Loss","text":"<p>This time we're using PyTorch's loss functions, specifically <code>MSELoss</code>.</p>\n<pre><code>loss_fn = nn.MSELoss()\ny_pred = torch.Tensor([0., 0., 1., 1.])\ny_true =  torch.Tensor([1., 1., 1., 0.])\nloss = loss_fn(y_pred, y_true)\nprint(\"Loss: \", loss.numpy())\n</code></pre>\n<pre>\nLoss:  0.75\n</pre>"},{"location":"courses/foundations/linear-regression/#optimizer","title":"Optimizer","text":"<p>When we implemented linear regression with just NumPy, we used batch gradient descent to update our weights (used entire training set). But there are actually many different gradient descent optimization algorithms to choose from and it depends on the situation. However, the ADAM optimizer has become a standard algorithm for most cases.</p>\n<p><pre><code>from torch.optim import Adam\n</code></pre>\n<pre><code># Optimizer\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n</code></pre></p>"},{"location":"courses/foundations/linear-regression/#training_1","title":"Training","text":"<p><pre><code># Convert data to tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.Tensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.Tensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.Tensor(y_test)\n</code></pre>\n<pre><code># Training\nfor epoch in range(NUM_EPOCHS):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%20==0:\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}\")\n</code></pre></p>\n<pre>\nEpoch: 0 | loss: 0.22\nEpoch: 20 | loss: 0.03\nEpoch: 40 | loss: 0.02\nEpoch: 60 | loss: 0.02\nEpoch: 80 | loss: 0.02\n</pre>"},{"location":"courses/foundations/linear-regression/#evaluation_1","title":"Evaluation","text":"<p>Now we're ready to evaluate our trained model.</p>\n<p><pre><code># Predictions\npred_train = model(X_train)\npred_test = model(X_test)\n</code></pre>\n<pre><code># Performance\ntrain_error = loss_fn(pred_train, y_train)\ntest_error = loss_fn(pred_test, y_test)\nprint(f\"train_error: {train_error:.2f}\")\nprint(f\"test_error: {test_error:.2f}\")\n</code></pre></p>\n<pre>\ntrain_error: 0.02\ntest_error: 0.01\n</pre>\n\n<p>Since we only have one feature, it's easy to visually inspect the model.\n<pre><code># Figure size\nplt.figure(figsize=(15,5))\n\n# Plot train data\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplt.scatter(X_train, y_train, label=\"y_train\")\nplt.plot(X_train, pred_train.detach().numpy(), color=\"red\", linewidth=1, linestyle=\"-\", label=\"model\")\nplt.legend(loc=\"lower right\")\n\n# Plot test data\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplt.scatter(X_test, y_test, label='y_test')\nplt.plot(X_test, pred_test.detach().numpy(), color=\"red\", linewidth=1, linestyle=\"-\", label=\"model\")\nplt.legend(loc=\"lower right\")\n\n# Show plots\nplt.show()\n</code></pre></p>"},{"location":"courses/foundations/linear-regression/#inference","title":"Inference","text":"<p>After training a model, we can use it to predict on new data.</p>\n<pre><code># Feed in your own inputs\nsample_indices = [10, 15, 25]\nX_infer = np.array(sample_indices, dtype=np.float32)\nX_infer = torch.Tensor(X_scaler.transform(X_infer.reshape(-1, 1)))\n</code></pre>\n<p>Recall that we need to unstandardize our predictions.</p>\n\\[ \\hat{y}_{scaled} = \\frac{\\hat{y} - \\mu_{\\hat{y}}}{\\sigma_{\\hat{y}}} \\]\n\\[ \\hat{y} = \\hat{y}_{scaled} * \\sigma_{\\hat{y}} + \\mu_{\\hat{y}} \\]\n<pre><code># Unstandardize predictions\npred_infer = model(X_infer).detach().numpy() * np.sqrt(y_scaler.var_) + y_scaler.mean_\nfor i, index in enumerate(sample_indices):\n    print (f\"{df.iloc[index][\"y\"]:.2f} (actual) \u2192 {pred_infer[i][0]:.2f} (predicted)\")\n</code></pre>\n<pre>\n35.73 (actual) \u2192 42.11 (predicted)\n59.34 (actual) \u2192 59.17 (predicted)\n97.04 (actual) \u2192 93.30 (predicted)\n</pre>"},{"location":"courses/foundations/linear-regression/#interpretability_1","title":"Interpretability","text":"<p>Linear regression offers the great advantage of being highly interpretable. Each feature has a coefficient which signifies its importance/impact on the output variable y. We can interpret our coefficient as follows: by increasing X by 1 unit, we increase y by \\(W\\) (~3.65) units.\n<pre><code># Unstandardize coefficients\nW = model.fc1.weight.data.numpy()[0][0]\nb = model.fc1.bias.data.numpy()[0]\nW_unscaled = W * (y_scaler.scale_/X_scaler.scale_)\nb_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)\nprint (\"[actual] y = 3.5X + noise\")\nprint (f\"[model] y_hat = {W_unscaled[0]:.1f}X + {b_unscaled[0]:.1f}\")\n</code></pre></p>\n<pre>\n[actual] y = 3.5X + noise\n[model] y_hat = 3.4X + 8.0\n</pre>"},{"location":"courses/foundations/linear-regression/#regularization","title":"Regularization","text":"<p>Regularization helps decrease overfitting. Below is <code>L2</code> regularization (ridge regression). There are many forms of regularization but they all work to reduce overfitting in our models. With <code>L2</code> regularization, we are penalizing large weight values by decaying them because having large weights will lead to preferential bias with the respective inputs and we want the model to work with all the inputs and not just a select few. There are also other types of regularization like <code>L1</code> (lasso regression) which is useful for creating sparse models where some feature coefficients are zeroed out, or elastic which combines <code>L1</code> and <code>L2</code> penalties.</p>\n<p>Regularization is not just for linear regression. You can use it to regularize any model's weights including the ones we will look at in future lessons.</p>\n\\[ J(\\theta) = \\frac{1}{2}\\sum_{i}(X_iW - y_i)^2 + \\frac{\\lambda}{2}W^TW \\]\n\\[ \\frac{\\partial{J}}{\\partial{W}}  = X (\\hat{y} - y) + \\lambda W \\]\n\\[ W = W - \\alpha\\frac{\\partial{J}}{\\partial{W}} \\]\n<p>\nVariable\nDescription\n\\(\\lambda\\)\nregularization coefficient\n\\(\\alpha\\)\nlearning rate\n<p></p>\n<p>In PyTorch, we can add L2 regularization by adjusting our optimizer. The Adam optimizer has a <code>weight_decay</code> parameter which to control the L2 penalty.</p>\n<p><pre><code>L2_LAMBDA = 1e-2\n</code></pre>\n<pre><code># Initialize model\nmodel = LinearRegression(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\n</code></pre>\n<pre><code># Optimizer (w/ L2 regularization)\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n</code></pre>\n<pre><code># Training\nfor epoch in range(NUM_EPOCHS):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%20==0:\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}\")\n</code></pre></p>\n<pre>\nEpoch: 0 | loss: 2.20\nEpoch: 20 | loss: 0.06\nEpoch: 40 | loss: 0.03\nEpoch: 60 | loss: 0.02\nEpoch: 80 | loss: 0.02\n</pre>\n<p><pre><code># Predictions\npred_train = model(X_train)\npred_test = model(X_test)\n</code></pre>\n<pre><code># Performance\ntrain_error = loss_fn(pred_train, y_train)\ntest_error = loss_fn(pred_test, y_test)\nprint(f\"train_error: {train_error:.2f}\")\nprint(f\"test_error: {test_error:.2f}\")\n</code></pre></p>\n<pre>\ntrain_error: 0.02\ntest_error: 0.01\n</pre>\n\n<p>Regularization didn't make a difference in performance with this specific example because our data is generated from a perfect linear equation but for large realistic data, regularization can help our model generalize well.</p>\n<p>To cite this content, please use:</p>\n<pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Linear regression - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/logistic-regression/","title":"Logistic Regression","text":""},{"location":"courses/foundations/logistic-regression/#overview","title":"Overview","text":"<p>Logistic regression is an extension on linear regression (both are generalized linear methods). We will still learn to model a line (plane) that models \\(y\\) given \\(X\\). Except now we are dealing with classification problems as opposed to regression problems so we'll be predicting probability distributions as opposed to discrete values. We'll be using the softmax operation to normalize our logits (\\(XW\\)) to derive probabilities.</p> <p>Our goal is to learn a logistic model \\(\\hat{y}\\) that models \\(y\\) given \\(X\\).</p> \\[ \\hat{y} = \\frac{e^{XW_y}}{\\sum_j e^{XW}} \\] <p> Variable Description \\(N\\) total numbers of samples \\(C\\) number of classes \\(\\hat{y}\\) predictions \\(\\in \\mathbb{R}^{NXC}\\) \\(X\\) inputs \\(\\in \\mathbb{R}^{NXD}\\) \\(W\\) weights \\(\\in \\mathbb{R}^{DXC}\\) <p>(*) bias term (\\(b\\)) excluded to avoid crowding the notations</p> <p></p> <p>This function is known as the multinomial logistic regression or the softmax classifier. The softmax classifier will use the linear equation (\\(z=XW\\)) and normalize it (using the softmax function) to produce the probability for class y given the inputs.</p> <ul> <li>Objectives:<ul> <li>Predict the probability of class \\(y\\) given the inputs \\(X\\). The softmax classifier normalizes the linear outputs to determine class probabilities.</li> </ul> </li> <li>Advantages:<ul> <li>Can predict class probabilities given a set on inputs.</li> </ul> </li> <li>Disadvantages:<ul> <li>Sensitive to outliers since objective is to minimize cross entropy loss. Support vector machines (SVMs) are a good alternative to counter outliers.</li> </ul> </li> <li>Miscellaneous:<ul> <li>Softmax classifier is widely in neural network architectures as the last layer since it produces class probabilities.</li> </ul> </li> </ul>"},{"location":"courses/foundations/logistic-regression/#set-up","title":"Set up","text":"<p>We'll set our seeds for reproducibility. <pre><code>import numpy as np\nimport random\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(SEED)\nrandom.seed(SEED)\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#load-data","title":"Load data","text":"<p>We'll used some synthesized data to train our models on. The task is to determine whether a tumor will be benign (harmless) or malignant (harmful) based on leukocyte (white blood cells) count and blood pressure. Note that this is a synthetic dataset that has no clinical relevance.</p> <p><pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(SEED)\n</code></pre> <pre><code># Read from CSV to Pandas DataFrame\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tumors.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> leukocyte_count blood_pressure tumor_class 0 15.335860 14.637535 benign 1 9.857535 14.518942 malignant 2 17.632579 15.869585 benign 3 18.369174 14.774547 benign 4 14.509367 15.892224 malignant <p><pre><code># Define X and y\nX = df[[\"leukocyte_count\", \"blood_pressure\"]].values\ny = df[\"tumor_class\"].values\n</code></pre> <pre><code># Plot data\ncolors = {\"benign\": \"red\", \"malignant\": \"blue\"}\nplt.scatter(X[:, 0], X[:, 1], c=[colors[_y] for _y in y], s=25, edgecolors=\"k\")\nplt.xlabel(\"leukocyte count\")\nplt.ylabel(\"blood pressure\")\nplt.legend([\"malignant\", \"benign\"], loc=\"upper right\")\nplt.show()\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#split-data","title":"Split data","text":"<p>We want to split our dataset so that each of the three splits has the same distribution of classes so that we can train and evaluate properly. We can easily achieve this by telling scikit-learn's <code>train_test_split</code> function what to <code>stratify</code> on. <pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (700, 2), y_train: (700,)\nX_val: (150, 2), y_val: (150,)\nX_test: (150, 2), y_test: (150,)\nSample point: [11.5066204  15.98030799] \u2192 malignant\n</pre> <p>Now let's see how many samples per class each data split has: <pre><code># Overall class distribution\nclass_counts = dict(collections.Counter(y))\nprint (f\"Classes: {class_counts}\")\nprint (f'm:b = {class_counts[\"malignant\"]/class_counts[\"benign\"]:.2f}')\n</code></pre></p> <pre>\nClasses: {\"malignant\": 611, \"benign\": 389}\nm:b = 1.57\n</pre> <pre><code># Per data split class distribution\ntrain_class_counts = dict(collections.Counter(y_train))\nval_class_counts = dict(collections.Counter(y_val))\ntest_class_counts = dict(collections.Counter(y_test))\nprint (f'train m:b = {train_class_counts[\"malignant\"]/train_class_counts[\"benign\"]:.2f}')\nprint (f'val m:b = {val_class_counts[\"malignant\"]/val_class_counts[\"benign\"]:.2f}')\nprint (f'test m:b = {test_class_counts[\"malignant\"]/test_class_counts[\"benign\"]:.2f}')\n</code></pre> <pre>\ntrain m:b = 1.57\nval m:b = 1.54\ntest m:b = 1.59\n</pre>"},{"location":"courses/foundations/logistic-regression/#label-encoding","title":"Label encoding","text":"<p>You'll notice that our class labels are text. We need to encode them into integers so we can use them in our models. We could scikit-learn's <code>LabelEncoder</code> to do this but we're going to write our own simple label encoder class so we can see what's happening under the hood.</p> <p><pre><code>import itertools\n</code></pre> <pre><code>class LabelEncoder(object):\n\"\"\"Label encoder for tag labels.\"\"\"\n    def __init__(self, class_to_index={}):\n        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n\n    def __len__(self):\n        return len(self.class_to_index)\n\n    def __str__(self):\n        return f\"&lt;LabelEncoder(num_classes={len(self)})&gt;\"\n\n    def fit(self, y):\n        classes = np.unique(y)\n        for i, class_ in enumerate(classes):\n            self.class_to_index[class_] = i\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n        return self\n\n    def encode(self, y):\n        encoded = np.zeros((len(y)), dtype=int)\n        for i, item in enumerate(y):\n            encoded[i] = self.class_to_index[item]\n        return encoded\n\n    def decode(self, y):\n        classes = []\n        for i, item in enumerate(y):\n            classes.append(self.index_to_class[item])\n        return classes\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {'class_to_index': self.class_to_index}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Fit\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nlabel_encoder.class_to_index\n</code></pre></p> <pre>\n{\"benign\": 0, \"malignant\": 1}\n</pre> <pre><code># Encoder\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.encode(y_train)\ny_val = label_encoder.encode(y_val)\ny_test = label_encoder.encode(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\nprint (f\"decoded: {label_encoder.decode([y_train[0]])}\")\n</code></pre> <pre>\ny_train[0]: malignant\ny_train[0]: 1\ndecoded: [\"malignant\"]\n</pre> <p>We also want to calculate our class weights, which are useful for weighting the loss function during training. It tells the model to focus on samples from an under-represented class. The loss section below will show how to incorporate these weights. <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre></p> <pre>\ncounts: [272 428]\nweights: {0: 0.003676470588235294, 1: 0.002336448598130841}\n</pre>"},{"location":"courses/foundations/logistic-regression/#standardize-data","title":"Standardize data","text":"<p>We need to standardize our data (zero mean and unit variance) so a specific feature's magnitude doesn't affect how the model learns its weights. We're only going to standardize the inputs X because our outputs y are class values. <pre><code>from sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code># Standardize the data (mean=0, std=1) using training data\nX_scaler = StandardScaler().fit(X_train)\n</code></pre> <pre><code># Apply scaler on training and test data (don't standardize outputs for classification)\nX_train = X_scaler.transform(X_train)\nX_val = X_scaler.transform(X_val)\nX_test = X_scaler.transform(X_test)\n</code></pre> <pre><code># Check (means should be ~0 and std should be ~1)\nprint (f\"X_test[0]: mean: {np.mean(X_test[:, 0], axis=0):.1f}, std: {np.std(X_test[:, 0], axis=0):.1f}\")\nprint (f\"X_test[1]: mean: {np.mean(X_test[:, 1], axis=0):.1f}, std: {np.std(X_test[:, 1], axis=0):.1f}\")\n</code></pre></p> <pre>\nX_test[0]: mean: 0.0, std: 1.0\nX_test[1]: mean: 0.1, std: 1.0\n</pre>"},{"location":"courses/foundations/logistic-regression/#numpy","title":"NumPy","text":"<p>Now that we have our data prepared, we'll first implement logistic regression using just NumPy. This will let us really understand the underlying operations. It's normal to find the math and code in this section slightly complex. You can still read each of the steps to build intuition for when we implement this using PyTorch.</p> <p>Our goal is to learn a logistic model \\(\\hat{y}\\) that models \\(y\\) given \\(X\\).</p> \\[ \\hat{y} = \\frac{e^{XW_y}}{\\sum_j e^{XW}} \\] <p>We are going to use multinomial logistic regression even though our task only involves two classes because you can generalize the softmax classifier to any number of classes.</p>"},{"location":"courses/foundations/logistic-regression/#initialize-weights","title":"Initialize weights","text":"<p><code>Step 1</code>: Randomly initialize the model's weights \\(W\\). <pre><code>INPUT_DIM = X_train.shape[1] # X is 2-dimensional\nNUM_CLASSES = len(label_encoder.classes) # y has two possibilities (benign or malignant)\n</code></pre> <pre><code># Initialize random weights\nW = 0.01 * np.random.randn(INPUT_DIM, NUM_CLASSES)\nb = np.zeros((1, NUM_CLASSES))\nprint (f\"W: {W.shape}\")\nprint (f\"b: {b.shape}\")\n</code></pre></p> <pre>\nW: (2, 2)\nb: (1, 2)\n</pre>"},{"location":"courses/foundations/logistic-regression/#model","title":"Model","text":"<p><code>Step 2</code>: Feed inputs \\(X\\) into the model to receive the logits (\\(z=XW\\)). Apply the softmax operation on the logits to get the class probabilities \\(\\hat{y}\\) in one-hot encoded form. For example, if there are three classes, the predicted class probabilities could look like [0.3, 0.3, 0.4].</p> \\[ \\hat{y} = softmax(z) = softmax(XW) = \\frac{e^{XW_y}}{\\sum_j e^{XW}} \\] <pre><code># Forward pass [NX2] \u00b7 [2X2] + [1,2] = [NX2]\nlogits = np.dot(X_train, W) + b\nprint (f\"logits: {logits.shape}\")\nprint (f\"sample: {logits[0]}\")\n</code></pre> <pre>\nlogits: (722, 2)\nsample: [0.01817675 0.00635562]\n</pre> <pre><code># Normalization via softmax to obtain class probabilities\nexp_logits = np.exp(logits)\ny_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\nprint (f\"y_hat: {y_hat.shape}\")\nprint (f\"sample: {y_hat[0]}\")\n</code></pre> <pre>\ny_hat: (722, 2)\nsample: [0.50295525 0.49704475]\n</pre>"},{"location":"courses/foundations/logistic-regression/#loss","title":"Loss","text":"<p><code>Step 3</code>: Compare the predictions \\(\\hat{y}\\) (ex.  [0.3, 0.3, 0.4]) with the actual target values \\(y\\) (ex. class 2 would look like [0, 0, 1]) with the objective (cost) function to determine loss \\(J\\). A common objective function for logistics regression is cross-entropy loss.</p> \\[ J(\\theta) = - \\sum_i ln(\\hat{y_i}) = - \\sum_i ln (\\frac{e^{X_iW_y}}{\\sum_j e^{X_iW}}) \\] <p>bias term (\\(b\\)) excluded to avoid crowding the notations</p> <pre><code># Loss\ncorrect_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\nloss = np.sum(correct_class_logprobs) / len(y_train)\nprint (f\"loss: {loss:.2f}\")\n</code></pre> <pre>\nloss: 0.69\n</pre>"},{"location":"courses/foundations/logistic-regression/#gradients","title":"Gradients","text":"<p><code>Step 4</code>: Calculate the gradient of loss \\(J(\\theta)\\) w.r.t to the model weights. Let's assume that our classes are mutually exclusive (a set of inputs could only belong to one class).</p> \\[ \\frac{\\partial{J}}{\\partial{W_j}} = \\frac{\\partial{J}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{W_j}} = - \\frac{1}{\\hat{y}}\\frac{\\partial{\\hat{y}}}{\\partial{W_j}} = \\] \\[ = - \\frac{1}{\\frac{e^{XW_y}}{\\sum_j e^{XW}}}\\frac{\\sum_j e^{XW}e^{XW_y}0 - e^{XW_y}e^{XW_j}X}{(\\sum_j e^{XW})^2} = \\frac{Xe^{XW_j}}{\\sum_j e^{XW}} = X\\hat{y} \\] \\[ \\frac{\\partial{J}}{\\partial{W_y}} = \\frac{\\partial{J}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{W_y}} = - \\frac{1}{\\hat{y}}\\frac{\\partial{\\hat{y}}}{\\partial{W_y}} = \\] \\[ = - \\frac{1}{\\frac{e^{XW_y}}{\\sum_j e^{XW}}}\\frac{\\sum_j e^{XW}e^{XW_y}X - e^{W_yX}e^{XW_y}X}{(\\sum_j e^{XW})^2} = \\frac{1}{\\hat{y}}(X\\hat{y} - X\\hat{y}^2) = X(\\hat{y}-1) \\] <pre><code># Backpropagation\ndscores = y_hat\ndscores[range(len(y_hat)), y_train] -= 1\ndscores /= len(y_train)\ndW = np.dot(X_train.T, dscores)\ndb = np.sum(dscores, axis=0, keepdims=True)\n</code></pre>"},{"location":"courses/foundations/logistic-regression/#update-weights","title":"Update weights","text":"<p><code>Step 5</code>: Update the weights \\(W\\)  using a small learning rate \\(\\alpha\\). The updates will penalize the probability for the incorrect classes (j) and encourage a higher probability for the correct class (y).</p> \\[ W_j = W_j - \\alpha\\frac{\\partial{J}}{\\partial{W_j}} \\] <p><pre><code>LEARNING_RATE = 1e-1\n</code></pre> <pre><code># Update weights\nW += -LEARNING_RATE * dW\nb += -LEARNING_RATE * db\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#training","title":"Training","text":"<p><code>Step 6</code>: Repeat steps 2 - 5 to minimize the loss and train the model. <pre><code>NUM_EPOCHS = 50\n</code></pre> <pre><code># Initialize random weights\nW = 0.01 * np.random.randn(INPUT_DIM, NUM_CLASSES)\nb = np.zeros((1, NUM_CLASSES))\n</code></pre> <pre><code># Training loop\nfor epoch_num in range(NUM_EPOCHS):\n\n    # Forward pass [NX2] \u00b7 [2X2] = [NX2]\n    logits = np.dot(X_train, W) + b\n\n    # Normalization via softmax to obtain class probabilities\n    exp_logits = np.exp(logits)\n    y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n    # Loss\n    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\n    loss = np.sum(correct_class_logprobs) / len(y_train)\n\n    # show progress\n    if epoch_num%10 == 0:\n        # Accuracy\n        y_pred = np.argmax(logits, axis=1)\n        accuracy =  np.mean(np.equal(y_train, y_pred))\n        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n\n    # Backpropagation\n    dscores = y_hat\n    dscores[range(len(y_hat)), y_train] -= 1\n    dscores /= len(y_train)\n    dW = np.dot(X_train.T, dscores)\n    db = np.sum(dscores, axis=0, keepdims=True)\n\n    # Update weights\n    W += -LEARNING_RATE * dW\n    b += -LEARNING_RATE * db\n</code></pre></p> <pre>\nEpoch: 0, loss: 0.684, accuracy: 0.889\nEpoch: 10, loss: 0.447, accuracy: 0.978\nEpoch: 20, loss: 0.348, accuracy: 0.978\nEpoch: 30, loss: 0.295, accuracy: 0.981\nEpoch: 40, loss: 0.260, accuracy: 0.981\n</pre>"},{"location":"courses/foundations/logistic-regression/#evaluation","title":"Evaluation","text":"<p>Now we're ready to evaluate our trained model on our test (hold-out) data split. <pre><code>class LogisticRegressionFromScratch():\n    def predict(self, x):\n        logits = np.dot(x, W) + b\n        exp_logits = np.exp(logits)\n        y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        return y_hat\n</code></pre> <pre><code># Evaluation\nmodel = LogisticRegressionFromScratch()\nlogits_train = model.predict(X_train)\npred_train = np.argmax(logits_train, axis=1)\nlogits_test = model.predict(X_test)\npred_test = np.argmax(logits_test, axis=1)\n</code></pre> <pre><code># Training and test accuracy\ntrain_acc =  np.mean(np.equal(y_train, pred_train))\ntest_acc = np.mean(np.equal(y_test, pred_test))\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\n</code></pre></p> <pre>\ntrain acc: 0.98, test acc: 0.94\n</pre> <p><pre><code>def plot_multiclass_decision_boundary(model, X, y, savefig_fp=None):\n\"\"\"Plot the multiclass decision boundary for a model that accepts 2D inputs.\n    Credit: https://cs231n.github.io/neural-networks-case-study/\n\n    Arguments:\n        model {function} -- trained model with function model.predict(x_in).\n        X {numpy.ndarray} -- 2D inputs with shape (N, 2).\n        y {numpy.ndarray} -- 1D outputs with shape (N,).\n    \"\"\"\n    # Axis boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101),\n                         np.linspace(y_min, y_max, 101))\n\n    # Create predictions\n    x_in = np.c_[xx.ravel(), yy.ravel()]\n    y_pred = model.predict(x_in)\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n\n    # Plot decision boundary\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    # Plot\n    if savefig_fp:\n        plt.savefig(savefig_fp, format=\"png\")\n</code></pre> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\nplt.show()\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#pytorch","title":"PyTorch","text":"<p>Now that we've implemented logistic regression with Numpy, let's do the same with PyTorch. <pre><code>import torch\n</code></pre> <pre><code># Set seed for reproducibility\ntorch.manual_seed(SEED)\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#model_1","title":"Model","text":"<p>We will be using PyTorch's Linear layers  to recreate the same model. <pre><code>from torch import nn\nimport torch.nn.functional as F\n</code></pre> <pre><code>class LogisticRegression(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(LogisticRegression, self).__init__()\n        self.fc1 = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x_in):\n        z = self.fc1(x_in)\n        return z\n</code></pre> <pre><code># Initialize model\nmodel = LogisticRegression(input_dim=INPUT_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of LogisticRegression(\n  (fc1): Linear(in_features=2, out_features=2, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/logistic-regression/#loss_1","title":"Loss","text":"<p>Our loss will be the categorical crossentropy. <pre><code>loss_fn = nn.CrossEntropyLoss()\ny_pred = torch.randn(3, NUM_CLASSES, requires_grad=False)\ny_true = torch.empty(3, dtype=torch.long).random_(NUM_CLASSES)\nprint (y_true)\nloss = loss_fn(y_pred, y_true)\nprint(f\"Loss: {loss.numpy()}\")\n</code></pre></p> <pre>\ntensor([0, 0, 1])\nLoss: 1.6113080978393555\n</pre> <p>In our case, we will also incorporate the class weights into our loss function to counter any class imbalances. <pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values()))\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#metrics","title":"Metrics","text":"<p>We'll compute accuracy as we train our model because just looking the loss value isn't super intuitive to look at. We'll look at other metrics (precision, recall, f1) in the evaluation section below. <pre><code># Accuracy\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\n</code></pre> <pre><code>y_pred = torch.Tensor([0, 0, 1])\ny_true = torch.Tensor([1, 1, 1])\nprint(\"Accuracy: {accuracy_fn(y_pred, y_true):.1f}\")\n</code></pre></p> <pre>\nAccuracy: 33.3\n</pre>"},{"location":"courses/foundations/logistic-regression/#optimizer","title":"Optimizer","text":"<p>We'll be sticking with our Adam optimizer from previous lessons. <pre><code>from torch.optim import Adam\n</code></pre> <pre><code># Optimizer\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#training_1","title":"Training","text":"<p><pre><code># Convert data to tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\n</code></pre> <pre><code># Training\nfor epoch in range(NUM_EPOCHS):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%10==0:\n        predictions = y_pred.max(dim=1)[1] # class\n        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\n</code></pre></p> <pre>\nEpoch: 0 | loss: 0.95, accuracy: 60.8\nEpoch: 10 | loss: 0.27, accuracy: 86.7\nEpoch: 20 | loss: 0.15, accuracy: 96.1\nEpoch: 30 | loss: 0.11, accuracy: 98.2\nEpoch: 40 | loss: 0.09, accuracy: 98.9\n</pre>"},{"location":"courses/foundations/logistic-regression/#evaluation_1","title":"Evaluation","text":"<p>First let's see the accuracy of our model on our test split. <pre><code>from sklearn.metrics import accuracy_score\n</code></pre> <pre><code># Predictions\npred_train = F.softmax(model(X_train), dim=1)\npred_test = F.softmax(model(X_test), dim=1)\nprint (f\"sample probability: {pred_test[0]}\")\npred_train = pred_train.max(dim=1)[1]\npred_test = pred_test.max(dim=1)[1]\nprint (f\"sample class: {pred_test[0]}\")\n</code></pre></p> <pre>\nsample probability: tensor([9.2047e-04, 9.9908e-01])\nsample class: 1\n</pre> <pre><code># Accuracy (could've also used our own accuracy function)\ntrain_acc = accuracy_score(y_train, pred_train)\ntest_acc = accuracy_score(y_test, pred_test)\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\n</code></pre> <pre>\ntrain acc: 0.98, test acc: 0.98\n</pre> <p>We can also evaluate our model on other meaningful metrics such as precision and recall. These are especially useful when there is data imbalance present.</p> \\[ \\text{accuracy} = \\frac{TP+TN}{TP+TN+FP+FN} \\] \\[ \\text{recall} = \\frac{TP}{TP+FN} \\] \\[ \\text{precision} = \\frac{TP}{TP+FP} \\] \\[ F_1 = 2 * \\frac{\\text{precision }  *  \\text{ recall}}{\\text{precision } + \\text{ recall}} \\] <p> Variable Description \\(TP\\) # of samples truly predicted to be positive and were positive \\(TN\\) # of samples truly predicted to be negative and were negative \\(FP\\) # of samples falsely predicted to be positive but were negative \\(FN\\) # of samples falsely predicted to be negative but were positive <p></p> <p><pre><code>import json\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre> <pre><code># # Performance\nperformance = get_metrics(y_true=y_test, y_pred=pred_test, classes=label_encoder.classes)\nprint (json.dumps(performance, indent=2))\n</code></pre></p> <pre>\n{\n  \"overall\": {\n    \"precision\": 0.9754098360655737,\n    \"recall\": 0.9836956521739131,\n    \"f1\": 0.9791076651655137,\n    \"num_samples\": 150.0\n  },\n  \"class\": {\n    \"benign\": {\n      \"precision\": 0.9508196721311475,\n      \"recall\": 1.0,\n      \"f1\": 0.9747899159663865,\n      \"num_samples\": 58.0\n    },\n    \"malignant\": {\n      \"precision\": 1.0,\n      \"recall\": 0.967391304347826,\n      \"f1\": 0.9834254143646408,\n      \"num_samples\": 92.0\n    }\n  }\n}\n</pre> <p>With logistic regression (extension of linear regression), the model creates a linear decision boundary that we can easily visualize. <pre><code>def plot_multiclass_decision_boundary(model, X, y):\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n    cmap = plt.cm.Spectral\n\n    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n    y_pred = F.softmax(model(X_test), dim=1)\n    _, y_pred = y_pred.max(dim=1)\n    y_pred = y_pred.reshape(xx.shape)\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n</code></pre> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\nplt.show()\n</code></pre></p>"},{"location":"courses/foundations/logistic-regression/#inference","title":"Inference","text":"<p><pre><code># Inputs for inference\nX_infer = pd.DataFrame([{\"leukocyte_count\": 13, \"blood_pressure\": 12}])\n</code></pre> <pre><code># Standardize\nX_infer = X_scaler.transform(X_infer)\nprint (X_infer)\n</code></pre></p> <pre>\n[[-0.66523095 -3.08638693]]\n</pre> <pre><code># Predict\ny_infer = F.softmax(model(torch.Tensor(X_infer)), dim=1)\nprob, _class = y_infer.max(dim=1)\nlabel = label_encoder.decode(_class.detach().numpy())[0]\nprint (f\"The probability that you have a {label} tumor is {prob.detach().numpy()[0]*100.0:.0f}%\")\n</code></pre> <pre>\nThe probability that you have a benign tumor is 93%\n</pre>"},{"location":"courses/foundations/logistic-regression/#unscaled-weights","title":"Unscaled weights","text":"<p>Note that only \\(X\\) was standardized.</p> \\[ \\hat{y}_{unscaled} = b_{scaled} + \\sum_{j=1}^{k}{W_{scaled}}_j{x_{scaled}}_j \\] <p> Variable Description \\(x_{scaled}\\) \\(\\frac{x_j - \\bar{x}_j}{\\sigma_j}\\) \\(\\hat{y}_{unscaled}\\) \\(b_{scaled} + \\sum_{j=1}^{k} {W_{scaled}}_j (\\frac{x_j - \\bar{x}_j}{\\sigma_j})\\) <p></p> \\[ \\hat{y}_{unscaled} = (b_{scaled} - \\sum_{j=1}^{k} {W_{scaled}}_j \\frac{\\bar{x}_j}{\\sigma_j}) + \\sum_{j=1}^{k} (\\frac{ {W_{scaled}}_j }{\\sigma_j})x_j \\] <p>In the expression above, we can see the expression \\(\\hat{y}_{unscaled} = W_{unscaled}x + b_{unscaled}\\), therefore:</p> <p> Variable Description \\(W_{unscaled}\\) \\(\\frac{ {W_{scaled}}_j }{\\sigma_j}\\) \\(b_{unscaled}\\) \\(b_{scaled} - \\sum_{j=1}^{k} {W_{scaled}}_j\\frac{\\bar{x}_j}{\\sigma_j}\\) <p></p> <pre><code># Unstandardize weights\nW = model.fc1.weight.data.numpy()\nb = model.fc1.bias.data.numpy()\nW_unscaled = W / X_scaler.scale_\nb_unscaled = b - np.sum((W_unscaled * X_scaler.mean_))\nprint (W_unscaled)\nprint (b_unscaled)\n</code></pre> <pre>\n[[ 0.61700419 -1.20196244]\n [-0.95664431  0.89996245]]\n [ 8.913242 10.183178]\n</pre> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Logistic regression - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/neural-networks/","title":"Neural Networks","text":""},{"location":"courses/foundations/neural-networks/#overview","title":"Overview","text":"<p>Our goal is to learn a model \\(\\hat{y}\\) that models \\(y\\) given \\(X\\) . You'll notice that neural networks are just extensions of the generalized linear methods we've seen so far but with non-linear activation functions since our data will be highly non-linear.</p> \\[ z_1 = XW_1 \\] \\[ a_1 = f(z_1) \\] \\[ z_2 = a_1W_2 \\] \\[ \\hat{y} = softmax(z_2) \\] <p> Variable Description \\(N\\) total numbers of samples \\(D\\) number of features \\(H\\) number of hidden units \\(C\\) number of classes \\(W_1\\) 1st layer weights \\(\\in \\mathbb{R}^{DXH}\\) \\(z_1\\) outputs from first layer \\(\\in \\mathbb{R}^{NXH}\\) \\(f\\) non-linear activation function \\(a_1\\) activations from first layer \\(\\in \\mathbb{R}^{NXH}\\) \\(W_2\\) 2nd layer weights \\(\\in \\mathbb{R}^{HXC}\\) \\(z_2\\) outputs from second layer \\(\\in \\mathbb{R}^{NXC}\\) \\(\\hat{y}\\) prediction \\(\\in \\mathbb{R}^{NXC}\\) <p>(*) bias term (\\(b\\)) excluded to avoid crowding the notations</p> <p></p> <ul> <li>Objective:<ul> <li>Predict the probability of class \\(y\\) given the inputs \\(X\\). Non-linearity is introduced to model the complex, non-linear data.</li> </ul> </li> <li>Advantages:<ul> <li>Can model non-linear patterns in the data really well.</li> </ul> </li> <li>Disadvantages:<ul> <li>Overfits easily.</li> <li>Computationally intensive as network increases in size.</li> <li>Not easily interpretable.</li> </ul> </li> <li>Miscellaneous:<ul> <li>Future neural network architectures that we'll see use the MLP as a modular unit for feed forward operations (affine transformation (XW) followed by a non-linear operation).</li> </ul> </li> </ul>"},{"location":"courses/foundations/neural-networks/#set-up","title":"Set up","text":"<p>We'll set our seeds for reproducibility. <pre><code>import numpy as np\nimport random\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(SEED)\nrandom.seed(SEED)\n</code></pre></p>"},{"location":"courses/foundations/neural-networks/#load-data","title":"Load data","text":"<p>I created some non-linearly separable spiral data so let's go ahead and download it for our classification task. <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n</code></pre> <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/spiral.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> X1 X2 color 0 0.106737 0.114197 c1 1 0.311513 -0.664028 c1 2 0.019870 -0.703126 c1 3 -0.054017 0.508159 c3 4 -0.127751 -0.011382 c3 <pre><code># Data shapes\nX = df[[\"X1\", \"X2\"]].values\ny = df[\"color\"].values\nprint (\"X: \", np.shape(X))\nprint (\"y: \", np.shape(y))\n</code></pre> <pre>\nX:  (1500, 2)\ny:  (1500,)\n</pre> <pre><code># Visualize data\nplt.title(\"Generated non-linear data\")\ncolors = {\"c1\": \"red\", \"c2\": \"yellow\", \"c3\": \"blue\"}\nplt.scatter(X[:, 0], X[:, 1], c=[colors[_y] for _y in y], edgecolors=\"k\", s=25)\nplt.show()\n</code></pre>"},{"location":"courses/foundations/neural-networks/#split-data","title":"Split data","text":"<p>We'll shuffle our dataset (since it's ordered by class) and then create our data splits (stratified on class). <pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (1050, 2), y_train: (1050,)\nX_val: (225, 2), y_val: (225,)\nX_test: (225, 2), y_test: (225,)\nSample point: [ 0.44688413 -0.07360876] \u2192 c1\n</pre>"},{"location":"courses/foundations/neural-networks/#label-encoding","title":"Label encoding","text":"<p>In the previous lesson we wrote our own label encoder class to see the inner functions but this time we'll use scikit-learn <code>LabelEncoder</code> class which does the same operations as ours. <pre><code>from sklearn.preprocessing import LabelEncoder\n</code></pre> <pre><code># Output vectorizer\nlabel_encoder = LabelEncoder()\n</code></pre> <pre><code># Fit on train data\nlabel_encoder = label_encoder.fit(y_train)\nclasses = list(label_encoder.classes_)\nprint (f\"classes: {classes}\")\n</code></pre></p> <pre>\nclasses: [\"c1\", \"c2\", \"c3\"]\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.transform(y_train)\ny_val = label_encoder.transform(y_val)\ny_test = label_encoder.transform(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\n</code></pre> <pre>\ny_train[0]: c1\ny_train[0]: 0\n</pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [350 350 350]\nweights: {0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857}\n</pre>"},{"location":"courses/foundations/neural-networks/#standardize-data","title":"Standardize data","text":"<p>We need to standardize our data (zero mean and unit variance) so a specific feature's magnitude doesn't affect how the model learns its weights. We're only going to standardize the inputs X because our outputs y are class values. <pre><code>from sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code># Standardize the data (mean=0, std=1) using training data\nX_scaler = StandardScaler().fit(X_train)\n</code></pre> <pre><code># Apply scaler on training and test data (don't standardize outputs for classification)\nX_train = X_scaler.transform(X_train)\nX_val = X_scaler.transform(X_val)\nX_test = X_scaler.transform(X_test)\n</code></pre> <pre><code># Check (means should be ~0 and std should be ~1)\nprint (f\"X_test[0]: mean: {np.mean(X_test[:, 0], axis=0):.1f}, std: {np.std(X_test[:, 0], axis=0):.1f}\")\nprint (f\"X_test[1]: mean: {np.mean(X_test[:, 1], axis=0):.1f}, std: {np.std(X_test[:, 1], axis=0):.1f}\")\n</code></pre></p> <pre>\nX_test[0]: mean: -0.2, std: 0.8\nX_test[1]: mean: -0.2, std: 0.9\n</pre>"},{"location":"courses/foundations/neural-networks/#linear-model","title":"Linear model","text":"<p>Before we get to our neural network, we're going to motivate non-linear activation functions by implementing a generalized linear model (logistic regression). We'll see why linear models (with linear activations) won't suffice for our dataset.</p> <p><pre><code>import torch\n</code></pre> <pre><code># Set seed for reproducibility\ntorch.manual_seed(SEED)\n</code></pre></p>"},{"location":"courses/foundations/neural-networks/#model","title":"Model","text":"<p>We'll create our linear model using one layer of weights. <pre><code>from torch import nn\nimport torch.nn.functional as F\n</code></pre> <pre><code>INPUT_DIM = X_train.shape[1] # X is 2-dimensional\nHIDDEN_DIM = 100\nNUM_CLASSES = len(classes) # 3 classes\n</code></pre> <pre><code>class LinearModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(LinearModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x_in):\n        z = self.fc1(x_in) # linear activation\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Initialize model\nmodel = LinearModel(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\n</code></pre></p> <pre>\nModel:\n&lt;bound method Module.named_parameters of LinearModel(\n  (fc1): Linear(in_features=2, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=3, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/neural-networks/#training","title":"Training","text":"<p>We'll go ahead and train our initialized model for a few epochs. <pre><code>from torch.optim import Adam\n</code></pre> <pre><code>LEARNING_RATE = 1e-2\nNUM_EPOCHS = 10\nBATCH_SIZE = 32\n</code></pre> <pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values()))\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Accuracy\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\n</code></pre> <pre><code># Optimizer\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n</code></pre> <pre><code># Convert data to tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\n</code></pre> <pre><code># Training\nfor epoch in range(NUM_EPOCHS):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%1==0:\n        predictions = y_pred.max(dim=1)[1] # class\n        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\n</code></pre></p> <pre>\nEpoch: 0 | loss: 1.13, accuracy: 51.2\nEpoch: 1 | loss: 0.90, accuracy: 50.0\nEpoch: 2 | loss: 0.78, accuracy: 55.0\nEpoch: 3 | loss: 0.74, accuracy: 54.4\nEpoch: 4 | loss: 0.73, accuracy: 54.2\nEpoch: 5 | loss: 0.74, accuracy: 54.7\nEpoch: 6 | loss: 0.75, accuracy: 54.9\nEpoch: 7 | loss: 0.75, accuracy: 54.3\nEpoch: 8 | loss: 0.76, accuracy: 54.8\nEpoch: 9 | loss: 0.76, accuracy: 55.0\n</pre>"},{"location":"courses/foundations/neural-networks/#evaluation","title":"Evaluation","text":"<p>Now let's see how well our linear model does on our non-linear spiral data. <pre><code>import json\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre> <pre><code># Predictions\ny_prob = F.softmax(model(X_test), dim=1)\nprint (f\"sample probability: {y_prob[0]}\")\ny_pred = y_prob.max(dim=1)[1]\nprint (f\"sample class: {y_pred[0]}\")\n</code></pre></p> <pre>\nsample probability: tensor([0.9306, 0.0683, 0.0012])\nsample class: 0\n</pre> <pre><code># # Performance\nperformance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)\nprint (json.dumps(performance, indent=2))\n</code></pre> <pre>\n{\n  \"overall\": {\n    \"precision\": 0.5027661968102707,\n    \"recall\": 0.49333333333333335,\n    \"f1\": 0.4942485399571228,\n    \"num_samples\": 225.0\n  },\n  \"class\": {\n    \"c1\": {\n      \"precision\": 0.5068493150684932,\n      \"recall\": 0.49333333333333335,\n      \"f1\": 0.5,\n      \"num_samples\": 75.0\n    },\n    \"c2\": {\n      \"precision\": 0.43478260869565216,\n      \"recall\": 0.5333333333333333,\n      \"f1\": 0.47904191616766467,\n      \"num_samples\": 75.0\n    },\n    \"c3\": {\n      \"precision\": 0.5666666666666667,\n      \"recall\": 0.4533333333333333,\n      \"f1\": 0.5037037037037037,\n      \"num_samples\": 75.0\n    }\n  }\n}\n</pre> <p><pre><code>def plot_multiclass_decision_boundary(model, X, y):\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n    cmap = plt.cm.Spectral\n\n    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n    y_pred = F.softmax(model(X_test), dim=1)\n    _, y_pred = y_pred.max(dim=1)\n    y_pred = y_pred.reshape(xx.shape)\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n</code></pre> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\nplt.show()\n</code></pre></p>"},{"location":"courses/foundations/neural-networks/#activation-functions","title":"Activation functions","text":"<p>Using the generalized linear method (logistic regression) yielded poor results because of the non-linearity present in our data yet our activation functions were linear. We need to use an activation function that can allow our model to learn and map the non-linearity in our data. There are many different options so let's explore a few.</p> <pre><code># Fig size\nplt.figure(figsize=(12,3))\n\n# Data\nx = torch.arange(-5., 5., 0.1)\n\n# Sigmoid activation (constrain a value between 0 and 1.)\nplt.subplot(1, 3, 1)\nplt.title(\"Sigmoid activation\")\ny = torch.sigmoid(x)\nplt.plot(x.numpy(), y.numpy())\n\n# Tanh activation (constrain a value between -1 and 1.)\nplt.subplot(1, 3, 2)\ny = torch.tanh(x)\nplt.title(\"Tanh activation\")\nplt.plot(x.numpy(), y.numpy())\n\n# Relu (clip the negative values to 0)\nplt.subplot(1, 3, 3)\ny = F.relu(x)\nplt.title(\"ReLU activation\")\nplt.plot(x.numpy(), y.numpy())\n\n# Show plots\nplt.show()\n</code></pre> <p>The ReLU activation function (\\(max(0,z)\\)) is by far the most widely used activation function for neural networks. But as you can see, each activation function has its own constraints so there are circumstances where you'll want to use different ones. For example, if we need to constrain our outputs between 0 and 1, then the sigmoid activation is the best choice.</p> <p>In some cases, using a ReLU activation function may not be sufficient. For instance, when the outputs from our neurons are mostly negative, the activation function will produce zeros. This effectively creates a \"dying ReLU\" and a recovery is unlikely. To mitigate this effect, we could lower the learning rate or use alternative ReLU activations, ex. leaky ReLU or parametric ReLU (PReLU), which have a small slope for negative neuron outputs.</p>"},{"location":"courses/foundations/neural-networks/#numpy","title":"NumPy","text":"<p>Now let's create our multilayer perceptron (MLP) which is going to be exactly like the logistic regression model but with the activation function to map the non-linearity in our data.</p> <p>It's normal to find the math and code in this section slightly complex. You can still read each of the steps to build intuition for when we implement this using PyTorch.</p> <p>Our goal is to learn a model \\(\\hat{y}\\) that models \\(y\\) given \\(X\\). You'll notice that neural networks are just extensions of the generalized linear methods we've seen so far but with non-linear activation functions since our data will be highly non-linear.</p> \\[ z_1 = XW_1 \\] \\[ a_1 = f(z_1) \\] \\[ z_2 = a_1W_2 \\] \\[ \\hat{y} = softmax(z_2) \\]"},{"location":"courses/foundations/neural-networks/#initialize-weights","title":"Initialize weights","text":"<p><code>Step 1</code>: Randomly initialize the model's weights \\(W\\) (we'll cover more effective initialization strategies later in this lesson). <pre><code># Initialize first layer's weights\nW1 = 0.01 * np.random.randn(INPUT_DIM, HIDDEN_DIM)\nb1 = np.zeros((1, HIDDEN_DIM))\nprint (f\"W1: {W1.shape}\")\nprint (f\"b1: {b1.shape}\")\n</code></pre></p> <pre>\nW1: (2, 100)\nb1: (1, 100)\n</pre>"},{"location":"courses/foundations/neural-networks/#model_1","title":"Model","text":"<p><code>Step 2</code>: Feed inputs \\(X\\) into the model to do the forward pass and receive the probabilities. First we pass the inputs into the first layer.</p> \\[ z_1 = XW_1 \\] <pre><code># z1 = [NX2] \u00b7 [2X100] + [1X100] = [NX100]\nz1 = np.dot(X_train, W1) + b1\nprint (f\"z1: {z1.shape}\")\n</code></pre> <pre>\nz1: (1050, 100)\n</pre> <p>Next we apply the non-linear activation function, ReLU (\\(max(0,z)\\)) in this case.</p> \\[ a_1 = f(z_1) \\] <pre><code># Apply activation function\na1 = np.maximum(0, z1) # ReLU\nprint (f\"a_1: {a1.shape}\")\n</code></pre> <pre>\na_1: (1050, 100)\n</pre> <p>We pass the activations to the second layer to get our logits.</p> \\[ z_2 = a_1W_2 \\] <pre><code># Initialize second layer's weights\nW2 = 0.01 * np.random.randn(HIDDEN_DIM, NUM_CLASSES)\nb2 = np.zeros((1, NUM_CLASSES))\nprint (f\"W2: {W2.shape}\")\nprint (f\"b2: {b2.shape}\")\n</code></pre> <pre>\nW2: (100, 3)\nb2: (1, 3)\n</pre> <pre><code># z2 = logits = [NX100] \u00b7 [100X3] + [1X3] = [NX3]\nlogits = np.dot(a1, W2) + b2\nprint (f\"logits: {logits.shape}\")\nprint (f\"sample: {logits[0]}\")\n</code></pre> <pre>\nlogits: (1050, 3)\nsample: [-9.85444376e-05  1.67334360e-03 -6.31717987e-04]\n</pre> <p>We'll apply the softmax function to normalize the logits and obtain class probabilities.</p> \\[ \\hat{y} = softmax(z_2) \\] <pre><code># Normalization via softmax to obtain class probabilities\nexp_logits = np.exp(logits)\ny_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\nprint (f\"y_hat: {y_hat.shape}\")\nprint (f\"sample: {y_hat[0]}\")\n</code></pre> <pre>\ny_hat: (1050, 3)\nsample: [0.33319557 0.33378647 0.33301796]\n</pre>"},{"location":"courses/foundations/neural-networks/#loss","title":"Loss","text":"<p><code>Step 3</code>: Compare the predictions \\(\\hat{y}\\) (ex.  [0.3, 0.3, 0.4]) with the actual target values \\(y\\) (ex. class 2 would look like [0, 0, 1]) with the objective (cost) function to determine loss \\(J\\). A common objective function for classification tasks is cross-entropy loss.</p> \\[ J(\\theta) = - \\sum_i ln(\\hat{y_i}) = - \\sum_i ln (\\frac{e^{X_iW_y}}{\\sum_j e^{X_iW}}) \\] <p>(*) bias term (\\(b\\)) excluded to avoid crowding the notations</p> <pre><code># Loss\ncorrect_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\nloss = np.sum(correct_class_logprobs) / len(y_train)\nprint (f\"loss: {loss:.2f}\")\n</code></pre> <pre>\nloss: 0.70\n</pre>"},{"location":"courses/foundations/neural-networks/#gradients","title":"Gradients","text":"<p><code>Step 4</code>: Calculate the gradient of loss \\(J(\\theta)\\) w.r.t to the model weights.</p> <p>The gradient of the loss w.r.t to $$ W_2 $$ is the same as the gradients from logistic regression since $\\(hat{y} = softmax(z_2)\\).</p> \\[ \\frac{\\partial{J}}{\\partial{W_{2j}}} = \\frac{\\partial{J}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{W_{2j}}} = - \\frac{1}{\\hat{y}}\\frac{\\partial{\\hat{y}}}{\\partial{W_{2j}}} = \\] \\[ = - \\frac{1}{\\frac{e^{W_{2y}a_1}}{\\sum_j e^{a_1W}}}\\frac{\\sum_j e^{a_1W}e^{a_1W_{2y}}0 - e^{a_1W_{2y}}e^{a_1W_{2j}}a_1}{(\\sum_j e^{a_1W})^2} = \\frac{a_1e^{a_1W_{2j}}}{\\sum_j e^{a_1W}} = a_1\\hat{y} \\] \\[ \\frac{\\partial{J}}{\\partial{W_{2y}}} = \\frac{\\partial{J}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{W_{2y}}} = - \\frac{1}{\\hat{y}}\\frac{\\partial{\\hat{y}}}{\\partial{W_{2y}}} = \\] \\[ = - \\frac{1}{\\frac{e^{W_{2y}a_1}}{\\sum_j e^{a_1W}}}\\frac{\\sum_j e^{a_1W}e^{a_1W_{2y}}a_1 - e^{a_1W_{2y}}e^{a_1W_{2y}}a_1}{(\\sum_j e^{a_1W})^2} = -\\frac{1}{\\hat{y}}(a_1\\hat{y} - a_1\\hat{y}^2) = a_1(\\hat{y}-1) \\] <p>The gradient of the loss w.r.t \\(W_1\\) is a bit trickier since we have to backpropagate through two sets of weights.</p> \\[ \\frac{\\partial{J}}{\\partial{W_1}} = \\frac{\\partial{J}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{a_1}}  \\frac{\\partial{a_1}}{\\partial{z_1}}  \\frac{\\partial{z_1}}{\\partial{W_1}}  = W_2(\\partial{scores})(\\partial{ReLU})X \\] <p><pre><code># dJ/dW2\ndscores = y_hat\ndscores[range(len(y_hat)), y_train] -= 1\ndscores /= len(y_train)\ndW2 = np.dot(a1.T, dscores)\ndb2 = np.sum(dscores, axis=0, keepdims=True)\n</code></pre> <pre><code># dJ/dW1\ndhidden = np.dot(dscores, W2.T)\ndhidden[a1 &lt;= 0] = 0 # ReLu backprop\ndW1 = np.dot(X_train.T, dhidden)\ndb1 = np.sum(dhidden, axis=0, keepdims=True)\n</code></pre></p>"},{"location":"courses/foundations/neural-networks/#update-weights","title":"Update weights","text":"<p><code>Step 5</code>: Update the weights \\(W\\) using a small learning rate \\(\\alpha\\). The updates will penalize the probability for the incorrect classes (\\(j\\)) and encourage a higher probability for the correct class (\\(y\\)).</p> \\[ W_i = W_i - \\alpha\\frac{\\partial{J}}{\\partial{W_i}} \\] <pre><code># Update weights\nW1 += -LEARNING_RATE * dW1\nb1 += -LEARNING_RATE * db1\nW2 += -LEARNING_RATE * dW2\nb2 += -LEARNING_RATE * db2\n</code></pre>"},{"location":"courses/foundations/neural-networks/#training_1","title":"Training","text":"<p><code>Step 6</code>: Repeat steps 2 - 4 until model performs well. <pre><code># Convert tensors to NumPy arrays\nX_train = X_train.numpy()\ny_train = y_train.numpy()\nX_val = X_val.numpy()\ny_val = y_val.numpy()\nX_test = X_test.numpy()\ny_test = y_test.numpy()\n</code></pre> <pre><code># Initialize random weights\nW1 = 0.01 * np.random.randn(INPUT_DIM, HIDDEN_DIM)\nb1 = np.zeros((1, HIDDEN_DIM))\nW2 = 0.01 * np.random.randn(HIDDEN_DIM, NUM_CLASSES)\nb2 = np.zeros((1, NUM_CLASSES))\n\n# Training loop\nfor epoch_num in range(1000):\n\n    # First layer forward pass [NX2] \u00b7 [2X100] = [NX100]\n    z1 = np.dot(X_train, W1) + b1\n\n    # Apply activation function\n    a1 = np.maximum(0, z1) # ReLU\n\n    # z2 = logits = [NX100] \u00b7 [100X3] = [NX3]\n    logits = np.dot(a1, W2) + b2\n\n    # Normalization via softmax to obtain class probabilities\n    exp_logits = np.exp(logits)\n    y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n    # Loss\n    correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\n    loss = np.sum(correct_class_logprobs) / len(y_train)\n\n    # show progress\n    if epoch_num%100 == 0:\n        # Accuracy\n        y_pred = np.argmax(logits, axis=1)\n        accuracy =  np.mean(np.equal(y_train, y_pred))\n        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n\n    # dJ/dW2\n    dscores = y_hat\n    dscores[range(len(y_hat)), y_train] -= 1\n    dscores /= len(y_train)\n    dW2 = np.dot(a1.T, dscores)\n    db2 = np.sum(dscores, axis=0, keepdims=True)\n\n    # dJ/dW1\n    dhidden = np.dot(dscores, W2.T)\n    dhidden[a1 &lt;= 0] = 0 # ReLu backprop\n    dW1 = np.dot(X_train.T, dhidden)\n    db1 = np.sum(dhidden, axis=0, keepdims=True)\n\n    # Update weights\n    W1 += -1e0 * dW1\n    b1 += -1e0 * db1\n    W2 += -1e0 * dW2\n    b2 += -1e0 * db2\n</code></pre></p> <pre>\nEpoch: 0, loss: 1.099, accuracy: 0.339\nEpoch: 100, loss: 0.549, accuracy: 0.678\nEpoch: 200, loss: 0.238, accuracy: 0.907\nEpoch: 300, loss: 0.151, accuracy: 0.946\nEpoch: 400, loss: 0.098, accuracy: 0.972\nEpoch: 500, loss: 0.074, accuracy: 0.985\nEpoch: 600, loss: 0.059, accuracy: 0.988\nEpoch: 700, loss: 0.050, accuracy: 0.991\nEpoch: 800, loss: 0.043, accuracy: 0.992\nEpoch: 900, loss: 0.038, accuracy: 0.993\n</pre>"},{"location":"courses/foundations/neural-networks/#evaluation_1","title":"Evaluation","text":"<p>Now let's see how our model performs on the test (hold-out) data split.</p> <p><pre><code>class MLPFromScratch():\n    def predict(self, x):\n        z1 = np.dot(x, W1) + b1\n        a1 = np.maximum(0, z1)\n        logits = np.dot(a1, W2) + b2\n        exp_logits = np.exp(logits)\n        y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        return y_hat\n</code></pre> <pre><code># Evaluation\nmodel = MLPFromScratch()\ny_prob = model.predict(X_test)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># # Performance\nperformance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)\nprint (json.dumps(performance, indent=2))\n</code></pre></p> <pre>\n{\n  \"overall\": {\n    \"precision\": 0.9824531024531025,\n    \"recall\": 0.9822222222222222,\n    \"f1\": 0.982220641694326,\n    \"num_samples\": 225.0\n  },\n  \"class\": {\n    \"c1\": {\n      \"precision\": 1.0,\n      \"recall\": 0.9733333333333334,\n      \"f1\": 0.9864864864864865,\n      \"num_samples\": 75.0\n    },\n    \"c2\": {\n      \"precision\": 0.974025974025974,\n      \"recall\": 1.0,\n      \"f1\": 0.9868421052631579,\n      \"num_samples\": 75.0\n    },\n    \"c3\": {\n      \"precision\": 0.9733333333333334,\n      \"recall\": 0.9733333333333334,\n      \"f1\": 0.9733333333333334,\n      \"num_samples\": 75.0\n    }\n  }\n}\n</pre> <p><pre><code>def plot_multiclass_decision_boundary_numpy(model, X, y, savefig_fp=None):\n\"\"\"Plot the multiclass decision boundary for a model that accepts 2D inputs.\n    Credit: https://cs231n.github.io/neural-networks-case-study/\n\n    Arguments:\n        model {function} -- trained model with function model.predict(x_in).\n        X {numpy.ndarray} -- 2D inputs with shape (N, 2).\n        y {numpy.ndarray} -- 1D outputs with shape (N,).\n    \"\"\"\n    # Axis boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101),\n                         np.linspace(y_min, y_max, 101))\n\n    # Create predictions\n    x_in = np.c_[xx.ravel(), yy.ravel()]\n    y_pred = model.predict(x_in)\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n\n    # Plot decision boundary\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    # Plot\n    if savefig_fp:\n        plt.savefig(savefig_fp, format=\"png\")\n</code></pre> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)\nplt.show()\n</code></pre></p>"},{"location":"courses/foundations/neural-networks/#pytorch","title":"PyTorch","text":"<p>Now let's implement the same MLP in PyTorch.</p>"},{"location":"courses/foundations/neural-networks/#model_2","title":"Model","text":"<p>We'll be using two linear layers along with PyTorch Functional API's ReLU operation. <pre><code>class MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x_in):\n        z = F.relu(self.fc1(x_in)) # ReLU activation function added!\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Initialize model\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of MLP(\n  (fc1): Linear(in_features=2, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=3, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/neural-networks/#training_2","title":"Training","text":"<p><pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values()))\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Accuracy\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\n</code></pre> <pre><code># Optimizer\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n</code></pre> <pre><code># Convert data to tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\n</code></pre> <pre><code># Training\nfor epoch in range(NUM_EPOCHS*10):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%10==0:\n        predictions = y_pred.max(dim=1)[1] # class\n        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\n</code></pre></p> <pre>\nEpoch: 0 | loss: 1.11, accuracy: 21.9\nEpoch: 10 | loss: 0.66, accuracy: 59.8\nEpoch: 20 | loss: 0.50, accuracy: 73.0\nEpoch: 30 | loss: 0.38, accuracy: 89.8\nEpoch: 40 | loss: 0.28, accuracy: 92.3\nEpoch: 50 | loss: 0.21, accuracy: 93.8\nEpoch: 60 | loss: 0.17, accuracy: 95.2\nEpoch: 70 | loss: 0.14, accuracy: 96.1\nEpoch: 80 | loss: 0.12, accuracy: 97.4\nEpoch: 90 | loss: 0.10, accuracy: 97.8\n</pre>"},{"location":"courses/foundations/neural-networks/#evaluation_2","title":"Evaluation","text":"<p><pre><code># Predictions\ny_prob = F.softmax(model(X_test), dim=1)\ny_pred = y_prob.max(dim=1)[1]\n</code></pre> <pre><code># # Performance\nperformance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)\nprint (json.dumps(performance, indent=2))\n</code></pre></p> <pre>\n{\n  \"overall\": {\n    \"precision\": 0.9706790123456791,\n    \"recall\": 0.9688888888888889,\n    \"f1\": 0.9690388976103262,\n    \"num_samples\": 225.0\n  },\n  \"class\": {\n    \"c1\": {\n      \"precision\": 1.0,\n      \"recall\": 0.96,\n      \"f1\": 0.9795918367346939,\n      \"num_samples\": 75.0\n    },\n    \"c2\": {\n      \"precision\": 0.9259259259259259,\n      \"recall\": 1.0,\n      \"f1\": 0.9615384615384615,\n      \"num_samples\": 75.0\n    },\n    \"c3\": {\n      \"precision\": 0.9861111111111112,\n      \"recall\": 0.9466666666666667,\n      \"f1\": 0.9659863945578231,\n      \"num_samples\": 75.0\n    }\n  }\n}\n</pre> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\nplt.show()\n</code></pre>"},{"location":"courses/foundations/neural-networks/#inference","title":"Inference","text":"<p>Let's look at the inference operations when using our trained model.</p> <p><pre><code># Inputs for inference\nX_infer = pd.DataFrame([{\"X1\": 0.1, \"X2\": 0.1}])\n</code></pre> <pre><code># Standardize\nX_infer = X_scaler.transform(X_infer)\nprint (X_infer)\n</code></pre></p> <pre>\n[[0.22746497 0.29242354]]\n</pre> <pre><code># Predict\ny_infer = F.softmax(model(torch.Tensor(X_infer)), dim=1)\nprob, _class = y_infer.max(dim=1)\nlabel = label_encoder.inverse_transform(_class.detach().numpy())[0]\nprint (f\"The probability that you have {label} is {prob.detach().numpy()[0]*100.0:.0f}%\")\n</code></pre> <pre>\nThe probability that you have c1 is 92%\n</pre>"},{"location":"courses/foundations/neural-networks/#initializing-weights","title":"Initializing weights","text":"<p>So far we have been initializing weights with small random values but this isn't optimal for convergence during training. The objective is to initialize the appropriate weights such that our activations (outputs of layers) don't vanish (too small) or explode (too large), as either of these situations will hinder convergence. We can do this by sampling the weights uniformly from a bound distribution (many that take into account the precise activation function used) such that all activations have unit variance.</p> <p>You may be wondering why we don't do this for every forward pass and that's a great question. We'll look at more advanced strategies that help with optimization like batch normalization, etc. in future lessons. Meanwhile you can check out other initializers here.</p> <p><pre><code>from torch.nn import init\n</code></pre> <pre><code>class MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def init_weights(self):\n        init.xavier_normal(self.fc1.weight, gain=init.calculate_gain(\"relu\"))\n\n    def forward(self, x_in):\n        z = F.relu(self.fc1(x_in)) # ReLU activation function added!\n        z = self.fc2(z)\n        return z\n</code></pre></p>"},{"location":"courses/foundations/neural-networks/#dropout","title":"Dropout","text":"<p>A great technique to have our models generalize (perform well on test data) is to increase the size of your data but this isn't always an option. Fortunately, there are methods like regularization and dropout that can help create a more robust model.</p> <p>Dropout is a technique (used only during training) that allows us to zero the outputs of neurons. We do this for <code>dropout_p</code>% of the total neurons in each layer and it changes every batch. Dropout prevents units from co-adapting too much to the data and acts as a sampling strategy since we drop a different set of neurons each time.</p> Dropout: A Simple Way to Prevent Neural Networks from Overfitting <p><pre><code>DROPOUT_P = 0.1 # percentage of weights that are dropped each pass\n</code></pre> <pre><code>class MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout_p, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p) # dropout\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def init_weights(self):\n        init.xavier_normal(self.fc1.weight, gain=init.calculate_gain(\"relu\"))\n\n    def forward(self, x_in):\n        z = F.relu(self.fc1(x_in))\n        z = self.dropout(z) # dropout\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Initialize model\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of MLP(\n  (fc1): Linear(in_features=2, out_features=100, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc2): Linear(in_features=100, out_features=3, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/neural-networks/#overfitting","title":"Overfitting","text":"<p>Though neural networks are great at capturing non-linear relationships they are highly susceptible to overfitting to the training data and failing to generalize on test data. Just take a look at the example below where we generate completely random data and are able to fit a model with \\(2*N*C + D\\) (where <code>N</code> = # of samples, <code>C</code> = # of classes and <code>D</code> = input dimension) hidden units. The training performance is good (~70%) but the overfitting leads to very poor test performance. We'll be covering strategies to tackle overfitting in future lessons.</p> <p><pre><code>NUM_EPOCHS = 500\nNUM_SAMPLES_PER_CLASS = 50\nLEARNING_RATE = 1e-1\nHIDDEN_DIM = 2 * NUM_SAMPLES_PER_CLASS * NUM_CLASSES + INPUT_DIM # 2*N*C + D\n</code></pre> <pre><code># Generate random data\nX = np.random.rand(NUM_SAMPLES_PER_CLASS * NUM_CLASSES, INPUT_DIM)\ny = np.array([[i]*NUM_SAMPLES_PER_CLASS for i in range(NUM_CLASSES)]).reshape(-1)\nprint (\"X: \", format(np.shape(X)))\nprint (\"y: \", format(np.shape(y)))\n</code></pre></p> <pre>\nX:  (150, 2)\ny:  (150,)\n</pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre> <pre>\nX_train: (105, 2), y_train: (105,)\nX_val: (22, 2), y_val: (22,)\nX_test: (23, 2), y_test: (23,)\nSample point: [0.52553355 0.33956916] \u2192 0\n</pre> <p><pre><code># Standardize the inputs (mean=0, std=1) using training data\nX_scaler = StandardScaler().fit(X_train)\nX_train = X_scaler.transform(X_train)\nX_val = X_scaler.transform(X_val)\nX_test = X_scaler.transform(X_test)\n</code></pre> <pre><code># Convert data to tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\n</code></pre> <pre><code># Initialize model\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of MLP(\n  (fc1): Linear(in_features=2, out_features=302, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc2): Linear(in_features=302, out_features=3, bias=True)\n)&gt;\n</pre> <p><pre><code># Optimizer\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n</code></pre> <pre><code># Training\nfor epoch in range(NUM_EPOCHS):\n    # Forward pass\n    y_pred = model(X_train)\n\n    # Loss\n    loss = loss_fn(y_pred, y_train)\n\n    # Zero all gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update weights\n    optimizer.step()\n\n    if epoch%20==0:\n        predictions = y_pred.max(dim=1)[1] # class\n        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\n</code></pre></p> <pre>\nEpoch: 0 | loss: 1.15, accuracy: 37.1\nEpoch: 20 | loss: 1.04, accuracy: 47.6\nEpoch: 40 | loss: 0.98, accuracy: 51.4\nEpoch: 60 | loss: 0.90, accuracy: 57.1\nEpoch: 80 | loss: 0.87, accuracy: 59.0\nEpoch: 100 | loss: 0.88, accuracy: 58.1\nEpoch: 120 | loss: 0.84, accuracy: 64.8\nEpoch: 140 | loss: 0.86, accuracy: 61.0\nEpoch: 160 | loss: 0.81, accuracy: 64.8\nEpoch: 180 | loss: 0.89, accuracy: 59.0\nEpoch: 200 | loss: 0.91, accuracy: 60.0\nEpoch: 220 | loss: 0.82, accuracy: 63.8\nEpoch: 240 | loss: 0.86, accuracy: 59.0\nEpoch: 260 | loss: 0.77, accuracy: 66.7\nEpoch: 280 | loss: 0.82, accuracy: 67.6\nEpoch: 300 | loss: 0.88, accuracy: 57.1\nEpoch: 320 | loss: 0.81, accuracy: 61.9\nEpoch: 340 | loss: 0.79, accuracy: 63.8\nEpoch: 360 | loss: 0.80, accuracy: 61.0\nEpoch: 380 | loss: 0.86, accuracy: 64.8\nEpoch: 400 | loss: 0.77, accuracy: 64.8\nEpoch: 420 | loss: 0.79, accuracy: 64.8\nEpoch: 440 | loss: 0.81, accuracy: 65.7\nEpoch: 460 | loss: 0.77, accuracy: 70.5\nEpoch: 480 | loss: 0.80, accuracy: 67.6\n</pre> <p><pre><code># Predictions\ny_prob = F.softmax(model(X_test), dim=1)\ny_pred = y_prob.max(dim=1)[1]\n</code></pre> <pre><code># # Performance\nperformance = get_metrics(y_true=y_test, y_pred=y_pred, classes=classes)\nprint (json.dumps(performance, indent=2))\n</code></pre></p> <pre>\n{\n  \"overall\": {\n    \"precision\": 0.17857142857142858,\n    \"recall\": 0.16666666666666666,\n    \"f1\": 0.1722222222222222,\n    \"num_samples\": 23.0\n  },\n  \"class\": {\n    \"c1\": {\n      \"precision\": 0.0,\n      \"recall\": 0.0,\n      \"f1\": 0.0,\n      \"num_samples\": 7.0\n    },\n    \"c2\": {\n      \"precision\": 0.2857142857142857,\n      \"recall\": 0.25,\n      \"f1\": 0.26666666666666666,\n      \"num_samples\": 8.0\n    },\n    \"c3\": {\n      \"precision\": 0.25,\n      \"recall\": 0.25,\n      \"f1\": 0.25,\n      \"num_samples\": 8.0\n    }\n  }\n}\n</pre> <pre><code># Visualize the decision boundary\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\nplt.show()\n</code></pre> <p>It's important that we experiment, starting with simple models that underfit (high bias) and improve it towards a good fit. Starting with simple models (linear/logistic regression) let's us catch errors without the added complexity of more sophisticated models (neural networks).</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Neural networks - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/notebooks/","title":"Working in Notebooks","text":""},{"location":"courses/foundations/notebooks/#set-up","title":"Set up","text":"<ol> <li>Click on this link to open the accompanying notebook for this lesson or create a blank one on Google Colab.</li> <li>Sign into your Google account to start using the notebook. If you don't want to save your work, you can skip the steps below. If you do not have access to Google, you can follow along using Jupyter Lab.</li> <li>If you do want to save your work, click the COPY TO DRIVE button on the toolbar. This will open a new notebook in a new tab. Rename this new notebook by removing the words Copy of from the title (change <code>Copy of 01_Notebooks</code> to <code>1_Notebooks</code>).</li> </ol> <p>Alternatives to Google Colab</p> <p>Alternatively, you can run these notebooks locally by using JupyterLab. You should first set up a directory for our project, create a virtual environment and install jupyterlab.</p> <pre><code>mkdir mlops\npython3 -m venv venv\nsource venv/bin/activate\npip install jupyterlab\njupyter lab\n</code></pre>"},{"location":"courses/foundations/notebooks/#types-of-cells","title":"Types of cells","text":"<p>Notebooks are made up of cells. There are two types of cells:</p> <ul> <li><code>code cell</code>: used for writing and executing code.</li> <li><code>text cell</code>: used for writing text, HTML, Markdown, etc.</li> </ul>"},{"location":"courses/foundations/notebooks/#text-cells","title":"Text cells","text":"<p>Click on a desired location in the notebook and create the cell by clicking on the <code>\u2795 TEXT</code> (located in the top left corner).</p> <p>Once you create the cell, click on it and type the following text inside it:</p> <pre><code>### This is a header\nHello world!\n</code></pre>"},{"location":"courses/foundations/notebooks/#run-a-cell","title":"Run a cell","text":"<p>Once you type inside the cell, press the <code>SHIFT</code> and <code>RETURN</code> (enter key) together to run the cell.</p>"},{"location":"courses/foundations/notebooks/#edit-a-cell","title":"Edit a cell","text":"<p>To edit a cell, double click on it and make any changes.</p>"},{"location":"courses/foundations/notebooks/#move-a-cell","title":"Move a cell","text":"<p>Move a cell up and down by clicking on the cell and then pressing the \u2b06 and \u2b07 button on the top right of the cell.</p>"},{"location":"courses/foundations/notebooks/#delete-a-cell","title":"Delete a cell","text":"<p>Delete the cell by clicking on it and pressing the trash can button \ud83d\uddd1\ufe0f on the top right corner of the cell. Alternatively, you can also press \u2318/Ctrl + M + D.</p>"},{"location":"courses/foundations/notebooks/#code-cells","title":"Code cells","text":"<p>Repeat the steps above to create and edit a code cell. You can create a code cell by clicking on the <code>\u2795 CODE</code> (located in the top left corner).</p> <p>Once you've created the code cell, double click on it, type the following inside it and then press Shift + Enter to execute the code. <pre><code>print (\"Hello world!\")\n</code></pre></p> <pre>\nHello world!\n</pre> <p>These are the basic concepts we'll need to use these notebooks but we'll learn few more tricks in subsequent lessons.</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Notebooks - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/numpy/","title":"NumPy for Machine Learning","text":""},{"location":"courses/foundations/numpy/#set-up","title":"Set up","text":"<p>First we'll import the NumPy package and set seeds for reproducibility so that we can receive the exact same results every time.</p> <p><pre><code>import numpy as np\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(seed=1234)\n</code></pre></p> <pre></pre>"},{"location":"courses/foundations/numpy/#basics","title":"Basics","text":"<pre><code># Scalar\nx = np.array(6)\nprint (\"x: \", x)\nprint (\"x ndim: \", x.ndim) # number of dimensions\nprint (\"x shape:\", x.shape) # dimensions\nprint (\"x size: \", x.size) # size of elements\nprint (\"x dtype: \", x.dtype) # data type\n</code></pre> <pre>\nx:  6\nx ndim:  0\nx shape: ()\nx size:  1\nx dtype:  int64\n</pre> <pre><code># Vector\nx = np.array([1.3 , 2.2 , 1.7])\nprint (\"x: \", x)\nprint (\"x ndim: \", x.ndim)\nprint (\"x shape:\", x.shape)\nprint (\"x size: \", x.size)\nprint (\"x dtype: \", x.dtype) # notice the float datatype\n</code></pre> <pre>\nx:  [1.3 2.2 1.7]\nx ndim:  1\nx shape: (3,)\nx size:  3\nx dtype:  float64\n</pre> <pre><code># Matrix\nx = np.array([[1,2], [3,4]])\nprint (\"x:\\n\", x)\nprint (\"x ndim: \", x.ndim)\nprint (\"x shape:\", x.shape)\nprint (\"x size: \", x.size)\nprint (\"x dtype: \", x.dtype)\n</code></pre> <pre>\nx:\n [[1 2]\n [3 4]]\nx ndim:  2\nx shape: (2, 2)\nx size:  4\nx dtype:  int64\n</pre> <pre><code># 3-D Tensor\nx = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\nprint (\"x:\\n\", x)\nprint (\"x ndim: \", x.ndim)\nprint (\"x shape:\", x.shape)\nprint (\"x size: \", x.size)\nprint (\"x dtype: \", x.dtype)\n</code></pre> <pre>\nx:\n [[[1 2]\n  [3 4]]\n\n [[5 6]\n  [7 8]]]\nx ndim:  3\nx shape: (2, 2, 2)\nx size:  8\nx dtype:  int64\n</pre> <p>NumPy also comes with several functions that allow us to create tensors quickly. <pre><code># Functions\nprint (\"np.zeros((2,2)):\\n\", np.zeros((2,2)))\nprint (\"np.ones((2,2)):\\n\", np.ones((2,2)))\nprint (\"np.eye((2)):\\n\", np.eye((2))) # identity matrix\nprint (\"np.random.random((2,2)):\\n\", np.random.random((2,2)))\n</code></pre></p> <pre>\nnp.zeros((2,2)):\n [[0. 0.]\n [0. 0.]]\nnp.ones((2,2)):\n [[1. 1.]\n [1. 1.]]\nnp.eye((2)):\n [[1. 0.]\n [0. 1.]]\nnp.random.random((2,2)):\n [[0.19151945 0.62210877]\n [0.43772774 0.78535858]]\n</pre>"},{"location":"courses/foundations/numpy/#indexing","title":"Indexing","text":"<p>We can extract specific values from our tensors using indexing.</p> <p>Keep in mind that when indexing the row and column, indices start at <code>0</code>. And like indexing with lists, we can use negative indices as well (where <code>-1</code> is the last item).</p> <pre><code># Indexing\nx = np.array([1, 2, 3])\nprint (\"x: \", x)\nprint (\"x[0]: \", x[0])\nx[0] = 0\nprint (\"x: \", x)\n</code></pre> <pre>\nx:  [1 2 3]\nx[0]:  1\nx:  [0 2 3]\n</pre> <pre><code># Slicing\nx = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nprint (x)\nprint (\"x column 1: \", x[:, 1])\nprint (\"x row 0: \", x[0, :])\nprint (\"x rows 0,1 &amp; cols 1,2: \\n\", x[0:2, 1:3])\n</code></pre> <pre>\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\nx column 1:  [ 2  6 10]\nx row 0:  [1 2 3 4]\nx rows 0,1 &amp; cols 1,2:\n [[2 3]\n [6 7]]\n</pre> <pre><code># Integer array indexing\nprint (x)\nrows_to_get = np.array([0, 1, 2])\nprint (\"rows_to_get: \", rows_to_get)\ncols_to_get = np.array([0, 2, 1])\nprint (\"cols_to_get: \", cols_to_get)\n# Combine sequences above to get values to get\nprint (\"indexed values: \", x[rows_to_get, cols_to_get]) # (0, 0), (1, 2), (2, 1)\n</code></pre> <pre>\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\nrows_to_get:  [0 1 2]\ncols_to_get:  [0 2 1]\nindexed values:  [ 1  7 10]\n</pre> <pre><code># Boolean array indexing\nx = np.array([[1, 2], [3, 4], [5, 6]])\nprint (\"x:\\n\", x)\nprint (\"x &gt; 2:\\n\", x &gt; 2)\nprint (\"x[x &gt; 2]:\\n\", x[x &gt; 2])\n</code></pre> <pre>\nx:\n [[1 2]\n [3 4]\n [5 6]]\nx &gt; 2:\n [[False False]\n [ True  True]\n [ True  True]]\nx[x &gt; 2]:\n [3 4 5 6]\n</pre>"},{"location":"courses/foundations/numpy/#arithmetic","title":"Arithmetic","text":"<pre><code># Basic math\nx = np.array([[1,2], [3,4]], dtype=np.float64)\ny = np.array([[1,2], [3,4]], dtype=np.float64)\nprint (\"x + y:\\n\", np.add(x, y)) # or x + y\nprint (\"x - y:\\n\", np.subtract(x, y)) # or x - y\nprint (\"x * y:\\n\", np.multiply(x, y)) # or x * y\n</code></pre> <pre>\nx + y:\n [[2. 4.]\n [6. 8.]]\nx - y:\n [[0. 0.]\n [0. 0.]]\nx * y:\n [[ 1.  4.]\n [ 9. 16.]]\n</pre>"},{"location":"courses/foundations/numpy/#dot-product","title":"Dot product","text":"<p>One of the most common NumPy operations we\u2019ll use in machine learning is matrix multiplication using the dot product. Suppose we wanted to take the dot product of two matrices with shapes <code>[2 X 3]</code> and <code>[3 X 2]</code>. We take the rows of our first matrix (2) and the columns of our second matrix (2) to determine the dot product, giving us an output of <code>[2 X 2]</code>. The only requirement is that the inside dimensions match, in this case the first matrix has 3 columns and the second matrix has 3 rows.</p> <pre><code># Dot product\na = np.array([[1,2,3], [4,5,6]], dtype=np.float64) # we can specify dtype\nb = np.array([[7,8], [9,10], [11, 12]], dtype=np.float64)\nc = a.dot(b)\nprint (f\"{a.shape} \u00b7 {b.shape} = {c.shape}\")\nprint (c)\n</code></pre> <pre>\n(2, 3) \u00b7 (3, 2) = (2, 2)\n[[ 58.  64.]\n [139. 154.]]\n</pre>"},{"location":"courses/foundations/numpy/#axis-operations","title":"Axis operations","text":"<p>We can also do operations across a specific axis.</p> <pre><code># Sum across a dimension\nx = np.array([[1,2],[3,4]])\nprint (x)\nprint (\"sum all: \", np.sum(x)) # adds all elements\nprint (\"sum axis=0: \", np.sum(x, axis=0)) # sum across rows\nprint (\"sum axis=1: \", np.sum(x, axis=1)) # sum across columns\n</code></pre> <pre>\n[[1 2]\n [3 4]]\nsum all:  10\nsum axis=0:  [4 6]\nsum axis=1:  [3 7]\n</pre> <pre><code># Min/max\nx = np.array([[1,2,3], [4,5,6]])\nprint (\"min: \", x.min())\nprint (\"max: \", x.max())\nprint (\"min axis=0: \", x.min(axis=0))\nprint (\"min axis=1: \", x.min(axis=1))\n</code></pre> <pre>\nmin:  1\nmax:  6\nmin axis=0:  [1 2 3]\nmin axis=1:  [1 4]\n</pre>"},{"location":"courses/foundations/numpy/#broadcast","title":"Broadcast","text":"<p>What happens when we try to do operations with tensors with seemingly incompatible shapes? Their dimensions aren\u2019t compatible as is but how does NumPy still gives us the right result? This is where broadcasting comes in. The scalar is broadcast across the vector so that they have compatible shapes.</p> <pre><code># Broadcasting\nx = np.array([1,2]) # vector\ny = np.array(3) # scalar\nz = x + y\nprint (\"z:\\n\", z)\n</code></pre> <pre>\nz:\n [4 5]\n</pre>"},{"location":"courses/foundations/numpy/#gotchas","title":"Gotchas","text":"<p>In the situation below, what is the value of <code>c</code> and what are its dimensions?</p> <pre><code>a = np.array((3, 4, 5))\nb = np.expand_dims(a, axis=1)\nc = a + b\n</code></pre> <pre><code>a.shape # (3,)\nb.shape # (3, 1)\nc.shape # (3, 3)\nprint (c)\n</code></pre> <pre>\narray([[ 6,  7,  8],\n        [ 7,  8,  9],\n        [ 8,  9, 10]])\n</pre> <p>How can we fix this? We need to be careful to ensure that <code>a</code> is the same shape as <code>b</code> if we don't want this unintentional broadcasting behavior. <pre><code>a = a.reshape(-1, 1)\na.shape # (3, 1)\nc = a + b\nc.shape # (3, 1)\nprint (c)\n</code></pre></p> <pre>\narray([[ 6],\n       [ 8],\n       [10]])\n</pre> <p>This kind of unintended broadcasting happens more often then you'd think because this is exactly what happens when we create an array from a list. So we need to ensure that we apply the proper reshaping before using it for any operations.</p> <pre><code>a = np.array([3, 4, 5])\na.shape # (3,)\na = a.reshape(-1, 1)\na.shape # (3, 1)\n</code></pre>"},{"location":"courses/foundations/numpy/#transpose","title":"Transpose","text":"<p>We often need to change the dimensions of our tensors for operations like the dot product. If we need to switch two dimensions, we can transpose the tensor.</p> <pre><code># Transposing\nx = np.array([[1,2,3], [4,5,6]])\nprint (\"x:\\n\", x)\nprint (\"x.shape: \", x.shape)\ny = np.transpose(x, (1,0)) # flip dimensions at index 0 and 1\nprint (\"y:\\n\", y)\nprint (\"y.shape: \", y.shape)\n</code></pre> <pre>\nx:\n [[1 2 3]\n [4 5 6]]\nx.shape:  (2, 3)\ny:\n [[1 4]\n [2 5]\n [3 6]]\ny.shape:  (3, 2)\n</pre>"},{"location":"courses/foundations/numpy/#reshape","title":"Reshape","text":"<p>Sometimes, we'll need to alter the dimensions of the matrix. Reshaping allows us to transform a tensor into different permissible shapes. Below, our reshaped tensor has the same number of values as the original tensor. (<code>1X6</code> = <code>2X3</code>). We can also use <code>-1</code> on a dimension and NumPy will infer the dimension based on our input tensor.</p> <pre><code># Reshaping\nx = np.array([[1,2,3,4,5,6]])\nprint (x)\nprint (\"x.shape: \", x.shape)\ny = np.reshape(x, (2, 3))\nprint (\"y: \\n\", y)\nprint (\"y.shape: \", y.shape)\nz = np.reshape(x, (2, -1))\nprint (\"z: \\n\", z)\nprint (\"z.shape: \", z.shape)\n</code></pre> <pre>\n[[1 2 3 4 5 6]]\nx.shape:  (1, 6)\ny:\n [[1 2 3]\n [4 5 6]]\ny.shape:  (2, 3)\nz:\n [[1 2 3]\n [4 5 6]]\nz.shape:  (2, 3)\n</pre> <p>The way reshape works is by looking at each dimension of the new tensor and separating our original tensor into that many units. So here the dimension at index 0 of the new tensor is 2 so we divide our original tensor into 2 units, and each of those has 3 values.</p> <p>Unintended reshaping</p> <p>Though reshaping is very convenient to manipulate tensors, we must be careful of its pitfalls as well. Let's look at the example below. Suppose we have <code>x</code>, which has the shape <code>[2 X 3 X 4]</code>.</p> <pre><code>x = np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]],\n            [[10, 10, 10, 10], [20, 20, 20, 20], [30, 30, 30, 30]]])\nprint (\"x:\\n\", x)\nprint (\"x.shape: \", x.shape)\n</code></pre> <p><pre>\nx:\n[[[ 1  1  1  1]\n[ 2  2  2  2]\n[ 3  3  3  3]]\n<p>[[10 10 10 10]\n[20 20 20 20]\n[30 30 30 30]]]\nx.shape:  (2, 3, 4)\n</p>\n<p>We want to reshape x so that it has shape <code>[3 X 8]</code> but we want the output to look like this:</p>\n<p><pre>\n[[ 1  1  1  1 10 10 10 10]\n[ 2  2  2  2 20 20 20 20]\n[ 3  3  3  3 30 30 30 30]]\n</pre></p>\n<p>and not like:</p>\n<p><pre>\n[[ 1  1  1  1  2  2  2  2]\n[ 3  3  3  3 10 10 10 10]\n[20 20 20 20 30 30 30 30]]\n</pre></p>\n<p>even though they both have the same shape <code>[3X8]</code>. What is the right way to reshape this?</p>\nShow answer\n<p>When we naively do a reshape, we get the right shape but the values are not what we're looking for.</p>\n<p>\n    </p>\n<p><pre><code># Unintended reshaping\nz_incorrect = np.reshape(x, (x.shape[1], -1))\nprint (\"z_incorrect:\\n\", z_incorrect)\nprint (\"z_incorrect.shape: \", z_incorrect.shape)\n</code></pre>\n<pre>\nz_incorrect:\n[[ 1  1  1  1  2  2  2  2]\n[ 3  3  3  3 10 10 10 10]\n[20 20 20 20 30 30 30 30]]\nz_incorrect.shape:  (3, 8)\n</pre></p>\n<p>Instead, if we transpose the tensor and then do a reshape, we get our desired tensor. Transpose allows us to put our two vectors that we want to combine together and then we use reshape to join them together. And as a general rule, we should always get our dimensions together before reshaping to combine them.</p>\n<p>\n    </p>\n<p><pre><code># Intended reshaping\ny = np.transpose(x, (1,0,2))\nprint (\"y:\\n\", y)\nprint (\"y.shape: \", y.shape)\nz_correct = np.reshape(y, (y.shape[0], -1))\nprint (\"z_correct:\\n\", z_correct)\nprint (\"z_correct.shape: \", z_correct.shape)\n</code></pre>\n<pre>\ny:\n[[[ 1  1  1  1]\n[10 10 10 10]]\n<p>[[ 2  2  2  2]\n[20 20 20 20]]</p>\n<p>[[ 3  3  3  3]\n[30 30 30 30]]]\ny.shape:  (3, 2, 4)\nz_correct:\n[[ 1  1  1  1 10 10 10 10]\n[ 2  2  2  2 20 20 20 20]\n[ 3  3  3  3 30 30 30 30]]\nz_correct.shape:  (3, 8)\n</p>\n<p>This becomes difficult when we're dealing with weight tensors with random values in many machine learning tasks. So a good idea is to always create a dummy example like this when you\u2019re unsure about reshaping. Blindly going by the tensor shape can lead to lots of issues downstream.</p>"},{"location":"courses/foundations/numpy/#joining","title":"Joining","text":"<p>We can also join our tensors via concatentation or stacking.</p>\n<pre><code>x = np.random.random((2, 3))\nprint (x)\nprint (x.shape)\n</code></pre>\n<pre>\n[[0.79564718 0.73023418 0.92340453]\n [0.24929281 0.0513762  0.66149188]]\n(2, 3)\n</pre>\n\n<pre><code># Concatenation\ny = np.concatenate([x, x], axis=0) # concat on a specified axis\nprint (y)\nprint (y.shape)\n</code></pre>\n<pre>\n[[0.79564718 0.73023418 0.92340453]\n [0.24929281 0.0513762  0.66149188]\n [0.79564718 0.73023418 0.92340453]\n [0.24929281 0.0513762  0.66149188]]\n(4, 3)\n</pre>\n\n<pre><code># Stacking\nz = np.stack([x, x], axis=0) # stack on new axis\nprint (z)\nprint (z.shape)\n</code></pre>\n<pre>\n[[[0.79564718 0.73023418 0.92340453]\n  [0.24929281 0.0513762  0.66149188]]\n\n [[0.79564718 0.73023418 0.92340453]\n  [0.24929281 0.0513762  0.66149188]]]\n(2, 2, 3)\n</pre>"},{"location":"courses/foundations/numpy/#expanding-reducing","title":"Expanding / reducing","text":"<p>We can also easily add and remove dimensions to our tensors and we'll want to do this to make tensors compatible for certain operations.</p>\n<pre><code># Adding dimensions\nx = np.array([[1,2,3],[4,5,6]])\nprint (\"x:\\n\", x)\nprint (\"x.shape: \", x.shape)\ny = np.expand_dims(x, 1) # expand dim 1\nprint (\"y: \\n\", y)\nprint (\"y.shape: \", y.shape)   # notice extra set of brackets are added\n</code></pre>\n<pre>\nx:\n [[1 2 3]\n  [4 5 6]]\nx.shape:  (2, 3)\ny:\n [[[1 2 3]]\n  [[4 5 6]]]\ny.shape:  (2, 1, 3)\n</pre>\n\n<pre><code># Removing dimensions\nx = np.array([[[1,2,3]],[[4,5,6]]])\nprint (\"x:\\n\", x)\nprint (\"x.shape: \", x.shape)\ny = np.squeeze(x, 1) # squeeze dim 1\nprint (\"y: \\n\", y)\nprint (\"y.shape: \", y.shape)  # notice extra set of brackets are gone\n</code></pre>\n<pre>\nx:\n [[[1 2 3]]\n  [[4 5 6]]]\nx.shape:  (2, 1, 3)\ny:\n [[1 2 3]\n  [4 5 6]]\ny.shape:  (2, 3)\n</pre>\n\n<p>Check out Dask for scaling NumPy workflows with minimal change to existing code.</p>\n<p>To cite this content, please use:</p>\n<pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { NumPy - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/pandas/","title":"Pandas for Machine Learning","text":""},{"location":"courses/foundations/pandas/#set-up","title":"Set up","text":"<p>First we'll import the NumPy and Pandas libraries and set seeds for reproducibility. We'll also download the dataset we'll be working with to disk. <pre><code>import numpy as np\nimport pandas as pd\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(seed=1234)\n</code></pre></p> <pre></pre>"},{"location":"courses/foundations/pandas/#load-data","title":"Load data","text":"<p>We're going to work with the Titanic dataset which has data on the people who embarked the RMS Titanic in 1912 and whether they survived the expedition or not. It's a very common and rich dataset which makes it very apt for exploratory data analysis with Pandas.</p> <p>Let's load the data from the CSV file into a Pandas dataframe. The <code>header=0</code> signifies that the first row (0th index) is a header row which contains the names of each column in our dataset.</p> <p><pre><code># Read from CSV to Pandas DataFrame\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/titanic.csv\"\ndf = pd.read_csv(url, header=0)\n</code></pre> <pre><code># First few items\ndf.head(3)\n</code></pre></p> pclass name sex age sibsp parch ticket fare cabin embarked survived 0 1 Allen, Miss. Elisabeth Walton female 29.0000 0 0 24160 211.3375 B5 S 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 1 2 1 Allison, Miss. Helen Loraine female 2.0000 1 2 113781 151.5500 C22 C26 S 0 <p>These are the different features:</p> <ul> <li><code>class</code>: class of travel</li> <li><code>name</code>: full name of the passenger</li> <li><code>sex</code>: gender</li> <li><code>age</code>: numerical age</li> <li><code>sibsp</code>: # of siblings/spouse aboard</li> <li><code>parch</code>: number of parents/child aboard</li> <li><code>ticket</code>: ticket number</li> <li><code>fare</code>: cost of the ticket</li> <li><code>cabin</code>: location of room</li> <li><code>embarked</code>: port that the passenger embarked at</li> <li><code>survived</code>: survival metric (0 - died, 1 - survived)</li> </ul>"},{"location":"courses/foundations/pandas/#exploratory-data-analysis-eda","title":"Exploratory data analysis (EDA)","text":"<p>Now that we loaded our data, we're ready to start exploring it to find interesting information.</p> <p>Be sure to check out our entire lesson focused on EDA in our MLOps course.</p> <pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre></pre> <p>We can use <code>.describe()</code> to extract some standard details about our numerical features.</p> <pre><code># Describe features\ndf.describe()\n</code></pre> pclass age sibsp parch fare survived count 1309.000000 1046.000000 1309.000000 1309.000000 1308.000000 1309.000000 mean 2.294882 29.881135 0.498854 0.385027 33.295479 0.381971 std 0.837836 14.413500 1.041658 0.865560 51.758668 0.486055 min 1.000000 0.166700 0.000000 0.000000 0.000000 0.000000 25% 2.000000 21.000000 0.000000 0.000000 7.895800 0.000000 50% 3.000000 28.000000 0.000000 0.000000 14.454200 0.000000 75% 3.000000 39.000000 1.000000 0.000000 31.275000 1.000000 max 3.000000 80.000000 8.000000 9.000000 512.329200 1.000000 <pre><code># Correlation matrix\nplt.matshow(df.corr())\ncontinuous_features = df.describe().columns\nplt.xticks(range(len(continuous_features)), continuous_features, rotation=\"45\")\nplt.yticks(range(len(continuous_features)), continuous_features, rotation=\"45\")\nplt.colorbar()\nplt.show()\n</code></pre> <p>We can also use <code>.hist()</code> to view the histogram of values for each feature. <pre><code># Histograms\ndf[\"age\"].hist()\n</code></pre></p> <pre><code># Unique values\ndf[\"embarked\"].unique()\n</code></pre> <pre>\narray(['S', 'C', nan, 'Q'], dtype=object)\n</pre>"},{"location":"courses/foundations/pandas/#filtering","title":"Filtering","text":"<p>We can filter our data by features and even by specific values (or value ranges) within specific features. <pre><code># Selecting data by feature\ndf[\"name\"].head()\n</code></pre></p> <pre>\n0                      Allen, Miss. Elisabeth Walton\n1                     Allison, Master. Hudson Trevor\n2                       Allison, Miss. Helen Loraine\n3               Allison, Mr. Hudson Joshua Creighton\n4    Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\nName: name, dtype: object\n</pre> <pre><code># Filtering\ndf[df[\"sex\"]==\"female\"].head() # only the female data appear\n</code></pre> pclass name sex age sibsp parch ticket fare cabin embarked survived 0 1 Allen, Miss. Elisabeth Walton female 29.0 0 0 24160 211.3375 B5 S 1 2 1 Allison, Miss. Helen Loraine female 2.0 1 2 113781 151.5500 C22 C26 S 0 4 1 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0 1 2 113781 151.5500 C22 C26 S 0 6 1 Andrews, Miss. Kornelia Theodosia female 63.0 1 0 13502 77.9583 D7 S 1 8 1 Appleton, Mrs. Edward Dale (Charlotte Lamson) female 53.0 2 0 11769 51.4792 C101 S 1"},{"location":"courses/foundations/pandas/#sorting","title":"Sorting","text":"<p>We can also sort our features in ascending or descending order. <pre><code># Sorting\ndf.sort_values(\"age\", ascending=False).head()\n</code></pre></p> pclass name sex age sibsp parch ticket fare cabin embarked survived 14 1 Barkworth, Mr. Algernon Henry Wilson male 80.0 0 0 27042 30.0000 A23 S 1 61 1 Cavendish, Mrs. Tyrell William (Julia Florence... female 76.0 1 0 19877 78.8500 C46 S 1 1235 3 Svensson, Mr. Johan male 74.0 0 0 347060 7.7750 NaN S 0 135 1 Goldschmidt, Mr. George B male 71.0 0 0 PC 17754 34.6542 A5 C 0 9 1 Artagaveytia, Mr. Ramon male 71.0 0 0 PC 17609 49.5042 NaN C 0"},{"location":"courses/foundations/pandas/#grouping","title":"Grouping","text":"<p>We can also get statistics across our features for certain groups. Here we wan to see the average of our continuous features based on whether the passenger survived or not. <pre><code># Grouping\nsurvived_group = df.groupby(\"survived\")\nsurvived_group.mean()\n</code></pre></p> survived pclass age sibsp parch fare 0 2.500618 30.545369 0.521632 0.328801 23.353831 1 1.962000 28.918228 0.462000 0.476000 49.361184"},{"location":"courses/foundations/pandas/#indexing","title":"Indexing","text":"<p>We can use <code>iloc</code> to get rows or columns at particular positions in the dataframe. <pre><code># Selecting row 0\ndf.iloc[0, :]\n</code></pre></p> <pre>\npclass                                  1\nname        Allen, Miss. Elisabeth Walton\nsex                                female\nage                                    29\nsibsp                                   0\nparch                                   0\nticket                              24160\nfare                              211.338\ncabin                                  B5\nembarked                                S\nsurvived                                1\nName: 0, dtype: object\n</pre> <pre><code># Selecting a specific value\ndf.iloc[0, 1]\n</code></pre> <pre>\n'Allen, Miss. Elisabeth Walton'\n</pre>"},{"location":"courses/foundations/pandas/#preprocessing","title":"Preprocessing","text":"<p>After exploring, we can clean and preprocess our dataset.</p> <p>Be sure to check out our entire lesson focused on preprocessing in our MLOps course.</p> <pre><code># Rows with at least one NaN value\ndf[pd.isnull(df).any(axis=1)].head()\n</code></pre> pclass name sex age sibsp parch ticket fare cabin embarked survived 9 1 Artagaveytia, Mr. Ramon male 71.0 0 0 PC 17609 49.5042 NaN C 0 13 1 Barber, Miss. Ellen \"Nellie\" female 26.0 0 0 19877 78.8500 NaN S 1 15 1 Baumann, Mr. John D male NaN 0 0 PC 17318 25.9250 NaN S 0 23 1 Bidois, Miss. Rosalie female 42.0 0 0 PC 17757 227.5250 NaN C 1 25 1 Birnbaum, Mr. Jakob male 25.0 0 0 13905 26.0000 NaN C 0 <pre><code># Drop rows with Nan values\ndf = df.dropna() # removes rows with any NaN values\ndf = df.reset_index() # reset's row indexes in case any rows were dropped\ndf.head()\n</code></pre> index pclass name sex age sibsp parch ticket fare cabin embarked survived 0 0 1 Allen, Miss. Elisabeth Walton female 29.0000 0 0 24160 211.3375 B5 S 1 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 1 2 2 1 Allison, Miss. Helen Loraine female 2.0000 1 2 113781 151.5500 C22 C26 S 0 3 3 1 Allison, Mr. Hudson Joshua Creighton male 30.0000 1 2 113781 151.5500 C22 C26 S 0 4 4 1 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000 1 2 113781 151.5500 C22 C26 S 0 <pre><code># Dropping multiple columns\ndf = df.drop([\"name\", \"cabin\", \"ticket\"], axis=1) # we won't use text features for our initial basic models\ndf.head()\n</code></pre> index pclass sex age sibsp parch fare embarked survived 0 0 1 female 29.0000 0 0 211.3375 S 1 1 1 1 male 0.9167 1 2 151.5500 S 1 2 2 1 female 2.0000 1 2 151.5500 S 0 3 3 1 male 30.0000 1 2 151.5500 S 0 4 4 1 female 25.0000 1 2 151.5500 S 0 <pre><code># Map feature values\ndf[\"sex\"] = df[\"sex\"].map( {\"female\": 0, \"male\": 1} ).astype(int)\ndf[\"embarked\"] = df[\"embarked\"].dropna().map( {\"S\":0, \"C\":1, \"Q\":2} ).astype(int)\ndf.head()\n</code></pre> index pclass sex age sibsp parch fare embarked survived 0 0 1 0 29.0000 0 0 211.3375 0 1 1 1 1 1 0.9167 1 2 151.5500 0 1 2 2 1 0 2.0000 1 2 151.5500 0 0 3 3 1 1 30.0000 1 2 151.5500 0 0 4 4 1 0 25.0000 1 2 151.5500 0 0"},{"location":"courses/foundations/pandas/#feature-engineering","title":"Feature engineering","text":"<p>We're now going to use feature engineering to create a column called <code>family_size</code>. We'll first define a function called <code>get_family_size</code> that will determine the family size using the number of parents and siblings. <pre><code># Lambda expressions to create new features\ndef get_family_size(sibsp, parch):\n    family_size = sibsp + parch\n    return family_size\n</code></pre> Once we define the function, we can use <code>lambda</code> to <code>apply</code> that function on each row (using the numbers of siblings and parents in each row to determine the family size for each row). <pre><code>df[\"family_size\"] = df[[\"sibsp\", \"parch\"]].apply(lambda x: get_family_size(x[\"sibsp\"], x[\"parch\"]), axis=1)\ndf.head()\n</code></pre></p> index pclass sex age sibsp parch fare embarked survived family_size 0 0 1 0 29.0000 0 0 211.3375 0 1 0 1 1 1 1 0.9167 1 2 151.5500 0 1 3 2 2 1 0 2.0000 1 2 151.5500 0 0 3 3 3 1 1 30.0000 1 2 151.5500 0 0 3 4 4 1 0 25.0000 1 2 151.5500 0 0 3 <pre><code># Reorganize headers\ndf = df[[\"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"family_size\", \"fare\", '\"mbarked\", \"survived\"]]\ndf.head()\n</code></pre> pclass sex age sibsp parch family_size fare embarked survived 0 1 0 29.0000 0 0 0 211.3375 0 1 1 1 1 0.9167 1 2 3 151.5500 0 1 2 1 0 2.0000 1 2 3 151.5500 0 0 3 1 1 30.0000 1 2 3 151.5500 0 0 4 1 0 25.0000 1 2 3 151.5500 0 0 <p>Tip</p> <p>Feature engineering can be done in collaboration with domain experts that can guide us on what features to engineer and use.</p>"},{"location":"courses/foundations/pandas/#save-data","title":"Save data","text":"<p>Finally, let's save our preprocessed data into a new CSV file to use later. <pre><code># Saving dataframe to CSV\ndf.to_csv(\"processed_titanic.csv\", index=False)\n</code></pre> <pre><code># See the saved file\n!ls -l\n</code></pre></p> <pre>\ntotal 96\n-rw-r--r-- 1 root root  6975 Dec  3 17:36 processed_titanic.csv\ndrwxr-xr-x 1 root root  4096 Nov 21 16:30 sample_data\n-rw-r--r-- 1 root root 85153 Dec  3 17:36 titanic.csv\n</pre>"},{"location":"courses/foundations/pandas/#scaling","title":"Scaling","text":"<p>When working with very large datasets, our Pandas DataFrames can become very large and it can be very slow or impossible to operate on them. This is where packages that can distribute workloads or run on more efficient hardware can come in handy.</p> <ul> <li>Dask: parallel computing to scale packages like Numpy, Pandas and scikit-learn on one/multiple machines.</li> <li>cuDF: efficient dataframe loading and computation on a GPU.</li> </ul> <p>And, of course, we can combine these together (Dask-cuDF) to operate on partitions of a dataframe on the GPU.</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Pandas - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/python/","title":"Python for Machine Learning","text":""},{"location":"courses/foundations/python/#variables","title":"Variables","text":"<p>Variables are containers for holding data and they're defined by a name and value.</p> <pre><code># Integer variable\nx = 5\nprint (x)\nprint (type(x))\n</code></pre> <pre>\n5\n&lt;class 'int'&gt;\n</pre> <p>Here we use the variable name <code>x</code> in our examples but when you're working on a specific task, be sure to be explicit (ex. <code>first_name</code>) when creating variables (applies to functions, classes, etc. as well).</p> <p>We can change the value of a variable by simply assigning a new value to it.</p> <pre><code># String variable\nx = \"hello\"\nprint (x)\nprint (type(x))\n</code></pre> <pre>\nhello\n&lt;class 'str'&gt;\n</pre> <p>There are many different types of variables: integers, floats, strings, boolean etc. <pre><code># int variable\nx = 5\nprint (x, type(x))\n</code></pre></p> <pre>\n5 &lt;class 'int'&gt;\n</pre> <pre><code># float variable\nx = 5.0\nprint (x, type(x))\n</code></pre> <pre>\n5.0 &lt;class 'float'&gt;\n</pre> <pre><code># text variable\nx = \"5\"\nprint (x, type(x))\n</code></pre> <pre>\n5 &lt;class 'str'&gt;\n</pre> <pre><code># boolean variable\nx = True\nprint (x, type(x))\n</code></pre> <pre>\nTrue &lt;class 'bool'&gt;\n</pre> <p>We can also do operations with variables: <pre><code># Variables can be used with each other\na = 1\nb = 2\nc = a + b\nprint (c)\n</code></pre></p> <pre>\n3\n</pre> <p>Know your types!</p> <p>We should always know what types of variables we're dealing with so we can do the right operations with them. Here's a common mistake that can happen if we're using the wrong variable type. <pre><code># int variables\na = 5\nb = 3\nprint (a + b)\n</code></pre></p> Show answer <p><pre>\n8\n</pre></p> <pre><code># string variables\na = \"5\"\nb = \"3\"\nprint (a + b)\n</code></pre> Show answer <p><pre>\n53\n</pre></p>"},{"location":"courses/foundations/python/#lists","title":"Lists","text":"<p>Lists are an ordered, mutable (changeable) collection of values that are comma separated and enclosed by square brackets. A list can be comprised of many different types of variables. Below is a list with an integer, string and a float:</p> <pre><code># Creating a list\nx = [3, \"hello\", 1.2]\nprint (x)\n</code></pre> <pre>\n[3, 'hello', 1.2]\n</pre> <pre><code># Length of a list\nlen(x)\n</code></pre> <pre>\n3\n</pre> <p>We can add to a list by using the append function: <pre><code># Adding to a list\nx.append(7)\nprint (x)\nprint (len(x))\n</code></pre></p> <pre>\n[3, 'hello', 1.2, 7]\n4\n</pre> <p>and just as easily replace existing items: <pre><code># Replacing items in a list\nx[1] = \"bye\"\nprint (x)\n</code></pre></p> <pre>\n[3, 'bye', 1.2, 7]\n</pre> <p>and perform operations with lists: <pre><code># Operations\ny = [2.4, \"world\"]\nz = x + y\nprint (z)\n</code></pre></p> <pre>\n[3, 'bye', 1.2, 7, 2.4, 'world']\n</pre>"},{"location":"courses/foundations/python/#tuples","title":"Tuples","text":"<p>Tuples are collections that are ordered and immutable (unchangeable). We will use tuples to store values that will never be changed. <pre><code># Creating a tuple\nx = (3.0, \"hello\") # tuples start and end with ()\nprint (x)\n</code></pre></p> <pre>\n(3.0, 'hello')\n</pre> <pre><code># Adding values to a tuple\nx = x + (5.6, 4)\nprint (x)\n</code></pre> <pre>\n(3.0, 'hello', 5.6, 4)\n</pre> <pre><code># Try to change (it won't work and we get an error)\nx[0] = 1.2\n</code></pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n----&gt; 1 x[0] = 1.2\nTypeError: 'tuple' object does not support item assignment\n</pre>"},{"location":"courses/foundations/python/#sets","title":"Sets","text":"<p>Sets are collections that are unordered and mutable. However, every item in a set much be unique.</p> <pre><code># Sets\ntext = \"Learn ML with Made With ML\"\nprint (set(text))\nprint (set(text.split(\" \")))\n</code></pre> <pre>\n{'e', 'M', ' ', \"r\", \"w\", 'd', 'a', 'h', 't', 'i', 'L', 'n', \"w\"}\n{'with', 'Learn', 'ML', 'Made', 'With'}\n</pre>"},{"location":"courses/foundations/python/#indexing","title":"Indexing","text":"<p>Indexing and slicing from lists allow us to retrieve specific values within lists. Note that indices can be positive (starting from 0) or negative (-1 and lower, where -1 is the last item in the list).</p> <pre><code># Indexing\nx = [3, \"hello\", 1.2]\nprint (\"x[0]: \", x[0])\nprint (\"x[1]: \", x[1])\nprint (\"x[-1]: \", x[-1]) # the last item\nprint (\"x[-2]: \", x[-2]) # the second to last item\n</code></pre> <pre>\nx[0]:  3\nx[1]:  hello\nx[-1]:  1.2\nx[-2]:  hello\n</pre> <pre><code># Slicing\nprint (\"x[:]: \", x[:]) # all indices\nprint (\"x[1:]: \", x[1:]) # index 1 to the end of the list\nprint (\"x[1:2]: \", x[1:2]) # index 1 to index 2 (not including index 2)\nprint (\"x[:-1]: \", x[:-1]) # index 0 to last index (not including last index)\n</code></pre> <pre>\nx[:]:  [3, 'hello', 1.2]\nx[1:]:  ['hello', 1.2]\nx[1:2]:  ['hello']\nx[:-1]:  [3, 'hello']\n</pre> <p>Indexing beyond length</p> <p>What happens if we try to index beyond the length of a list? <pre><code>x = [3, \"hello\", 1.2]\nprint (x[:100])\nprint (len(x[:100]))\n</code></pre></p> Show answer <p><pre>\n[3, 'hello', 1.2]\n3\n</pre> Though this does produce results, we should always explicitly use the length of the list to index items from it to avoid incorrect assumptions for downstream processes.</p>"},{"location":"courses/foundations/python/#dictionaries","title":"Dictionaries","text":"<p>Dictionaries are an unordered, mutable collection of key-value pairs. You can retrieve values based on the key and a dictionary cannot have two of the same keys.</p> <pre><code># Creating a dictionary\nperson = {\"name\": \"Goku\",\n          \"eye_color\": \"brown\"}\nprint (person)\nprint (person[\"name\"])\nprint (person[\"eye_color\"])\n</code></pre> <pre>\n{\"name\": \"Goku\", \"eye_color\": \"brown\"}\nGoku\nbrown\n</pre> <pre><code># Changing the value for a key\nperson[\"eye_color\"] = \"green\"\nprint (person)\n</code></pre> <pre>\n{\"name\": \"Goku\", \"eye_color\": \"green\"}\n</pre> <pre><code># Adding new key-value pairs\nperson[\"age\"] = 24\nprint (person)\n</code></pre> <pre>\n{\"name\": \"Goku\", \"eye_color\": \"green\", \"age\": 24}\n</pre> <pre><code># Length of a dictionary\nprint (len(person))\n</code></pre> <pre>\n3\n</pre> <p>Sort of the structures</p> <p>See if you can recall and sort out the similarities and differences of the foundational data structures we've seen so far.</p> Mutable Ordered Indexable Unique List \u2753 \u2753 \u2753 \u2753 Tuple \u2753 \u2753 \u2753 \u2753 Set \u2753 \u2753 \u2753 \u2753 Dictionary \u2753 \u2753 \u2753 \u2753 Show answer Mutable Ordered Indexable Unique List \u2705 \u2705 \u2705 \u274c Tuple \u274c \u2705 \u2705 \u274c Set \u2705 \u274c \u274c \u2705 Dictionary \u2705 \u274c \u274c \u2705 \u00a0keys\u274c \u00a0values <p>But of course, there is pretty much a way to do accomplish anything with Python. For example, even though native dictionaries are unordered, we can leverage the OrderedDict data structure to change that (useful if we want to iterate through keys in a certain order, etc.).</p> <pre><code>from collections import OrderedDict\n</code></pre> <pre><code># Native dict\nd = {}\nd[\"a\"] = 2\nd[\"c\"] = 3\nd[\"b\"] = 1\nprint (d)\n</code></pre> <pre>\n{'a': 2, 'c': 3, 'b': 1}\n</pre> <p>After Python 3.7+, native dictionaries are insertion ordered.</p> <pre><code># Dictionary items\nprint (d.items())\n</code></pre> <pre>\ndict_items([('a', 2), ('c', 3), ('b', 1)])\n</pre> <pre><code># Order by keys\nprint (OrderedDict(sorted(d.items())))\n</code></pre> <pre>\nOrderedDict([('a', 2), ('b', 1), ('c', 3)])\n</pre> <pre><code># Order by values\nprint (OrderedDict(sorted(d.items(), key=lambda x: x[1])))\n</code></pre> <pre>\nOrderedDict([('b', 1), ('a', 2), ('c', 3)])\n</pre>"},{"location":"courses/foundations/python/#if-statements","title":"If statements","text":"<p>We can use <code>if</code> statements to conditionally do something. The conditions are defined by the words <code>if</code>, <code>elif</code> (which stands for else if) and <code>else</code>. We can have as many <code>elif</code> statements as we want. The indented code below each condition is the code that will execute if the condition is <code>True</code>.</p> <pre><code># If statement\nx = 4\nif x &lt; 1:\n    score = \"low\"\nelif x &lt;= 4: # elif = else if\n    score = \"medium\"\nelse:\n    score = \"high\"\nprint (score)\n</code></pre> <pre>\nmedium\n</pre> <pre><code># If statement with a boolean\nx = True\nif x:\n    print (\"it worked\")\n</code></pre> <pre>\nit worked\n</pre>"},{"location":"courses/foundations/python/#loops","title":"Loops","text":""},{"location":"courses/foundations/python/#for-loops","title":"For loops","text":"<p>A <code>for</code> loop can iterate over a collection of values (lists, tuples, dictionaries, etc.) The indented code is executed for each item in the collection of values. <pre><code># For loop\nveggies = [\"carrots\", \"broccoli\", \"beans\"]\nfor veggie in veggies:\n    print (veggie)\n</code></pre></p> <pre>\ncarrots\nbroccoli\nbeans\n</pre> <p>When the loop encounters the break command, the loop will terminate immediately. If there were more items in the list, they will not be processed. <pre><code># `break` from a for loop\nveggies = [\"carrots\", \"broccoli\", \"beans\"]\nfor veggie in veggies:\n    if veggie == \"broccoli\":\n        break\n    print (veggie)\n</code></pre></p> <pre>\ncarrots\n</pre> <p>When the loop encounters the <code>continue</code> command, the loop will skip all other operations for that item in the list only. If there were more items in the list, the loop will continue normally. <pre><code># `continue` to the next iteration\nveggies = [\"carrots\", \"broccoli\", \"beans\"]\nfor veggie in veggies:\n    if veggie == \"broccoli\":\n        continue\n    print (veggie)\n</code></pre></p> <pre>\ncarrots\nbeans\n</pre>"},{"location":"courses/foundations/python/#while-loops","title":"While loops","text":"<p>A <code>while</code> loop can perform repeatedly as long as a condition is <code>True</code>. We can use <code>continue</code> and <code>break</code> commands in <code>while</code> loops as well. <pre><code># While loop\nx = 3\nwhile x &gt; 0:\n    x -= 1 # same as x = x - 1\n    print (x)\n</code></pre></p> <pre>\n2\n1\n0\n</pre>"},{"location":"courses/foundations/python/#list-comprehension","title":"List comprehension","text":"<p>We can combine our knowledge of lists and for loops to leverage list comprehensions to create succinct code.</p> <pre><code># For loop\nx = [1, 2, 3, 4, 5]\ny = []\nfor item in x:\n    if item &gt; 2:\n        y.append(item)\nprint (y)\n</code></pre> <pre>\n[3, 4, 5]\n</pre> <pre><code># List comprehension\ny = [item for item in x if item &gt; 2]\nprint (y)\n</code></pre> <pre>\n[3, 4, 5]\n</pre> <p>List comprehension for nested for loops</p> <p>For the nested for loop below, which list comprehension is correct?</p> <p><pre><code># Nested for loops\nwords = [[\"Am\", \"ate\", \"ATOM\", \"apple\"], [\"bE\", \"boy\", \"ball\", \"bloom\"]]\nsmall_words = []\nfor letter_list in words:\n    for word in letter_list:\n        if len(word) &lt; 3:\n            small_words.append(word.lower())\nprint (small_words)\n</code></pre> <pre>\n['am', 'be']\n</pre></p> <ul> <li> <code>[word.lower() if len(word) &lt; 3 for word in letter_list for letter_list in words]</code></li> <li> <code>[word.lower() for word in letter_list for letter_list in words if len(word) &lt; 3]</code></li> <li> <code>[word.lower() for letter_list in words for word in letter_list if len(word) &lt; 3]</code></li> </ul> Show answer <p>Python syntax is usually very straight forward, so the correct answer involves just directly copying the statements from the nested for loop from top to bottom!</p> <ul> <li> <code>[word.lower() if len(word) &lt; 3 for word in letter_list for letter_list in words]</code></li> <li> <code>[word.lower() for word in letter_list for letter_list in words if len(word) &lt; 3]</code></li> <li> <code>[word.lower() for letter_list in words for word in letter_list if len(word) &lt; 3]</code></li> </ul>"},{"location":"courses/foundations/python/#functions","title":"Functions","text":"<p>Functions are a way to modularize reusable pieces of code. They're defined by the keyword <code>def</code> which stands for definition and they can have the following components.</p> <pre><code># Define the function\ndef add_two(x):\n\"\"\"Increase x by 2.\"\"\"\n    x += 2\n    return x\n</code></pre> <p>Here are the components that may be required when we want to use the function. we need to ensure that the function name and the input parameters match with how we defined the function above.</p> <pre><code># Use the function\nscore = 0\nnew_score = add_two(x=score)\nprint (new_score)\n</code></pre> <pre>\n2\n</pre> <p>A function can have as many input parameters and outputs as we want. <pre><code># Function with multiple inputs\ndef join_name(first_name, last_name):\n\"\"\"Combine first name and last name.\"\"\"\n    joined_name = first_name + \" \" + last_name\n    return joined_name\n</code></pre></p> <pre><code># Use the function\nfirst_name = \"Goku\"\nlast_name = \"Mohandas\"\njoined_name = join_name(\n    first_name=first_name, last_name=last_name)\nprint (joined_name)\n</code></pre> <pre>\nGoku Mohandas\n</pre> <p>We can be even more explicit with our function definitions by specifying the types of our input and output arguments. We cover this in our documentation lesson because the typing information is automatically leveraged to create very intuitive documentation.</p> <p>It's good practice to always use keyword argument when using a function so that it's very clear what input variable belongs to what function input parameter. On a related note, you will often see the terms <code>*args</code> and <code>**kwargs</code> which stand for arguments and keyword arguments. You can extract them when they are passed into a function. The significance of the <code>*</code> is that any number of arguments and keyword arguments can be passed into the function.</p> <p><pre><code>def f(*args, **kwargs):\n    x = args[0]\n    y = kwargs.get(\"y\")\n    print (f\"x: {x}, y: {y}\")\n</code></pre> <pre><code>f(5, y=2)\n</code></pre></p> <pre>\nx: 5, y: 2\n</pre>"},{"location":"courses/foundations/python/#classes","title":"Classes","text":"<p>Classes are object constructors and are a fundamental component of object oriented programming in Python. They are composed of a set of functions that define the class and it's operations.</p>"},{"location":"courses/foundations/python/#magic-methods","title":"Magic methods","text":"<p>Classes can be customized with magic methods like <code>__init__</code> and <code>__str__</code>, to enable powerful operations. These are also known as dunder methods (ex. dunder init), which stands for <code>d</code>ouble <code>under</code>scores due to the leading and trailing underscores.</p> <p>The <code>__init__</code> function is used when an instance of the class is initialized. <pre><code># Creating the class\nclass Pet(object):\n\"\"\"Class object for a pet.\"\"\"\n\n    def __init__(self, species, name):\n\"\"\"Initialize a Pet.\"\"\"\n        self.species = species\n        self.name = name\n</code></pre> <pre><code># Creating an instance of a class\nmy_dog = Pet(species=\"dog\",\n             name=\"Scooby\")\nprint (my_dog)\nprint (my_dog.name)\n</code></pre></p> <pre>\n&lt;__main__.Pet object at 0x7fe487e9c358&gt;\nScooby\n</pre> <p>The <code>print (my_dog)</code> command printed something not so relevant to us. Let's fix that with the <code>__str__</code> function. <pre><code># Creating the class\n# Creating the class\nclass Pet(object):\n\"\"\"Class object for a pet.\"\"\"\n\n    def __init__(self, species, name):\n\"\"\"Initialize a Pet.\"\"\"\n        self.species = species\n        self.name = name\n\n    def __str__(self):\n\"\"\"Output when printing an instance of a Pet.\"\"\"\n        return f\"{self.species} named {self.name}\"\n</code></pre> <pre><code># Creating an instance of a class\nmy_dog = Pet(species=\"dog\",\n             name=\"Scooby\")\nprint (my_dog)\nprint (my_dog.name)\n</code></pre></p> <pre>\ndog named Scooby\nScooby\n</pre> <p>We'll be exploring additional built-in functions in subsequent notebooks (like <code>__len__</code>, <code>__iter__</code> and <code>__getitem__</code>, etc.) but if you're curious, here is a tutorial on more magic methods.</p>"},{"location":"courses/foundations/python/#object-functions","title":"Object functions","text":"<p>Besides these magic functions, classes can also have object functions. <pre><code># Creating the class\nclass Pet(object):\n\"\"\"Class object for a pet.\"\"\"\n\n    def __init__(self, species, name):\n\"\"\"Initialize a Pet.\"\"\"\n        self.species = species\n        self.name = name\n\n    def __str__(self):\n\"\"\"Output when printing an instance of a Pet.\"\"\"\n        return f\"{self.species} named {self.name}\"\n\n    def change_name(self, new_name):\n\"\"\"Change the name of your Pet.\"\"\"\n        self.name = new_name\n</code></pre> <pre><code># Creating an instance of a class\nmy_dog = Pet(species=\"dog\", name=\"Scooby\")\nprint (my_dog)\nprint (my_dog.name)\n</code></pre></p> <pre>\ndog named Scooby\nScooby\n</pre> <pre><code># Using a class's function\nmy_dog.change_name(new_name=\"Scrappy\")\nprint (my_dog)\nprint (my_dog.name)\n</code></pre> <pre>\ndog named Scrappy\nScrappy\n</pre>"},{"location":"courses/foundations/python/#inheritance","title":"Inheritance","text":"<p>We can also build classes on top of one another using inheritance, which allows us to inherit all the properties and methods from another class (the parent). <pre><code>class Dog(Pet):\n    def __init__(self, name, breed):\n        super().__init__(species=\"dog\", name=name)\n        self.breed = breed\n\n    def __str__(self):\n        return f\"A {self.breed} doggo named {self.name}\"\n</code></pre> <pre><code>scooby = Dog(species=\"dog\", breed=\"Great Dane\", name=\"Scooby\")\nprint (scooby)\n</code></pre></p> <pre>\nA Great Dane doggo named Scooby\n</pre> <pre><code>scooby.change_name(\"Scooby Doo\")\nprint (scooby)\n</code></pre> <pre>\nA Great Dane doggo named Scooby Doo\n</pre> <p>Notice how we inherited the initialized variables from the parent <code>Pet</code> class like species and name. We also inherited functions such as <code>change_name()</code>.</p> <p>Which function is executed?</p> <p>Which function is executed if the parent and child functions have functions with the same name?</p> Show answer <p>As you can see, both our parent class (<code>Pet</code>) and the child class (<code>Dog</code>) have different <code>__str__</code> functions defined but share the same function name. The child class inherits everything from the parent classes but when there is conflict between function names, the child class' functions take precedence and overwrite the parent class' functions.</p>"},{"location":"courses/foundations/python/#methods","title":"Methods","text":"<p>There are two important decorator methods to know about when it comes to classes: <code>@classmethod</code> and <code>@staticmethod</code>. We'll learn about decorators in the next section below but these specific methods pertain to classes so we'll cover them here.</p> <pre><code>class Dog(Pet):\n    def __init__(self, name, breed):\n        super().__init__(species=\"dog\", name=name)\n        self.breed = breed\n\n    def __str__(self):\n        return f\"{self.breed} named {self.name}\"\n\n    @classmethod\n    def from_dict(cls, d):\n        return cls(name=d[\"name\"], breed=d[\"breed\"])\n\n    @staticmethod\n    def is_cute(breed):\n        return True  # all animals are cute!\n</code></pre> <p>A <code>@classmethod</code> allows us to create class instances by passing in the uninstantiated class itself (<code>cls</code>). This is a great way to create (or load) classes from objects (ie. dictionaries).</p> <pre><code># Create instance\nd = {\"name\": \"Cassie\", \"breed\": \"Border Collie\"}\ncassie = Dog.from_dict(d=d)\nprint(cassie)\n</code></pre> <pre>\nBorder Collie named Cassie\n</pre> <p>A <code>@staticmethod</code> can be called from an uninstantiated class object so we can do things like this: <pre><code># Static method\nDog.is_cute(breed=\"Border Collie\")\n</code></pre></p> <pre>\nTrue\n</pre>"},{"location":"courses/foundations/python/#decorators","title":"Decorators","text":"<p>Recall that functions allow us to modularize code and reuse them. However, we'll often want to add some functionality before or after the main function executes and we may want to do this for many different functions. Instead of adding more code to the original function, we can use decorators!</p> <ul> <li>decorators: augment a function with pre/post-processing. Decorators wrap around the main function and allow us to operate on the inputs and or outputs.</li> </ul> <p>Suppose we have a function called operations which increments the input value x by 1. <pre><code>def operations(x):\n\"\"\"Basic operations.\"\"\"\n    x += 1\n    return x\n</code></pre> <pre><code>operations(x=1)\n</code></pre></p> <pre>\n2\n</pre> <p>Now let's say we want to increment our input x by 1 before and after the operations function executes and, to illustrate this example, let's say the increments have to be separate steps. Here's how we would do it by changing the original code: <pre><code>def operations(x):\n\"\"\"Basic operations.\"\"\"\n    x += 1\n    x += 1\n    x += 1\n    return x\n</code></pre> <pre><code>operations(x=1)\n</code></pre></p> <pre>\n4\n</pre> <p>We were able to achieve what we want but we now increased the size of our <code>operations</code> function and if we want to do the same incrementing for any other function, we have to add the same code to all of those as well ... not very efficient. To solve this, let's create a decorator called <code>add</code> which increments <code>x</code> by 1 before and after the main function <code>f</code> executes.</p>"},{"location":"courses/foundations/python/#creating-a-decorator","title":"Creating a decorator","text":"<p>The decorator function accepts a function <code>f</code> which is the function we wish to wrap around, in our case, it's <code>operations()</code>. The output of the decorator is its <code>wrapper</code> function which receives the arguments and keyword arguments passed to function <code>f</code>.</p> <p>Inside the <code>wrapper</code> function, we can:</p> <ol> <li>extract the input parameters passed to function <code>f</code>.</li> <li>make any changes we want to the function inputs.</li> <li>function <code>f</code> is executed</li> <li>make any changes to the function outputs</li> <li><code>wrapper</code> function returns some value(s), which is what the decorator returns as well since it returns <code>wrapper</code>.</li> </ol> <pre><code># Decorator\ndef add(f):\n    def wrapper(*args, **kwargs):\n\"\"\"Wrapper function for @add.\"\"\"\n        x = kwargs.pop(\"x\") # .get() if not altering x\n        x += 1 # executes before function f\n        x = f(*args, **kwargs, x=x)\n        x += 1 # executes after function f\n        return x\n    return wrapper\n</code></pre> <pre></pre> <p>We can use this decorator by simply adding it to the top of our main function preceded by the <code>@</code> symbol. <pre><code>@add\ndef operations(x):\n\"\"\"Basic operations.\"\"\"\n    x += 1\n    return x\n</code></pre> <pre><code>operations(x=1)\n</code></pre></p> <pre>\n4\n</pre> <p>Suppose we wanted to debug and see what function actually executed with <code>operations()</code>. <pre><code>operations.__name__, operations.__doc__\n</code></pre></p> <pre>\n('wrapper', 'Wrapper function for @add.')\n</pre> <p>The function name and docstring are not what we're looking for but it appears this way because the <code>wrapper</code> function is what was executed. In order to fix this, Python offers <code>functools.wraps</code> which carries the main function's metadata. <pre><code>from functools import wraps\n</code></pre> <pre><code># Decorator\ndef add(f):\n    @wraps(f)\n    def wrap(*args, **kwargs):\n\"\"\"Wrapper function for @add.\"\"\"\n        x = kwargs.pop(\"x\")\n        x += 1\n        x = f(*args, **kwargs, x=x)\n        x += 1\n        return x\n    return wrap\n</code></pre> <pre><code>@add\ndef operations(x):\n\"\"\"Basic operations.\"\"\"\n    x += 1\n    return x\n</code></pre> <pre><code>operations.__name__, operations.__doc__\n</code></pre></p> <pre>\n('operations', 'Basic operations.')\n</pre> <p>Awesome! We were able to decorate our main function <code>operation()</code> to achieve the customization we wanted without actually altering the function. We can reuse our decorator for other functions that may need the same customization!</p> <p>This was a dummy example to show how decorators work but we'll be using them heavily during our MLOps lessons. A simple scenario would be using decorators to create uniform JSON responses from each API endpoint without including the bulky code in each endpoint.</p>"},{"location":"courses/foundations/python/#callbacks","title":"Callbacks","text":"<p>Decorators allow for customized operations before and after the main function's execution but what about in between? Suppose we want to conditionally/situationally do some operations. Instead of writing a whole bunch of if-statements and make our functions bulky, we can use callbacks!</p> <ul> <li>callbacks: conditional/situational processing within the function.</li> </ul> <p>Our callbacks will be classes that have functions with key names that will execute at various periods during the main function's execution. The function names are up to us but we need to invoke the same callback functions within our main function. <pre><code># Callback\nclass x_tracker(object):\n    def __init__(self, x):\n        self.history = []\n    def at_start(self, x):\n        self.history.append(x)\n    def at_end(self, x):\n        self.history.append(x)\n</code></pre> We can pass in as many callbacks as we want and because they have appropriately named functions, they will be invoked at the appropriate times. <pre><code>def operations(x, callbacks=[]):\n\"\"\"Basic operations.\"\"\"\n    for callback in callbacks:\n        callback.at_start(x)\n    x += 1\n    for callback in callbacks:\n        callback.at_end(x)\n    return x\n</code></pre> <pre><code>x = 1\ntracker = x_tracker(x=x)\noperations(x=x, callbacks=[tracker])\n</code></pre></p> <pre>\n2\n</pre> <pre><code>tracker.history\n</code></pre> <pre>\n[1, 2]\n</pre> <p>What's the difference compared to a decorator?</p> <p>It seems like we've just done some operations before and after the function's main process? Isn't that what a decorator is for?</p> Show answer <p>With callbacks, it's easier to keep track of objects since it's all defined in a separate callback class. It's also now possible to interact with our function, not just before or after but throughout the entire process! Imagine a function with:</p> <ul> <li>multiple processes where we want to execute operations in between them</li> <li>execute operations repeatedly when loops are involved in functions</li> </ul>"},{"location":"courses/foundations/python/#putting-it-all-together","title":"Putting it all together","text":"<p>decorators + callbacks = powerful customization before, during and after the main function\u2019s execution without increasing its complexity. We will be using this duo to create powerful ML training scripts that are highly customizable in future lessons.</p> <p><pre><code>from functools import wraps\n</code></pre> <pre><code># Decorator\ndef add(f):\n    @wraps(f)\n    def wrap(*args, **kwargs):\n\"\"\"Wrapper function for @add.\"\"\"\n        x = kwargs.pop(\"x\") # .get() if not altering x\n        x += 1 # executes before function f\n        x = f(*args, **kwargs, x=x)\n        # can do things post function f as well\n        return x\n    return wrap\n</code></pre> <pre><code># Callback\nclass x_tracker(object):\n    def __init__(self, x):\n        self.history = [x]\n    def at_start(self, x):\n        self.history.append(x)\n    def at_end(self, x):\n        self.history.append(x)\n</code></pre> <pre><code># Main function\n@add\ndef operations(x, callbacks=[]):\n\"\"\"Basic operations.\"\"\"\n    for callback in callbacks:\n        callback.at_start(x)\n    x += 1\n    for callback in callbacks:\n        callback.at_end(x)\n    return x\n</code></pre> <pre><code>x = 1\ntracker = x_tracker(x=x)\noperations(x=x, callbacks=[tracker])\n</code></pre></p> <pre>\n3\n</pre> <pre><code>tracker.history\n</code></pre> <pre>\n[1, 2, 3]\n</pre> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Python - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/pytorch/","title":"PyTorch Fundamentals","text":""},{"location":"courses/foundations/pytorch/#set-up","title":"Set up","text":"<p>We'll import PyTorch and set seeds for reproducibility. Note that PyTorch also required a seed since we will be generating random tensors. <pre><code>import numpy as np\nimport torch\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code># Set seed for reproducibility\nnp.random.seed(seed=SEED)\ntorch.manual_seed(SEED)\n</code></pre></p> <pre>"},{"location":"courses/foundations/pytorch/#basics","title":"Basics","text":"<p>We'll first cover some basics with PyTorch such as creating tensors and converting from common data structures (lists, arrays, etc.) to tensors.\n<pre><code># Creating a random tensor\nx = torch.randn(2, 3) # normal distribution (rand(2,3) -&gt; uniform distribution)\nprint(f\"Type: {x.type()}\")\nprint(f\"Size: {x.shape}\")\nprint(f\"Values: \\n{x}\")\n</code></pre></p>\n<pre>\nType: torch.FloatTensor\nSize: torch.Size([2, 3])\nValues:\ntensor([[ 0.0461,  0.4024, -1.0115],\n        [ 0.2167, -0.6123,  0.5036]])\n</pre>\n<pre><code># Zero and Ones tensor\nx = torch.zeros(2, 3)\nprint (x)\nx = torch.ones(2, 3)\nprint (x)\n</code></pre>\n<pre>\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n</pre>\n<pre><code># List \u2192 Tensor\nx = torch.Tensor([[1, 2, 3],[4, 5, 6]])\nprint(f\"Size: {x.shape}\")\nprint(f\"Values: \\n{x}\")\n</code></pre>\n<pre>\nSize: torch.Size([2, 3])\nValues:\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\n</pre>\n<pre><code># NumPy array \u2192 Tensor\nx = torch.Tensor(np.random.rand(2, 3))\nprint(f\"Size: {x.shape}\")\nprint(f\"Values: \\n{x}\")\n</code></pre>\n<pre>\nSize: torch.Size([2, 3])\nValues:\ntensor([[0.1915, 0.6221, 0.4377],\n        [0.7854, 0.7800, 0.2726]])\n</pre>\n<pre><code># Changing tensor type\nx = torch.Tensor(3, 4)\nprint(f\"Type: {x.type()}\")\nx = x.long()\nprint(f\"Type: {x.type()}\")\n</code></pre>\n<pre>\nType: torch.FloatTensor\nType: torch.LongTensor\n</pre>"},{"location":"courses/foundations/pytorch/#operations","title":"Operations","text":"<p>Now we'll explore some basic operations with tensors.\n<pre><code># Addition\nx = torch.randn(2, 3)\ny = torch.randn(2, 3)\nz = x + y\nprint(f\"Size: {z.shape}\")\nprint(f\"Values: \\n{z}\")\n</code></pre></p>\n<pre>\nSize: torch.Size([2, 3])\nValues:\ntensor([[ 0.0761, -0.6775, -0.3988],\n        [ 3.0633, -0.1589,  0.3514]])\n</pre>\n<pre><code># Dot product\nx = torch.randn(2, 3)\ny = torch.randn(3, 2)\nz = torch.mm(x, y)\nprint(f\"Size: {z.shape}\")\nprint(f\"Values: \\n{z}\")\n</code></pre>\n<pre>\nSize: torch.Size([2, 2])\nValues:\ntensor([[ 1.0796, -0.0759],\n        [ 1.2746, -0.5134]])\n</pre>\n<pre><code># Transpose\nx = torch.randn(2, 3)\nprint(f\"Size: {x.shape}\")\nprint(f\"Values: \\n{x}\")\ny = torch.t(x)\nprint(f\"Size: {y.shape}\")\nprint(f\"Values: \\n{y}\")\n</code></pre>\n<pre>\nSize: torch.Size([2, 3])\nValues:\ntensor([[ 0.8042, -0.1383,  0.3196],\n        [-1.0187, -1.3147,  2.5228]])\nSize: torch.Size([3, 2])\nValues:\ntensor([[ 0.8042, -1.0187],\n        [-0.1383, -1.3147],\n        [ 0.3196,  2.5228]])\n</pre>\n<pre><code># Reshape\nx = torch.randn(2, 3)\nz = x.view(3, 2)\nprint(f\"Size: {z.shape}\")\nprint(f\"Values: \\n{z}\")\n</code></pre>\n<pre>\nSize: torch.Size([3, 2])\nValues:\ntensor([[ 0.4501,  0.2709],\n        [-0.8087, -0.0217],\n        [-1.0413,  0.0702]])\n</pre>\n<pre><code># Dangers of reshaping (unintended consequences)\nx = torch.tensor([\n    [[1,1,1,1], [2,2,2,2], [3,3,3,3]],\n    [[10,10,10,10], [20,20,20,20], [30,30,30,30]]\n])\nprint(f\"Size: {x.shape}\")\nprint(f\"x: \\n{x}\\n\")\n\na = x.view(x.size(1), -1)\nprint(f\"\\nSize: {a.shape}\")\nprint(f\"a: \\n{a}\\n\")\n\nb = x.transpose(0,1).contiguous()\nprint(f\"\\nSize: {b.shape}\")\nprint(f\"b: \\n{b}\\n\")\n\nc = b.view(b.size(0), -1)\nprint(f\"\\nSize: {c.shape}\")\nprint(f\"c: \\n{c}\")\n</code></pre>\n<pre>\nSize: torch.Size([2, 3, 4])\nx:\ntensor([[[ 1,  1,  1,  1],\n         [ 2,  2,  2,  2],\n         [ 3,  3,  3,  3]],\n\n        [[10, 10, 10, 10],\n         [20, 20, 20, 20],\n         [30, 30, 30, 30]]])\n\n\nSize: torch.Size([3, 8])\na:\ntensor([[ 1,  1,  1,  1,  2,  2,  2,  2],\n        [ 3,  3,  3,  3, 10, 10, 10, 10],\n        [20, 20, 20, 20, 30, 30, 30, 30]])\n\n\nSize: torch.Size([3, 2, 4])\nb:\ntensor([[[ 1,  1,  1,  1],\n         [10, 10, 10, 10]],\n\n        [[ 2,  2,  2,  2],\n         [20, 20, 20, 20]],\n\n        [[ 3,  3,  3,  3],\n         [30, 30, 30, 30]]])\n\n\nSize: torch.Size([3, 8])\nc:\ntensor([[ 1,  1,  1,  1, 10, 10, 10, 10],\n        [ 2,  2,  2,  2, 20, 20, 20, 20],\n        [ 3,  3,  3,  3, 30, 30, 30, 30]])\n</pre>\n<pre><code># Dimensional operations\nx = torch.randn(2, 3)\nprint(f\"Values: \\n{x}\")\ny = torch.sum(x, dim=0) # add each row's value for every column\nprint(f\"Values: \\n{y}\")\nz = torch.sum(x, dim=1) # add each columns's value for every row\nprint(f\"Values: \\n{z}\")\n</code></pre>\n<pre>\nValues:\ntensor([[ 0.5797, -0.0599,  0.1816],\n        [-0.6797, -0.2567, -1.8189]])\nValues:\ntensor([-0.1000, -0.3166, -1.6373])\nValues:\ntensor([ 0.7013, -2.7553])\n</pre>"},{"location":"courses/foundations/pytorch/#indexing","title":"Indexing","text":"<p>Now we'll look at how to extract, separate and join values from our tensors.\n<pre><code>x = torch.randn(3, 4)\nprint (f\"x: \\n{x}\")\nprint (f\"x[:1]: \\n{x[:1]}\")\nprint (f\"x[:1, 1:3]: \\n{x[:1, 1:3]}\")\n</code></pre></p>\n<pre>\nx:\ntensor([[ 0.2111,  0.3372,  0.6638,  1.0397],\n        [ 1.8434,  0.6588, -0.2349, -0.0306],\n        [ 1.7462, -0.0722, -1.6794, -1.7010]])\nx[:1]:\ntensor([[0.2111, 0.3372, 0.6638, 1.0397]])\nx[:1, 1:3]:\ntensor([[0.3372, 0.6638]])\n</pre>"},{"location":"courses/foundations/pytorch/#slicing","title":"Slicing","text":"<pre><code># Select with dimensional indices\nx = torch.randn(2, 3)\nprint(f\"Values: \\n{x}\")\n\ncol_indices = torch.LongTensor([0, 2])\nchosen = torch.index_select(x, dim=1, index=col_indices) # values from column 0 &amp; 2\nprint(f\"Values: \\n{chosen}\")\n\nrow_indices = torch.LongTensor([0, 1])\ncol_indices = torch.LongTensor([0, 2])\nchosen = x[row_indices, col_indices] # values from (0, 0) &amp; (1, 2)\nprint(f\"Values: \\n{chosen}\")\n</code></pre>\n<pre>\nValues:\ntensor([[ 0.6486,  1.7653,  1.0812],\n        [ 1.2436,  0.8971, -0.0784]])\nValues:\ntensor([[ 0.6486,  1.0812],\n        [ 1.2436, -0.0784]])\nValues:\ntensor([ 0.6486, -0.0784])\n</pre>"},{"location":"courses/foundations/pytorch/#joining","title":"Joining","text":"<p>We can also combine our tensors via concatenation or stacking operations, which are consistent with NumPy's joining functions' behaviors as well.</p>\n<pre><code>x = torch.randn(2, 3)\nprint (x)\nprint (x.shape)\n</code></pre>\n<pre>\ntensor([[-1.5944, -0.4218, -1.8219],\n        [ 1.7446,  1.2058, -0.7753]])\ntorch.Size([2, 3])\n</pre>\n\n<pre><code># Concatenation\ny = torch.cat([x, x], dim=0) # concat on a specified dimension\nprint (y)\nprint (y.shape)\n</code></pre>\n<pre>\ntensor([[-1.5944, -0.4218, -1.8219],\n        [ 1.7446,  1.2058, -0.7753],\n        [-1.5944, -0.4218, -1.8219],\n        [ 1.7446,  1.2058, -0.7753]])\ntorch.Size([4, 3])\n</pre>\n\n<pre><code># Stacking\nz = torch.stack([x, x], dim=0) # stack on new dimension\nprint (z)\nprint (z.shape)\n</code></pre>\n<pre>\ntensor([[[-1.5944, -0.4218, -1.8219],\n         [ 1.7446,  1.2058, -0.7753]],\n\n        [[-1.5944, -0.4218, -1.8219],\n         [ 1.7446,  1.2058, -0.7753]]])\ntorch.Size([2, 2, 3])\n</pre>"},{"location":"courses/foundations/pytorch/#gradients","title":"Gradients","text":"<p>We can determine gradients (rate of change) of our tensors with respect to their constituents using gradient bookkeeping. The gradient is a vector that points in the direction of greatest increase of a function. We'll be using gradients in the next lesson to determine how to change our weights to affect a particular objective function (ex. loss).</p>\n\\[ y = 3x + 2 \\]\n\\[ z = \\sum{y}/N \\]\n\\[ \\frac{\\partial(z)}{\\partial(x)} = \\frac{\\partial(z)}{\\partial(y)} \\frac{\\partial(y)}{\\partial(x)} = \\frac{1}{N} * 3 = \\frac{1}{12} * 3 = 0.25 \\]\n<pre><code># Tensors with gradient bookkeeping\nx = torch.rand(3, 4, requires_grad=True)\ny = 3*x + 2\nz = y.mean()\nz.backward() # z has to be scalar\nprint(f\"x: \\n{x}\")\nprint(f\"x.grad: \\n{x.grad}\")\n</code></pre>\n<pre>\nx:\ntensor([[0.7379, 0.0846, 0.4245, 0.9778],\n        [0.6800, 0.3151, 0.3911, 0.8943],\n        [0.6889, 0.8389, 0.1780, 0.6442]], requires_grad=True)\nx.grad:\ntensor([[0.2500, 0.2500, 0.2500, 0.2500],\n        [0.2500, 0.2500, 0.2500, 0.2500],\n        [0.2500, 0.2500, 0.2500, 0.2500]])\n</pre>"},{"location":"courses/foundations/pytorch/#cuda","title":"CUDA","text":"<p>We also load our tensors onto the GPU for parallelized computation using CUDA (a parallel computing platform and API from Nvidia).\n<pre><code># Is CUDA available?\nprint (torch.cuda.is_available())\n</code></pre></p>\n<pre>\nFalse\n</pre>\n\n<p>If False (CUDA is not available), let's change that by following these steps: Go to Runtime &gt; Change runtime type &gt; Change Hardware accelerator to GPU &gt; Click Save\n<pre><code>import torch\n</code></pre>\n<pre><code># Is CUDA available now?\nprint (torch.cuda.is_available())\n</code></pre></p>\n<pre>\nTrue\n</pre>\n<pre><code># Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (device)\n</code></pre>\n<pre>\ncuda\n</pre>\n<pre><code>x = torch.rand(2,3)\nprint (x.is_cuda)\nx = torch.rand(2,3).to(device) # Tensor is stored on the GPU\nprint (x.is_cuda)\n</code></pre>\n<pre>\nFalse\nTrue\n</pre>\n\n<p>To cite this content, please use:</p>\n<pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { PyTorch - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/recurrent-neural-networks/","title":"Recurrent Neural Networks (RNN)","text":""},{"location":"courses/foundations/recurrent-neural-networks/#overview","title":"Overview","text":"<p>So far we've processed inputs as whole (ex. applying filters across the entire input to extract features) but we can also process our inputs sequentially. For example we can think of each token in our text as an event in time (timestep). We can process each timestep, one at a time, and predict the class after the last timestep (token) has been processed. This is very powerful because the model now has a meaningful way to account for the sequential order of tokens in our sequence and predict accordingly.</p> <p>$$ \\text{RNN forward pass for a single time step } X_t $$:</p> \\[ h_t = tanh(W_{hh}h_{t-1} + W_{xh}X_t+b_h) \\] <p> Variable Description \\(N\\) batch size \\(E\\) embeddings dimension \\(H\\) # of hidden units \\(W_{hh}\\) RNN weights \\(\\in \\mathbb{R}^{HXH}\\) \\(h_{t-1}\\) previous timestep's hidden state \\(\\in in \\mathbb{R}^{NXH}\\) \\(W_{xh}\\) input weights \\(\\in \\mathbb{R}^{EXH}\\) \\(X_t\\) input at time step \\(t \\in \\mathbb{R}^{NXE}\\) \\(b_h\\) hidden units bias \\(\\in \\mathbb{R}^{HX1}\\) \\(h_t\\) output from RNN for timestep \\(t\\) <p></p> <ul> <li>Objective:<ul> <li>Process sequential data by accounting for the current input and also what has been learned from previous inputs.</li> </ul> </li> <li>Advantages:<ul> <li>Account for order and previous inputs in a meaningful way.</li> <li>Conditioned generation for generating sequences.</li> </ul> </li> <li>Disadvantages:<ul> <li>Each time step's prediction depends on the previous prediction so it's difficult to parallelize RNN operations.</li> <li>Processing long sequences can yield memory and computation issues.</li> <li>Interpretability is difficult but there are few techniques that use the activations from RNNs to see what parts of the inputs are processed.</li> </ul> </li> <li>Miscellaneous:<ul> <li>Architectural tweaks to make RNNs faster and interpretable is an ongoing area of research.</li> </ul> </li> </ul>"},{"location":"courses/foundations/recurrent-neural-networks/#set-up","title":"Set up","text":"<p>Let's set our seed and device for our main task. <pre><code>import numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code>def set_seeds(seed=1234):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # multi-GPU\n</code></pre> <pre><code># Set seeds for reproducibility\nset_seeds(seed=SEED)\n</code></pre> <pre><code># Set device\ncuda = True\ndevice = torch.device(\"cuda\" if (\n    torch.cuda.is_available() and cuda) else \"cpu\")\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\nif device.type == \"cuda\":\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\nprint (device)\n</code></pre></p> <pre>\ncuda\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#load-data","title":"Load data","text":"<p>We will download the AG News dataset, which consists of 120K text samples from 4 unique classes (<code>Business</code>, <code>Sci/Tech</code>, <code>Sports</code>, <code>World</code>) <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/news.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> title category 0 Sharon Accepts Plan to Reduce Gaza Army Operation... World 1 Internet Key Battleground in Wildlife Crime Fight Sci/Tech 2 July Durable Good Orders Rise 1.7 Percent Business 3 Growing Signs of a Slowing on Wall Street Business 4 The New Faces of Reality TV World"},{"location":"courses/foundations/recurrent-neural-networks/#preprocessing","title":"Preprocessing","text":"<p>We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc. <pre><code>import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n</code></pre> <pre><code>nltk.download(\"stopwords\")\nSTOPWORDS = stopwords.words(\"english\")\nprint (STOPWORDS[:5])\nporter = PorterStemmer()\n</code></pre></p> <pre>\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n['i', 'me', 'my', 'myself', 'we']\n</pre> <p><pre><code>def preprocess(text, stopwords=STOPWORDS):\n\"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Remove stopwords\n    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n    text = pattern.sub(\"\", text)\n\n    # Remove words in parenthesis\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n\n    # Spacing and filters\n    text = re.sub(r\"([-;;.,!?&lt;=&gt;])\", r\" \\1 \", text)\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()\n\n    return text\n</code></pre> <pre><code># Sample\ntext = \"Great week for the NYSE!\"\npreprocess(text=text)\n</code></pre></p> <pre>\ngreat week nyse\n</pre> <pre><code># Apply to dataframe\npreprocessed_df = df.copy()\npreprocessed_df.title = preprocessed_df.title.apply(preprocess)\nprint (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")\n</code></pre> <pre>\nSharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n\nsharon accepts plan reduce gaza army operation haaretz says\n</pre> <p>Warning</p> <p>If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before applying those operations. This is because we cannot apply any knowledge gained from the test set accidentally (data leak) during preprocessing/training. However for global preprocessing steps like the function above where we aren't learning anything from the data itself, we can perform before splitting the data.</p>"},{"location":"courses/foundations/recurrent-neural-networks/#split-data","title":"Split data","text":"<p><pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Data\nX = preprocessed_df[\"title\"].values\ny = preprocessed_df[\"category\"].values\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (84000,), y_train: (84000,)\nX_val: (18000,), y_val: (18000,)\nX_test: (18000,), y_test: (18000,)\nSample point: china battles north korea nuclear talks \u2192 World\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#label-encoding","title":"Label encoding","text":"<p>Next we'll define a <code>LabelEncoder</code> to encode our text labels into unique indices <pre><code>import itertools\n</code></pre> <pre><code>class LabelEncoder(object):\n\"\"\"Label encoder for tag labels.\"\"\"\n    def __init__(self, class_to_index={}):\n        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n\n    def __len__(self):\n        return len(self.class_to_index)\n\n    def __str__(self):\n        return f\"&lt;LabelEncoder(num_classes={len(self)})&gt;\"\n\n    def fit(self, y):\n        classes = np.unique(y)\n        for i, class_ in enumerate(classes):\n            self.class_to_index[class_] = i\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n        return self\n\n    def encode(self, y):\n        encoded = np.zeros((len(y)), dtype=int)\n        for i, item in enumerate(y):\n            encoded[i] = self.class_to_index[item]\n        return encoded\n\n    def decode(self, y):\n        classes = []\n        for i, item in enumerate(y):\n            classes.append(self.index_to_class[item])\n        return classes\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {'class_to_index': self.class_to_index}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Encode\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nNUM_CLASSES = len(label_encoder)\nlabel_encoder.class_to_index\n</code></pre></p> <pre>\n{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.encode(y_train)\ny_val = label_encoder.encode(y_val)\ny_test = label_encoder.encode(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\n</code></pre> <pre>\ny_train[0]: World\ny_train[0]: 3\n</pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [21000 21000 21000 21000]\nweights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#tokenizer","title":"Tokenizer","text":"<p>We'll define a <code>Tokenizer</code> to convert our text input data into token indices.</p> <p><pre><code>import json\nfrom collections import Counter\nfrom more_itertools import take\n</code></pre> <pre><code>class Tokenizer(object):\n    def __init__(self, char_level, num_tokens=None,\n                 pad_token=\"&lt;PAD&gt;\", oov_token=\"&lt;UNK&gt;\",\n                 token_to_index=None):\n        self.char_level = char_level\n        self.separator = \"\" if self.char_level else \" \"\n        if num_tokens: num_tokens -= 2 # pad + unk tokens\n        self.num_tokens = num_tokens\n        self.pad_token = pad_token\n        self.oov_token = oov_token\n        if not token_to_index:\n            token_to_index = {pad_token: 0, oov_token: 1}\n        self.token_to_index = token_to_index\n        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n\n    def __len__(self):\n        return len(self.token_to_index)\n\n    def __str__(self):\n        return f\"&lt;Tokenizer(num_tokens={len(self)})&gt;\"\n\n    def fit_on_texts(self, texts):\n        if not self.char_level:\n            texts = [text.split(\" \") for text in texts]\n        all_tokens = [token for text in texts for token in text]\n        counts = Counter(all_tokens).most_common(self.num_tokens)\n        self.min_token_freq = counts[-1][1]\n        for token, count in counts:\n            index = len(self)\n            self.token_to_index[token] = index\n            self.index_to_token[index] = token\n        return self\n\n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            if not self.char_level:\n                text = text.split(\" \")\n            sequence = []\n            for token in text:\n                sequence.append(self.token_to_index.get(\n                    token, self.token_to_index[self.oov_token]))\n            sequences.append(np.asarray(sequence))\n        return sequences\n\n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = []\n            for index in sequence:\n                text.append(self.index_to_token.get(index, self.oov_token))\n            texts.append(self.separator.join([token for token in text]))\n        return texts\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {\n                \"char_level\": self.char_level,\n                \"oov_token\": self.oov_token,\n                \"token_to_index\": self.token_to_index\n            }\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre></p> <p>Warning</p> <p>It's important that we only fit using our train data split because during inference, our model will not always know every token so it's important to replicate that scenario with our validation and test splits as well.</p> <pre><code># Tokenize\ntokenizer = Tokenizer(char_level=False, num_tokens=5000)\ntokenizer.fit_on_texts(texts=X_train)\nVOCAB_SIZE = len(tokenizer)\nprint (tokenizer)\n</code></pre> <pre>\n&lt;Tokenizer(num_tokens=5000)&gt;\n\n</pre> <pre><code># Sample of tokens\nprint (take(5, tokenizer.token_to_index.items()))\nprint (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens\n</code></pre> <pre>\n[('&lt;PAD&gt;', 0), ('&lt;UNK&gt;', 1), ('39', 2), ('b', 3), ('gt', 4)]\nleast freq token's freq: 14\n</pre> <pre><code># Convert texts to sequences of indices\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(X_test)\npreprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\nprint (\"Text to indices:\\n\"\n    f\"  (preprocessed) \u2192 {preprocessed_text}\\n\"\n    f\"  (tokenized) \u2192 {X_train[0]}\")\n</code></pre> <pre>\nText to indices:\n  (preprocessed) \u2192 china battles north korea nuclear talks\n  (tokenized) \u2192 [  16 1491  285  142  114   24]\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#padding","title":"Padding","text":"<p>We'll need to do 2D padding to our tokenized text. <pre><code>def pad_sequences(sequences, max_seq_len=0):\n\"\"\"Pad sequences to max length in sequence.\"\"\"\n    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n    padded_sequences = np.zeros((len(sequences), max_seq_len))\n    for i, sequence in enumerate(sequences):\n        padded_sequences[i][:len(sequence)] = sequence\n    return padded_sequences\n</code></pre> <pre><code># 2D sequences\npadded = pad_sequences(X_train[0:3])\nprint (padded.shape)\nprint (padded)\n</code></pre></p> <pre>\n(3, 6)\n[[1.600e+01 1.491e+03 2.850e+02 1.420e+02 1.140e+02 2.400e+01]\n [1.445e+03 2.300e+01 6.560e+02 2.197e+03 1.000e+00 0.000e+00]\n [1.200e+02 1.400e+01 1.955e+03 1.005e+03 1.529e+03 4.014e+03]]\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#datasets","title":"Datasets","text":"<p>We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits.</p> <p><pre><code>class Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __str__(self):\n        return f\"&lt;Dataset(N={len(self)})&gt;\"\n\n    def __getitem__(self, index):\n        X = self.X[index]\n        y = self.y[index]\n        return [X, len(X), y]\n\n    def collate_fn(self, batch):\n\"\"\"Processing on a batch.\"\"\"\n        # Get inputs\n        batch = np.array(batch)\n        X = batch[:, 0]\n        seq_lens = batch[:, 1]\n        y = batch[:, 2]\n\n        # Pad inputs\n        X = pad_sequences(sequences=X)\n\n        # Cast\n        X = torch.LongTensor(X.astype(np.int32))\n        seq_lens = torch.LongTensor(seq_lens.astype(np.int32))\n        y = torch.LongTensor(y.astype(np.int32))\n\n        return X, seq_lens, y\n\n    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n        return torch.utils.data.DataLoader(\n            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n            shuffle=shuffle, drop_last=drop_last, pin_memory=True)\n</code></pre> <pre><code># Create datasets\ntrain_dataset = Dataset(X=X_train, y=y_train)\nval_dataset = Dataset(X=X_val, y=y_val)\ntest_dataset = Dataset(X=X_test, y=y_test)\nprint (\"Datasets:\\n\"\n    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {train_dataset[0][0]}\\n\"\n    f\"  seq_len: {train_dataset[0][1]}\\n\"\n    f\"  y: {train_dataset[0][2]}\")\n</code></pre></p> <pre>\nDatasets:\n  Train dataset: &lt;Dataset(N=84000)&gt;\n  Val dataset: &lt;Dataset(N=18000)&gt;\n  Test dataset: &lt;Dataset(N=18000)&gt;\nSample point:\n  X: [  16 1491  285  142  114   24]\n  seq_len: 6\n  y: 3\n</pre> <pre><code># Create dataloaders\nbatch_size = 64\ntrain_dataloader = train_dataset.create_dataloader(\n    batch_size=batch_size)\nval_dataloader = val_dataset.create_dataloader(\n    batch_size=batch_size)\ntest_dataloader = test_dataset.create_dataloader(\n    batch_size=batch_size)\nbatch_X, batch_seq_lens, batch_y = next(iter(train_dataloader))\nprint (\"Sample batch:\\n\"\n    f\"  X: {list(batch_X.size())}\\n\"\n    f\"  seq_lens: {list(batch_seq_lens.size())}\\n\"\n    f\"  y: {list(batch_y.size())}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {batch_X[0]}\\n\"\n    f\" seq_len: {batch_seq_lens[0]}\\n\"\n    f\"  y: {batch_y[0]}\")\n</code></pre> <pre>\nSample batch:\n  X: [64, 14]\n  seq_lens: [64]\n  y: [64]\nSample point:\n  X: tensor([  16, 1491,  285,  142,  114,   24,    0,    0,    0,    0,    0,    0,\n           0,    0])\n seq_len: 6\n  y: 3\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#trainer","title":"Trainer","text":"<p>Let's create the <code>Trainer</code> class that we'll use to facilitate training for our experiments.</p> <pre><code>class Trainer(object):\n    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n\n        # Set params\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train_step(self, dataloader):\n\"\"\"Train step.\"\"\"\n        # Set model to train mode\n        self.model.train()\n        loss = 0.0\n\n        # Iterate over train batches\n        for i, batch in enumerate(dataloader):\n\n            # Step\n            batch = [item.to(self.device) for item in batch]  # Set device\n            inputs, targets = batch[:-1], batch[-1]\n            self.optimizer.zero_grad()  # Reset gradients\n            z = self.model(inputs)  # Forward pass\n            J = self.loss_fn(z, targets)  # Define loss\n            J.backward()  # Backward pass\n            self.optimizer.step()  # Update weights\n\n            # Cumulative Metrics\n            loss += (J.detach().item() - loss) / (i + 1)\n\n        return loss\n\n    def eval_step(self, dataloader):\n\"\"\"Validation or test step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        loss = 0.0\n        y_trues, y_probs = [], []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Step\n                batch = [item.to(self.device) for item in batch]  # Set device\n                inputs, y_true = batch[:-1], batch[-1]\n                z = self.model(inputs)  # Forward pass\n                J = self.loss_fn(z, y_true).item()\n\n                # Cumulative Metrics\n                loss += (J - loss) / (i + 1)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n                y_trues.extend(y_true.cpu().numpy())\n\n        return loss, np.vstack(y_trues), np.vstack(y_probs)\n\n    def predict_step(self, dataloader):\n\"\"\"Prediction step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        y_probs = []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Forward pass w/ inputs\n                inputs, targets = batch[:-1], batch[-1]\n                z = self.model(inputs)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n\n        return np.vstack(y_probs)\n\n    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n        best_val_loss = np.inf\n        for epoch in range(num_epochs):\n            # Steps\n            train_loss = self.train_step(dataloader=train_dataloader)\n            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n            self.scheduler.step(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = self.model\n                _patience = patience  # reset _patience\n            else:\n                _patience -= 1\n            if not _patience:  # 0\n                print(\"Stopping early!\")\n                break\n\n            # Logging\n            print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.5f}, \"\n                f\"val_loss: {val_loss:.5f}, \"\n                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n                f\"_patience: {_patience}\"\n            )\n        return best_model\n</code></pre>"},{"location":"courses/foundations/recurrent-neural-networks/#vanilla-rnn","title":"Vanilla RNN","text":""},{"location":"courses/foundations/recurrent-neural-networks/#rnn","title":"RNN","text":"<p>Inputs to RNNs are sequential like text or time-series.</p> <p><pre><code>BATCH_SIZE = 64\nEMBEDDING_DIM = 100\n</code></pre> <pre><code># Input\nsequence_size = 8 # words per input\nx = torch.rand((BATCH_SIZE, sequence_size, EMBEDDING_DIM))\nseq_lens = torch.randint(high=sequence_size, size=(BATCH_SIZE, ))\nprint (x.shape)\nprint (seq_lens.shape)\n</code></pre></p> <pre>\ntorch.Size([64, 8, 100])\ntorch.Size([1, 64])\n</pre> <p>$$ \\text{RNN forward pass for a single time step } X_t $$:</p> \\[ h_t = tanh(W_{hh}h_{t-1} + W_{xh}X_t+b_h) \\] <p> Variable Description \\(N\\) batch size \\(E\\) embeddings dimension \\(H\\) # of hidden units \\(W_{hh}\\) RNN weights \\(\\in \\mathbb{R}^{HXH}\\) \\(h_{t-1}\\) previous timestep's hidden state \\(\\in in \\mathbb{R}^{NXH}\\) \\(W_{xh}\\) input weights \\(\\in \\mathbb{R}^{EXH}\\) \\(X_t\\) input at time step \\(t \\in \\mathbb{R}^{NXE}\\) \\(b_h\\) hidden units bias \\(\\in \\mathbb{R}^{HX1}\\) \\(h_t\\) output from RNN for timestep \\(t\\) <p></p> <p>At the first time step, the previous hidden state \\(h_{t-1}\\) can either be a zero vector (unconditioned) or initialized (conditioned). If we are conditioning the RNN, the first hidden state \\(h_0\\) can belong to a specific condition or we can concat the specific condition to the randomly initialized hidden vectors at each time step. More on this in the subsequent notebooks on RNNs.</p> <p><pre><code>RNN_HIDDEN_DIM = 128\nDROPOUT_P = 0.1\n</code></pre> <pre><code># Initialize hidden state\nhidden_t = torch.zeros((BATCH_SIZE, RNN_HIDDEN_DIM))\nprint (hidden_t.size())\n</code></pre></p> <pre>\ntorch.Size([64, 128])\n</pre> <p>We'll show how to create an RNN cell using PyTorch's <code>RNNCell</code> and the more abstracted <code>RNN</code>.</p> <pre><code># Initialize RNN cell\nrnn_cell = nn.RNNCell(EMBEDDING_DIM, RNN_HIDDEN_DIM)\nprint (rnn_cell)\n</code></pre> <pre>\nRNNCell(100, 128)\n</pre> <pre><code># Forward pass through RNN\nx = x.permute(1, 0, 2) # RNN needs batch_size to be at dim 1\n\n# Loop through the inputs time steps\nhiddens = []\nfor t in range(sequence_size):\n    hidden_t = rnn_cell(x[t], hidden_t)\n    hiddens.append(hidden_t)\nhiddens = torch.stack(hiddens)\nhiddens = hiddens.permute(1, 0, 2) # bring batch_size back to dim 0\nprint (hiddens.size())\n</code></pre> <pre>\ntorch.Size([64, 8, 128])\n</pre> <pre><code># We also could've used a more abstracted layer\nx = torch.rand((BATCH_SIZE, sequence_size, EMBEDDING_DIM))\nrnn = nn.RNN(EMBEDDING_DIM, RNN_HIDDEN_DIM, batch_first=True)\nout, h_n = rnn(x) # h_n is the last hidden state\nprint (\"out: \", out.shape)\nprint (\"h_n: \", h_n.shape)\n</code></pre> <pre>\nout:  torch.Size([64, 8, 128])\nh_n:  torch.Size([1, 64, 128])\n</pre> <pre><code># The same tensors\nprint (out[:,-1,:])\nprint (h_n.squeeze(0))\n</code></pre> <pre>\ntensor([[-0.0359, -0.3819,  0.2162,  ..., -0.3397,  0.0468,  0.1937],\n        [-0.4914, -0.3056, -0.0837,  ..., -0.3507, -0.4320,  0.3593],\n        [-0.0989, -0.2852,  0.1170,  ..., -0.0805, -0.0786,  0.3922],\n        ...,\n        [-0.3115, -0.4169,  0.2611,  ..., -0.3214,  0.0620,  0.0338],\n        [-0.2455, -0.3380,  0.2048,  ..., -0.4198, -0.0075,  0.0372],\n        [-0.2092, -0.4594,  0.1654,  ..., -0.5397, -0.1709,  0.0023]],\n       grad_fn=&lt;SliceBackward&gt;)\ntensor([[-0.0359, -0.3819,  0.2162,  ..., -0.3397,  0.0468,  0.1937],\n        [-0.4914, -0.3056, -0.0837,  ..., -0.3507, -0.4320,  0.3593],\n        [-0.0989, -0.2852,  0.1170,  ..., -0.0805, -0.0786,  0.3922],\n        ...,\n        [-0.3115, -0.4169,  0.2611,  ..., -0.3214,  0.0620,  0.0338],\n        [-0.2455, -0.3380,  0.2048,  ..., -0.4198, -0.0075,  0.0372],\n        [-0.2092, -0.4594,  0.1654,  ..., -0.5397, -0.1709,  0.0023]],\n       grad_fn=&lt;SqueezeBackward1&gt;)\n</pre> <p>In our model, we want to use the RNN's output after the last relevant token in the sentence is processed. The last relevant token doesn't refer the <code>&lt;PAD&gt;</code> tokens but to the last actual word in the sentence and its index is different for each input in the batch. This is why we included a <code>seq_lens</code> tensor in our batches.</p> <p><pre><code>def gather_last_relevant_hidden(hiddens, seq_lens):\n\"\"\"Extract and collect the last relevant\n    hidden state based on the sequence length.\"\"\"\n    seq_lens = seq_lens.long().detach().cpu().numpy() - 1\n    out = []\n    for batch_index, column_index in enumerate(seq_lens):\n        out.append(hiddens[batch_index, column_index])\n    return torch.stack(out)\n</code></pre> <pre><code># Get the last relevant hidden state\ngather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens).squeeze(0).shape\n</code></pre></p> <pre>\ntorch.Size([64, 128])\n</pre> <p>There are many different ways to use RNNs. So far we've processed our inputs one timestep at a time and we could either use the RNN's output at each time step or just use the final input timestep's RNN output. Let's look at a few other possibilities.</p>"},{"location":"courses/foundations/recurrent-neural-networks/#model","title":"Model","text":"<p><pre><code>import torch.nn.functional as F\n</code></pre> <pre><code>HIDDEN_DIM = 100\n</code></pre> <pre><code>class RNN(nn.Module):\n    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n        super(RNN, self).__init__()\n\n        # Initialize embeddings\n        self.embeddings = nn.Embedding(\n            embedding_dim=embedding_dim, num_embeddings=vocab_size,\n            padding_idx=padding_idx)\n\n        # RNN\n        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n\n        # FC weights\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs):\n        # Embed\n        x_in, seq_lens = inputs\n        x_in = self.embeddings(x_in)\n\n        # Rnn outputs\n        out, h_n = self.rnn(x_in)\n        z = gather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens)\n\n        # FC layers\n        z = self.fc1(z)\n        z = self.dropout(z)\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Simple RNN cell\nmodel = RNN(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of RNN(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (rnn): RNN(100, 128, batch_first=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=128, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#training","title":"Training","text":"<p><pre><code>from torch.optim import Adam\n</code></pre> <pre><code>NUM_LAYERS = 1\nLEARNING_RATE = 1e-4\nPATIENCE = 10\nNUM_EPOCHS = 50\n</code></pre> <pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p> <pre>\nEpoch: 1 | train_loss: 1.25605, val_loss: 1.10880, lr: 1.00E-04, _patience: 10\nEpoch: 2 | train_loss: 1.03074, val_loss: 0.96749, lr: 1.00E-04, _patience: 10\nEpoch: 3 | train_loss: 0.90110, val_loss: 0.86424, lr: 1.00E-04, _patience: 10\n...\nEpoch: 31 | train_loss: 0.32206, val_loss: 0.53581, lr: 1.00E-06, _patience: 3\nEpoch: 32 | train_loss: 0.32233, val_loss: 0.53587, lr: 1.00E-07, _patience: 2\nEpoch: 33 | train_loss: 0.32215, val_loss: 0.53572, lr: 1.00E-07, _patience: 1\nStopping early!\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#evaluation","title":"Evaluation","text":"<p><pre><code>import json\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre> <pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.8171357577653572,\n  \"recall\": 0.8176111111111112,\n  \"f1\": 0.8171696173843819,\n  \"num_samples\": 18000.0\n}\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#gated-rnn","title":"Gated RNN","text":"<p>While our simple RNNs so far are great for sequentially processing our inputs, they have quite a few disadvantages. They commonly suffer from exploding or vanishing gradients as a result using the same set of weights (\\(W_{xh}\\) and \\(W_{hh}\\)) with each timestep's input. During backpropagation, this can cause gradients to explode (&gt;1) or vanish (&lt;1). If you multiply any number greater than 1 with itself over and over, it moves towards infinity (exploding gradients) and similarly,  If you multiply any number less than 1 with itself over and over, it moves towards zero (vanishing gradients). To mitigate this issue, gated RNNs were devised to selectively retain information. If you're interested in learning more of the specifics, this post is a must-read.</p> <p>There are two popular types of gated RNNs: Long Short-term Memory (LSTMs) units and Gated Recurrent Units (GRUs).</p> <p>When deciding between LSTMs and GRUs, empirical performance is the best factor but in general GRUs offer similar performance with less complexity (less weights).</p> Understanding LSTM Networks - Chris Olah <pre><code># Input\nsequence_size = 8 # words per input\nx = torch.rand((BATCH_SIZE, sequence_size, EMBEDDING_DIM))\nprint (x.shape)\n</code></pre> <pre>\ntorch.Size([64, 8, 100])\n</pre> <p><pre><code># GRU\ngru = nn.GRU(input_size=EMBEDDING_DIM, hidden_size=RNN_HIDDEN_DIM, batch_first=True)\n</code></pre> <pre><code># Forward pass\nout, h_n = gru(x)\nprint (f\"out: {out.shape}\")\nprint (f\"h_n: {h_n.shape}\")\n</code></pre></p> <pre>\nout: torch.Size([64, 8, 128])\nh_n: torch.Size([1, 64, 128])\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#bidirectional-rnn","title":"Bidirectional RNN","text":"<p>We can also have RNNs that process inputs from both directions (first token to last token and vice versa) and combine their outputs. This architecture is known as a bidirectional RNN. <pre><code># GRU\ngru = nn.GRU(input_size=EMBEDDING_DIM, hidden_size=RNN_HIDDEN_DIM,\n             batch_first=True, bidirectional=True)\n</code></pre> <pre><code># Forward pass\nout, h_n = gru(x)\nprint (f\"out: {out.shape}\")\nprint (f\"h_n: {h_n.shape}\")\n</code></pre></p> <pre>\nout: torch.Size([64, 8, 256])\nh_n: torch.Size([2, 64, 128])\n</pre> <p>Notice that the output for each sample at each timestamp has size 256 (double the <code>RNN_HIDDEN_DIM</code>). This is because this includes both the forward and backward directions from the BiRNN.</p>"},{"location":"courses/foundations/recurrent-neural-networks/#model_gated","title":"Model","text":"<p><pre><code>class GRU(nn.Module):\n    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n        super(GRU, self).__init__()\n\n        # Initialize embeddings\n        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n                                       num_embeddings=vocab_size,\n                                       padding_idx=padding_idx)\n\n        # RNN\n        self.rnn = nn.GRU(embedding_dim, rnn_hidden_dim,\n                          batch_first=True, bidirectional=True)\n\n        # FC weights\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc1 = nn.Linear(rnn_hidden_dim*2, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs:\n        # Embed\n        x_in, seq_lens = inputs\n        x_in = self.embeddings(x_in)\n\n        # Rnn outputs\n        out, h_n = self.rnn(x_in)\n        z = gather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens)\n\n        # FC layers\n        z = self.fc1(z)\n        z = self.dropout(z)\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Simple RNN cell\nmodel = GRU(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of GRU(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (rnn): GRU(100, 128, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=256, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#training_1","title":"Training","text":"<p><pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p> <pre>\nEpoch: 1 | train_loss: 1.18125, val_loss: 0.93827, lr: 1.00E-04, _patience: 10\nEpoch: 2 | train_loss: 0.81291, val_loss: 0.72564, lr: 1.00E-04, _patience: 10\nEpoch: 3 | train_loss: 0.65413, val_loss: 0.64487, lr: 1.00E-04, _patience: 10\n...\nEpoch: 23 | train_loss: 0.30351, val_loss: 0.53904, lr: 1.00E-06, _patience: 3\nEpoch: 24 | train_loss: 0.30332, val_loss: 0.53912, lr: 1.00E-07, _patience: 2\nEpoch: 25 | train_loss: 0.30300, val_loss: 0.53909, lr: 1.00E-07, _patience: 1\nStopping early!\n</pre>"},{"location":"courses/foundations/recurrent-neural-networks/#evaluation_1","title":"Evaluation","text":"<p><pre><code>from pathlib import Path\n</code></pre> <pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.8192635071011053,\n  \"recall\": 0.8196111111111111,\n  \"f1\": 0.8192710197821547,\n  \"num_samples\": 18000.0\n}\n</pre> <pre><code># Save artifacts\ndir = Path(\"gru\")\ndir.mkdir(parents=True, exist_ok=True)\nlabel_encoder.save(fp=Path(dir, \"label_encoder.json\"))\ntokenizer.save(fp=Path(dir, 'tokenizer.json'))\ntorch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\nwith open(Path(dir, 'performance.json'), \"w\") as fp:\n    json.dump(performance, indent=2, sort_keys=False, fp=fp)\n</code></pre>"},{"location":"courses/foundations/recurrent-neural-networks/#inference","title":"Inference","text":"<p><pre><code>def get_probability_distribution(y_prob, classes):\n\"\"\"Create a dict of class probabilities from an array.\"\"\"\n    results = {}\n    for i, class_ in enumerate(classes):\n        results[class_] = np.float64(y_prob[i])\n    sorted_results = {k: v for k, v in sorted(\n        results.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_results\n</code></pre> <pre><code># Load artifacts\ndevice = torch.device(\"cpu\")\nlabel_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\ntokenizer = Tokenizer.load(fp=Path(dir, 'tokenizer.json'))\nmodel = GRU(\n    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE,\n    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\nmodel.to(device)\n</code></pre></p> <pre>\nGRU(\n  (embeddings): Embedding(5000, 100, padding_idx=0)\n  (rnn): GRU(100, 128, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc1): Linear(in_features=256, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=4, bias=True)\n)\n</pre> <p><pre><code># Initialize trainer\ntrainer = Trainer(model=model, device=device)\n</code></pre> <pre><code># Dataloader\ntext = \"The final tennis tournament starts next week.\"\nX = tokenizer.texts_to_sequences([preprocess(text)])\nprint (tokenizer.sequences_to_texts(X))\ny_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\ndataset = Dataset(X=X, y=y_filler)\ndataloader = dataset.create_dataloader(batch_size=batch_size)\n</code></pre></p> <pre>\n['final tennis tournament starts next week']\n</pre> <pre><code># Inference\ny_prob = trainer.predict_step(dataloader)\ny_pred = np.argmax(y_prob, axis=1)\nlabel_encoder.decode(y_pred)\n</code></pre> <pre>\n['Sports']\n</pre> <pre><code># Class distributions\nprob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\nprint (json.dumps(prob_dist, indent=2))\n</code></pre> <pre>\n{\n  \"Sports\": 0.49753469228744507,\n  \"World\": 0.2925860285758972,\n  \"Business\": 0.1932886838912964,\n  \"Sci/Tech\": 0.01659061387181282\n}\n</pre> <p>We will learn how to create more context-aware representations and a little bit of interpretability with RNNs in the next lesson on attention.</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { RNNs - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/transformers/","title":"Transformers","text":""},{"location":"courses/foundations/transformers/#overview","title":"Overview","text":"<p>Transformers are a very popular architecture that leverage and extend the concept of self-attention to create very useful representations of our input data for a downstream task.</p> <ul> <li> <p>advantages:</p> <ul> <li>better representation for our input tokens via contextual embeddings where the token representation is based on the specific neighboring tokens using self-attention.</li> <li>sub-word tokens, as opposed to character tokens, since they can hold more meaningful representation for many of our keywords, prefixes, suffixes, etc.</li> <li>attend (in parallel) to all the tokens in our input, as opposed to being limited by filter spans (CNNs) or memory issues from sequential processing (RNNs).</li> </ul> </li> <li> <p>disadvantages:</p> <ul> <li>computationally intensive</li> <li>required large amounts of data (mitigated using pretrained models)</li> </ul> </li> </ul> Attention Is All You Need"},{"location":"courses/foundations/transformers/#set-up","title":"Set up","text":"<p>Let's set our seed and device for our main task. <pre><code>import numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code>def set_seeds(seed=1234):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # multi-GPU\n</code></pre> <pre><code># Set seeds for reproducibility\nset_seeds(seed=SEED)\n</code></pre> <pre><code># Set device\ncuda = True\ndevice = torch.device(\"cuda\" if (\n    torch.cuda.is_available() and cuda) else \"cpu\")\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\nif device.type == \"cuda\":\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\nprint (device)\n</code></pre></p> <pre>\ncuda\n</pre>"},{"location":"courses/foundations/transformers/#load-data","title":"Load data","text":"<p>We will download the AG News dataset, which consists of 120K text samples from 4 unique classes (<code>Business</code>, <code>Sci/Tech</code>, <code>Sports</code>, <code>World</code>) <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/news.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> title category 0 Sharon Accepts Plan to Reduce Gaza Army Operation... World 1 Internet Key Battleground in Wildlife Crime Fight Sci/Tech 2 July Durable Good Orders Rise 1.7 Percent Business 3 Growing Signs of a Slowing on Wall Street Business 4 The New Faces of Reality TV World <pre><code># Reduce data size (too large to fit in Colab's limited memory)\ndf = df[:10000]\nprint (len(df))\n</code></pre> <pre>\n10000\n</pre>"},{"location":"courses/foundations/transformers/#preprocessing","title":"Preprocessing","text":"<p>We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc. <pre><code>import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n</code></pre> <pre><code>nltk.download(\"stopwords\")\nSTOPWORDS = stopwords.words(\"english\")\nprint (STOPWORDS[:5])\nporter = PorterStemmer()\n</code></pre></p> <pre>\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n['i', 'me', 'my', 'myself', 'we']\n</pre> <p><pre><code>def preprocess(text, stopwords=STOPWORDS):\n\"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Remove stopwords\n    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n    text = pattern.sub(\"\", text)\n\n    # Remove words in parenthesis\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n\n    # Spacing and filters\n    text = re.sub(r\"([-;;.,!?&lt;=&gt;])\", r\" \\1 \", text)\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()\n\n    return text\n</code></pre> <pre><code># Sample\ntext = \"Great week for the NYSE!\"\npreprocess(text=text)\n</code></pre></p> <pre>\ngreat week nyse\n</pre> <pre><code># Apply to dataframe\npreprocessed_df = df.copy()\npreprocessed_df.title = preprocessed_df.title.apply(preprocess)\nprint (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")\n</code></pre> <pre>\nSharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n\nsharon accepts plan reduce gaza army operation haaretz says\n</pre> <p>Warning</p> <p>If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before applying those operations. This is because we cannot apply any knowledge gained from the test set accidentally (data leak) during preprocessing/training. However for global preprocessing steps like the function above where we aren't learning anything from the data itself, we can perform before splitting the data.</p>"},{"location":"courses/foundations/transformers/#split-data","title":"Split data","text":"<p><pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Data\nX = preprocessed_df[\"title\"].values\ny = preprocessed_df[\"category\"].values\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (7000,), y_train: (7000,)\nX_val: (1500,), y_val: (1500,)\nX_test: (1500,), y_test: (1500,)\nSample point: lost flu paydays \u2192 Business\n</pre>"},{"location":"courses/foundations/transformers/#label-encoding","title":"Label encoding","text":"<p>Next we'll define a <code>LabelEncoder</code> to encode our text labels into unique indices <pre><code>import itertools\n</code></pre> <pre><code>class LabelEncoder(object):\n\"\"\"Label encoder for tag labels.\"\"\"\n    def __init__(self, class_to_index={}):\n        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n\n    def __len__(self):\n        return len(self.class_to_index)\n\n    def __str__(self):\n        return f\"&lt;LabelEncoder(num_classes={len(self)})&gt;\"\n\n    def fit(self, y):\n        classes = np.unique(y)\n        for i, class_ in enumerate(classes):\n            self.class_to_index[class_] = i\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n        return self\n\n    def encode(self, y):\n        y_one_hot = np.zeros((len(y), len(self.class_to_index)), dtype=int)\n        for i, item in enumerate(y):\n            y_one_hot[i][self.class_to_index[item]] = 1\n        return y_one_hot\n\n    def decode(self, y):\n        classes = []\n        for i, item in enumerate(y):\n            index = np.where(item == 1)[0][0]\n            classes.append(self.index_to_class[index])\n        return classes\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {'class_to_index': self.class_to_index}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Encode\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nNUM_CLASSES = len(label_encoder)\nlabel_encoder.class_to_index\n</code></pre></p> <pre>\n{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n</pre> <pre><code># Class weights\ncounts = np.bincount([label_encoder.class_to_index[class_] for class_ in y_train])\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [1746 1723 1725 1806]\nweights: {0: 0.000572737686139748, 1: 0.0005803830528148578, 2: 0.0005797101449275362, 3: 0.0005537098560354374}\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.encode(y_train)\ny_val = label_encoder.encode(y_val)\ny_test = label_encoder.encode(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\nprint (f\"decode([y_train[0]]): {label_encoder.decode([y_train[0]])}\")\n</code></pre> <pre>\ny_train[0]: Business\ny_train[0]: [1 0 0 0]\ndecode([y_train[0]]): ['Business']\n</pre>"},{"location":"courses/foundations/transformers/#tokenizer","title":"Tokenizer","text":"<p>We'll be using the BertTokenizer to tokenize our input text in to sub-word tokens.</p> <pre><code>from transformers import DistilBertTokenizer\nfrom transformers import BertTokenizer\n</code></pre> <pre><code># Load tokenizer and model\n# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nvocab_size = len(tokenizer)\nprint (vocab_size)\n</code></pre> <pre>\n31090\n</pre> <pre><code># Tokenize inputs\nencoded_input = tokenizer(X_train.tolist(), return_tensors=\"pt\", padding=True)\nX_train_ids = encoded_input[\"input_ids\"]\nX_train_masks = encoded_input[\"attention_mask\"]\nprint (X_train_ids.shape, X_train_masks.shape)\nencoded_input = tokenizer(X_val.tolist(), return_tensors=\"pt\", padding=True)\nX_val_ids = encoded_input[\"input_ids\"]\nX_val_masks = encoded_input[\"attention_mask\"]\nprint (X_val_ids.shape, X_val_masks.shape)\nencoded_input = tokenizer(X_test.tolist(), return_tensors=\"pt\", padding=True)\nX_test_ids = encoded_input[\"input_ids\"]\nX_test_masks = encoded_input[\"attention_mask\"]\nprint (X_test_ids.shape, X_test_masks.shape)\n</code></pre> <pre>\ntorch.Size([7000, 27]) torch.Size([7000, 27])\ntorch.Size([1500, 21]) torch.Size([1500, 21])\ntorch.Size([1500, 26]) torch.Size([1500, 26])\n</pre> <pre><code># Decode\nprint (f\"{X_train_ids[0]}\\n{tokenizer.decode(X_train_ids[0])}\")\n</code></pre> <pre>\ntensor([  102,  6677,  1441,  3982, 17973,   103,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0])\n[CLS] lost flu paydays [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n</pre> <pre><code># Sub-word tokens\nprint (tokenizer.convert_ids_to_tokens(ids=X_train_ids[0]))\n</code></pre> <pre>\n['[CLS]', 'lost', 'flu', 'pay', '##days', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n</pre>"},{"location":"courses/foundations/transformers/#datasets","title":"Datasets","text":"<p>We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits.</p> <p><pre><code>class TransformerTextDataset(torch.utils.data.Dataset):\n    def __init__(self, ids, masks, targets):\n        self.ids = ids\n        self.masks = masks\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __str__(self):\n        return f\"&lt;Dataset(N={len(self)})&gt;\"\n\n    def __getitem__(self, index):\n        ids = torch.tensor(self.ids[index], dtype=torch.long)\n        masks = torch.tensor(self.masks[index], dtype=torch.long)\n        targets = torch.FloatTensor(self.targets[index])\n        return ids, masks, targets\n\n    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n        return torch.utils.data.DataLoader(\n            dataset=self,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=drop_last,\n            pin_memory=False)\n</code></pre> <pre><code># Create datasets\ntrain_dataset = TransformerTextDataset(ids=X_train_ids, masks=X_train_masks, targets=y_train)\nval_dataset = TransformerTextDataset(ids=X_val_ids, masks=X_val_masks, targets=y_val)\ntest_dataset = TransformerTextDataset(ids=X_test_ids, masks=X_test_masks, targets=y_test)\nprint (\"Data splits:\\n\"\n    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n    \"Sample point:\\n\"\n    f\"  ids: {train_dataset[0][0]}\\n\"\n    f\"  masks: {train_dataset[0][1]}\\n\"\n    f\"  targets: {train_dataset[0][2]}\")\n</code></pre></p> <pre>\nData splits:\n  Train dataset: &lt;Dataset(N=7000)&gt;\n  Val dataset: &lt;Dataset(N=1500)&gt;\n  Test dataset: &lt;Dataset(N=1500)&gt;\nSample point:\n  ids: tensor([  102,  6677,  1441,  3982, 17973,   103,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0])\n  masks: tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0])\n  targets: tensor([1., 0., 0., 0.], device=\"cpu\")\n</pre> <pre><code># Create dataloaders\nbatch_size = 128\ntrain_dataloader = train_dataset.create_dataloader(\n    batch_size=batch_size)\nval_dataloader = val_dataset.create_dataloader(\n    batch_size=batch_size)\ntest_dataloader = test_dataset.create_dataloader(\n    batch_size=batch_size)\nbatch = next(iter(train_dataloader))\nprint (\"Sample batch:\\n\"\n    f\"  ids: {batch[0].size()}\\n\"\n    f\"  masks: {batch[1].size()}\\n\"\n    f\"  targets: {batch[2].size()}\")\n</code></pre> <pre>\nSample batch:\n  ids: torch.Size([128, 27])\n  masks: torch.Size([128, 27])\n  targets: torch.Size([128, 4])\n</pre>"},{"location":"courses/foundations/transformers/#trainer","title":"Trainer","text":"<p>Let's create the <code>Trainer</code> class that we'll use to facilitate training for our experiments.</p> <pre><code>import torch.nn.functional as F\n</code></pre> <pre><code>class Trainer(object):\n    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n\n        # Set params\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train_step(self, dataloader):\n\"\"\"Train step.\"\"\"\n        # Set model to train mode\n        self.model.train()\n        loss = 0.0\n\n        # Iterate over train batches\n        for i, batch in enumerate(dataloader):\n\n            # Step\n            batch = [item.to(self.device) for item in batch]  # Set device\n            inputs, targets = batch[:-1], batch[-1]\n            self.optimizer.zero_grad()  # Reset gradients\n            z = self.model(inputs)  # Forward pass\n            J = self.loss_fn(z, targets)  # Define loss\n            J.backward()  # Backward pass\n            self.optimizer.step()  # Update weights\n\n            # Cumulative Metrics\n            loss += (J.detach().item() - loss) / (i + 1)\n\n        return loss\n\n    def eval_step(self, dataloader):\n\"\"\"Validation or test step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        loss = 0.0\n        y_trues, y_probs = [], []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Step\n                batch = [item.to(self.device) for item in batch]  # Set device\n                inputs, y_true = batch[:-1], batch[-1]\n                z = self.model(inputs)  # Forward pass\n                J = self.loss_fn(z, y_true).item()\n\n                # Cumulative Metrics\n                loss += (J - loss) / (i + 1)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n                y_trues.extend(y_true.cpu().numpy())\n\n        return loss, np.vstack(y_trues), np.vstack(y_probs)\n\n    def predict_step(self, dataloader):\n\"\"\"Prediction step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        y_probs = []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Forward pass w/ inputs\n                inputs, targets = batch[:-1], batch[-1]\n                z = self.model(inputs)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n\n        return np.vstack(y_probs)\n\n    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n        best_val_loss = np.inf\n        for epoch in range(num_epochs):\n            # Steps\n            train_loss = self.train_step(dataloader=train_dataloader)\n            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n            self.scheduler.step(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = self.model\n                _patience = patience  # reset _patience\n            else:\n                _patience -= 1\n            if not _patience:  # 0\n                print(\"Stopping early!\")\n                break\n\n            # Logging\n            print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.5f}, \"\n                f\"val_loss: {val_loss:.5f}, \"\n                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n                f\"_patience: {_patience}\"\n            )\n        return best_model\n</code></pre>"},{"location":"courses/foundations/transformers/#transformer","title":"Transformer","text":"<p>We'll first learn about the unique components within the Transformer architecture and then implement one for our text classification task.</p>"},{"location":"courses/foundations/transformers/#scaled-dot-product-attention","title":"Scaled dot-product attention","text":"<p>The most popular type of self-attention is scaled dot-product attention from the widely-cited Attention is all you need paper. This type of attention involves projecting our encoded input sequences onto three matrices, queries (Q), keys (K) and values (V), whose weights we learn.</p> \\[ Q = XW_q \\text{ where } W_q \\in \\mathbb{R}^{HXd_q} \\] \\[ K = XW_k \\text{ where } W_k \\in \\mathbb{R}^{HXd_k} \\] \\[ V = XW_v \\text{ where } W_v \\in \\mathbb{R}^{HXd_v} \\] \\[ attention (Q, K, V) = softmax( \\frac{Q K^{T}}{\\sqrt{d_k}} ) V \\in \\mathbb{R}^{MXd_v} \\] <p> Variable Description \\(X\\) encoded inputs \\(\\in \\mathbb{R}^{NXMXH}\\) \\(N\\) batch size \\(M\\) max sequence length in the batch \\(H\\) hidden dim, model dim, etc. \\(W_q\\) query weights \\(\\in \\mathbb{R}^{HXd_q}\\) \\(W_k\\) key weights \\(\\in \\mathbb{R}^{HXd_k}\\) \\(W_v\\) value weights \\(\\in \\mathbb{R}^{HXd_v}\\) <p></p>"},{"location":"courses/foundations/transformers/#multi-head-attention","title":"Multi-head attention","text":"<p>Instead of applying self-attention only once across the entire encoded input, we can also separate the input and apply self-attention in parallel (heads) to each input section and concatenate them. This allows the different head to learn unique representations while maintaining the complexity since we split the input into smaller subspaces.</p> \\[ MultiHead(Q, K, V) = concat({head}_1, ..., {head}_{h})W_O \\] \\[ {head}_i = attention(Q_i, K_i, V_i) \\] <p> Variable Description \\(h\\) number of attention heads \\(W_O\\) multi-head attention weights \\(\\in \\mathbb{R}^{hd_vXH}\\) \\(H\\) hidden dim (or dimension of the model \\(d_{model}\\)) <p></p>"},{"location":"courses/foundations/transformers/#positional-encoding","title":"Positional encoding","text":"<p>With self-attention, we aren't able to account for the sequential position of our input tokens. To address this, we can use positional encoding to create a representation of the location of each token with respect to the entire sequence. This can either be learned (with weights) or we can use a fixed function that can better extend to create positional encoding for lengths during inference that were not observed during training.</p> \\[ PE_{(pos,2i)} = sin({pos}/{10000^{2i/H}}) \\] \\[ PE_{(pos,2i+1)} = cos({pos}/{10000^{2i/H}}) \\] <p> Variable Description \\(pos\\) position of the token \\((1...M)\\) \\(i\\) hidden dim \\((1..H)\\) <p></p> <p>This effectively allows us to represent each token's relative position using a fixed function for very large sequences. And because we've constrained the positional encodings to have the same dimensions as our encoded inputs, we can simply concatenate them before feeding them into the multi-head attention heads.</p>"},{"location":"courses/foundations/transformers/#architecture","title":"Architecture","text":"<p>And here's how it all fits together! It's an end-to-end architecture that creates these contextual representations and uses an encoder-decoder architecture to predict the outcomes (one-to-one, many-to-one, many-to-many, etc.) Due to the complexity of the architecture, they require massive amounts of data for training without overfitting, however, they can be leveraged as pretrained models to finetune with smaller datasets that are similar to the larger set it was initially trained on.</p> Attention Is All You Need <p>We're not going to the implement the Transformer from scratch but we will use the Hugging Face library to do so in the training lesson!</p>"},{"location":"courses/foundations/transformers/#model","title":"Model","text":"<p>We're going to use a pretrained BertModel to act as a feature extractor. We'll only use the encoder to receive sequential and pooled outputs (<code>is_decoder=False</code> is default).</p> <pre><code>from transformers import BertModel\n</code></pre> <pre><code># transformer = BertModel.from_pretrained(\"distilbert-base-uncased\")\n# embedding_dim = transformer.config.dim\ntransformer = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nembedding_dim = transformer.config.hidden_size\n</code></pre> <pre><code>class Transformer(nn.Module):\n    def __init__(self, transformer, dropout_p, embedding_dim, num_classes):\n        super(Transformer, self).__init__()\n        self.transformer = transformer\n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n\n    def forward(self, inputs):\n        ids, masks = inputs\n        seq, pool = self.transformer(input_ids=ids, attention_mask=masks)\n        z = self.dropout(pool)\n        z = self.fc1(z)\n        return z\n</code></pre> <p>We decided to work with the pooled output, but we could have just as easily worked with the sequential output (encoder representation for each sub-token) and applied a CNN (or other decoder options) on top of it.</p> <pre><code># Initialize model\ndropout_p = 0.5\nmodel = Transformer(\n    transformer=transformer, dropout_p=dropout_p,\n    embedding_dim=embedding_dim, num_classes=num_classes)\nmodel = model.to(device)\nprint (model.named_parameters)\n</code></pre> <pre>\n&lt;bound method Module.named_parameters of Transformer(\n  (transformer): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        ...\n        11 more BertLayers\n        ...\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=768, out_features=4, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/transformers/#training","title":"Training","text":"<pre><code># Arguments\nlr = 1e-4\nnum_epochs = 10\npatience = 10\n</code></pre> <pre><code># Define loss\nclass_weights_tensor = torch.Tensor(np.array(list(class_weights.values())))\nloss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=5)\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(num_epochs, patience, train_dataloader, val_dataloader)\n</code></pre> <pre>\nEpoch: 1 | train_loss: 0.00022, val_loss: 0.00017, lr: 1.00E-04, _patience: 10\nEpoch: 2 | train_loss: 0.00014, val_loss: 0.00016, lr: 1.00E-04, _patience: 10\nEpoch: 3 | train_loss: 0.00010, val_loss: 0.00017, lr: 1.00E-04, _patience: 9\n...\nEpoch: 9 | train_loss: 0.00002, val_loss: 0.00022, lr: 1.00E-05, _patience: 3\nEpoch: 10 | train_loss: 0.00002, val_loss: 0.00022, lr: 1.00E-05, _patience: 2\nEpoch: 11 | train_loss: 0.00001, val_loss: 0.00022, lr: 1.00E-05, _patience: 1\nStopping early!\n</pre>"},{"location":"courses/foundations/transformers/#evaluation","title":"Evaluation","text":"<pre><code>import json\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code>def get_performance(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre> <pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_performance(\n    y_true=np.argmax(y_true, axis=1), y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre> <pre>\n{\n  \"precision\": 0.8085194951783808,\n  \"recall\": 0.8086666666666666,\n  \"f1\": 0.8083051845125695,\n  \"num_samples\": 1500.0\n}\n</pre> <pre><code># Save artifacts\nfrom pathlib import Path\ndir = Path(\"transformers\")\ndir.mkdir(parents=True, exist_ok=True)\nlabel_encoder.save(fp=Path(dir, \"label_encoder.json\"))\ntorch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\nwith open(Path(dir, \"performance.json\"), \"w\") as fp:\n    json.dump(performance, indent=2, sort_keys=False, fp=fp)\n</code></pre>"},{"location":"courses/foundations/transformers/#inference","title":"Inference","text":"<pre><code>def get_probability_distribution(y_prob, classes):\n\"\"\"Create a dict of class probabilities from an array.\"\"\"\n    results = {}\n    for i, class_ in enumerate(classes):\n        results[class_] = np.float64(y_prob[i])\n    sorted_results = {k: v for k, v in sorted(\n        results.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_results\n</code></pre> <pre><code># Load artifacts\ndevice = torch.device(\"cpu\")\ntokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nlabel_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\ntransformer = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nembedding_dim = transformer.config.hidden_size\nmodel = Transformer(\n    transformer=transformer, dropout_p=dropout_p,\n    embedding_dim=embedding_dim, num_classes=num_classes)\nmodel.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\nmodel.to(device);\n</code></pre> <pre><code># Initialize trainer\ntrainer = Trainer(model=model, device=device)\n</code></pre> <pre><code># Create datasets\ntrain_dataset = TransformerTextDataset(ids=X_train_ids, masks=X_train_masks, targets=y_train)\nval_dataset = TransformerTextDataset(ids=X_val_ids, masks=X_val_masks, targets=y_val)\ntest_dataset = TransformerTextDataset(ids=X_test_ids, masks=X_test_masks, targets=y_test)\nprint (\"Data splits:\\n\"\n    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n    \"Sample point:\\n\"\n    f\"  ids: {train_dataset[0][0]}\\n\"\n    f\"  masks: {train_dataset[0][1]}\\n\"\n    f\"  targets: {train_dataset[0][2]}\")\n</code></pre> <pre>\nData splits:\n  Train dataset: &lt;Dataset(N=7000)&gt;\n  Val dataset: &lt;Dataset(N=1500)&gt;\n  Test dataset: &lt;Dataset(N=1500)&gt;\nSample point:\n  ids: tensor([  102,  6677,  1441,  3982, 17973,   103,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0])\n  masks: tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0])\n  targets: tensor([1., 0., 0., 0.], device=\"cpu\")\n</pre> <pre><code># Dataloader\ntext = \"The final tennis tournament starts next week.\"\nX = preprocess(text)\nencoded_input = tokenizer(X, return_tensors=\"pt\", padding=True).to(torch.device(\"cpu\"))\nids = encoded_input[\"input_ids\"]\nmasks = encoded_input[\"attention_mask\"]\ny_filler = label_encoder.encode([label_encoder.classes[0]]*len(ids))\ndataset = TransformerTextDataset(ids=ids, masks=masks, targets=y_filler)\ndataloader = dataset.create_dataloader(batch_size=int(batch_size))\n</code></pre> <pre><code># Inference\ny_prob = trainer.predict_step(dataloader)\ny_pred = np.argmax(y_prob, axis=1)\nlabel_encoder.index_to_class[y_pred[0]]\n</code></pre> <pre>\nSports\n</pre> <pre><code># Class distributions\nprob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\nprint (json.dumps(prob_dist, indent=2))\n</code></pre> <pre>\n{\n  \"Sports\": 0.9999359846115112,\n  \"World\": 4.0660612285137177e-05,\n  \"Sci/Tech\": 1.1774928680097219e-05,\n  \"Business\": 1.1545793313416652e-05\n}\n</pre>"},{"location":"courses/foundations/transformers/#interpretability","title":"Interpretability","text":"<p>Let's visualize the self-attention weights from each of the attention heads in the encoder.</p> <pre><code>import sys\n!rm -r bertviz_repo\n!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\nif not \"bertviz_repo\" in sys.path:\n  sys.path += [\"bertviz_repo\"]\n</code></pre> <pre><code>from bertviz import head_view\n</code></pre> <pre><code># Print input ids\nprint (ids)\nprint (tokenizer.batch_decode(ids))\n</code></pre> <pre>\ntensor([[  102,  2531,  3617,  8869, 23589,  4972,  8553,  2205,  4082,   103]],\n       device=\"cpu\")\n['[CLS] final tennis tournament starts next week [SEP]']\n</pre> <pre><code># Get encoder attentions\nseq, pool, attn = model.transformer(input_ids=ids, attention_mask=masks, output_attentions=True)\nprint (len(attn)) # 12 attention layers (heads)\nprint (attn[0].shape)\n</code></pre> <pre>\n12\ntorch.Size([1, 12, 10, 10])\n</pre> <pre><code># HTML set up\ndef call_html():\n  import IPython\n  display(IPython.core.display.HTML('''\n        &lt;script src=\"/static/components/requirejs/require.js\"&gt;&lt;/script&gt;\n        &lt;script&gt;\n          requirejs.config({\n            paths: {\n              base: '/static/base',\n              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n            },\n          });\n        &lt;/script&gt;\n        '''))\n</code></pre> <pre><code># Visualize self-attention weights\ncall_html()\ntokens = tokenizer.convert_ids_to_tokens(ids[0])\nhead_view(attention=attn, tokens=tokens)\n</code></pre> <p>Now we're ready to start the MLOps course to learn how to apply all this foundational modeling knowledge to responsibly develop, deploy and maintain production machine learning applications.</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Transformers - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/foundations/utilities/","title":"Utilities for Machine Learning","text":""},{"location":"courses/foundations/utilities/#set-up","title":"Set up","text":"<p>We're having to set a lot of seeds for reproducibility now, so let's wrap it all up in a function.</p> <p><pre><code>import numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\n</code></pre> <pre><code>SEED = 1234\n</code></pre> <pre><code>def set_seeds(seed=1234):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # multi-GPU\n</code></pre> <pre><code># Set seeds for reproducibility\nset_seeds(seed=SEED)\n</code></pre> <pre><code># Set device\ncuda = True\ndevice = torch.device(\"cuda\" if (\n    torch.cuda.is_available() and cuda) else \"cpu\")\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\nif device.type == \"cuda\":\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\nprint (device)\n</code></pre></p> <pre>\ncuda\n</pre>"},{"location":"courses/foundations/utilities/#load-data","title":"Load data","text":"<p>We'll use the same spiral dataset from previous lessons to demonstrate our utilities. <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n</code></pre> <pre><code># Load data\nurl = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/spiral.csv\"\ndf = pd.read_csv(url, header=0) # load\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle\ndf.head()\n</code></pre></p> X1 X2 color 0 0.106737 0.114197 c1 1 0.311513 -0.664028 c1 2 0.019870 -0.703126 c1 3 -0.054017 0.508159 c3 4 -0.127751 -0.011382 c3 <pre><code># Data shapes\nX = df[[\"X1\", \"X2\"]].values\ny = df[\"color\"].values\nprint (\"X: \", np.shape(X))\nprint (\"y: \", np.shape(y))\n</code></pre> <pre>\nX:  (1500, 2)\ny:  (1500,)\n</pre> <pre><code># Visualize data\nplt.title(\"Generated non-linear data\")\ncolors = {\"c1\": \"red\", \"c2\": \"yellow\", \"c3\": \"blue\"}\nplt.scatter(X[:, 0], X[:, 1], c=[colors[_y] for _y in y], edgecolors=\"k\", s=25)\nplt.show()\n</code></pre>"},{"location":"courses/foundations/utilities/#split-data","title":"Split data","text":"<p><pre><code>import collections\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>TRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 0.15\n</code></pre> <pre><code>def train_val_test_split(X, y, train_size):\n\"\"\"Split dataset into data splits.\"\"\"\n    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n</code></pre> <pre><code># Create data splits\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n    X=X, y=y, train_size=TRAIN_SIZE)\nprint (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\nprint (f\"Sample point: {X_train[0]} \u2192 {y_train[0]}\")\n</code></pre></p> <pre>\nX_train: (1050, 2), y_train: (1050,)\nX_val: (225, 2), y_val: (225,)\nX_test: (225, 2), y_test: (225,)\nSample point: [-0.63919105 -0.69724176] \u2192 c1\n</pre>"},{"location":"courses/foundations/utilities/#label-encoding","title":"Label encoding","text":"<p>Next we'll define a <code>LabelEncoder</code> to encode our text labels into unique indices. We're not going to use scikit-learn's LabelEncoder anymore because we want to be able to save and load our instances the way we want to. <pre><code>import itertools\n</code></pre> <pre><code>class LabelEncoder(object):\n\"\"\"Label encoder for tag labels.\"\"\"\n    def __init__(self, class_to_index={}):\n        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n\n    def __len__(self):\n        return len(self.class_to_index)\n\n    def __str__(self):\n        return f\"&lt;LabelEncoder(num_classes={len(self)})&gt;\"\n\n    def fit(self, y):\n        classes = np.unique(y)\n        for i, class_ in enumerate(classes):\n            self.class_to_index[class_] = i\n        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n        self.classes = list(self.class_to_index.keys())\n        return self\n\n    def encode(self, y):\n        encoded = np.zeros((len(y)), dtype=int)\n        for i, item in enumerate(y):\n            encoded[i] = self.class_to_index[item]\n        return encoded\n\n    def decode(self, y):\n        classes = []\n        for i, item in enumerate(y):\n            classes.append(self.index_to_class[item])\n        return classes\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {'class_to_index': self.class_to_index}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Encode\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nlabel_encoder.class_to_index\n</code></pre></p> <pre>\n{\"c1\": 0, \"c2\": 1, \"c3\": 2}\n</pre> <pre><code># Convert labels to tokens\nprint (f\"y_train[0]: {y_train[0]}\")\ny_train = label_encoder.encode(y_train)\ny_val = label_encoder.encode(y_val)\ny_test = label_encoder.encode(y_test)\nprint (f\"y_train[0]: {y_train[0]}\")\n</code></pre> <pre>\ny_train[0]: c1\ny_train[0]: 0\n</pre> <pre><code># Class weights\ncounts = np.bincount(y_train)\nclass_weights = {i: 1.0/count for i, count in enumerate(counts)}\nprint (f\"counts: {counts}\\nweights: {class_weights}\")\n</code></pre> <pre>\ncounts: [350 350 350]\nweights: {0: 0.002857142857142857, 1: 0.002857142857142857, 2: 0.002857142857142857}\n</pre>"},{"location":"courses/foundations/utilities/#standardize-data","title":"Standardize data","text":"<p>We need to standardize our data (zero mean and unit variance) so a specific feature's magnitude doesn't affect how the model learns its weights. We're only going to standardize the inputs X because our outputs y are class values. We're going to compose our own <code>StandardScaler</code> class so we can easily save and load it later during inference. <pre><code>class StandardScaler(object):\n    def __init__(self, mean=None, std=None):\n        self.mean = np.array(mean)\n        self.std = np.array(std)\n\n    def fit(self, X):\n        self.mean =  np.mean(X_train, axis=0)\n        self.std = np.std(X_train, axis=0)\n\n    def scale(self, X):\n        return (X - self.mean) / self.std\n\n    def unscale(self, X):\n        return (X * self.std) + self.mean\n\n    def save(self, fp):\n        with open(fp, \"w\") as fp:\n            contents = {\"mean\": self.mean.tolist(), \"std\": self.std.tolist()}\n            json.dump(contents, fp, indent=4, sort_keys=False)\n\n    @classmethod\n    def load(cls, fp):\n        with open(fp, \"r\") as fp:\n            kwargs = json.load(fp=fp)\n        return cls(**kwargs)\n</code></pre> <pre><code># Standardize the data (mean=0, std=1) using training data\nX_scaler = StandardScaler()\nX_scaler.fit(X_train)\n</code></pre> <pre><code># Apply scaler on training and test data (don't standardize outputs for classification)\nX_train = X_scaler.scale(X_train)\nX_val = X_scaler.scale(X_val)\nX_test = X_scaler.scale(X_test)\n</code></pre> <pre><code># Check (means should be ~0 and std should be ~1)\nprint (f\"X_test[0]: mean: {np.mean(X_test[:, 0], axis=0):.1f}, std: {np.std(X_test[:, 0], axis=0):.1f}\")\nprint (f\"X_test[1]: mean: {np.mean(X_test[:, 1], axis=0):.1f}, std: {np.std(X_test[:, 1], axis=0):.1f}\")\n</code></pre></p> <pre>\nX_test[0]: mean: 0.1, std: 0.9\nX_test[1]: mean: 0.0, std: 1.0\n</pre>"},{"location":"courses/foundations/utilities/#dataloader","title":"DataLoader","text":"<p>We're going to place our data into a <code>Dataset</code> and use a <code>DataLoader</code> to efficiently create batches for training and evaluation.</p> <p><pre><code>import torch\n</code></pre> <pre><code># Seed seed for reproducibility\ntorch.manual_seed(SEED)\n</code></pre> <pre><code>class Dataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __str__(self):\n        return f\"&lt;Dataset(N={len(self)})&gt;\"\n\n    def __getitem__(self, index):\n        X = self.X[index]\n        y = self.y[index]\n        return [X, y]\n\n    def collate_fn(self, batch):\n\"\"\"Processing on a batch.\"\"\"\n        # Get inputs\n        batch = np.array(batch)\n        X = np.stack(batch[:, 0], axis=0)\n        y = batch[:, 1]\n\n        # Cast\n        X = torch.FloatTensor(X.astype(np.float32))\n        y = torch.LongTensor(y.astype(np.int32))\n\n        return X, y\n\n    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n        return torch.utils.data.DataLoader(\n            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n            shuffle=shuffle, drop_last=drop_last, pin_memory=True)\n</code></pre> We don't really need the <code>collate_fn</code> here but we wanted to make it transparent because we will need it when we want to do specific processing on our batch (ex. padding).</p> <pre><code># Create datasets\ntrain_dataset = Dataset(X=X_train, y=y_train)\nval_dataset = Dataset(X=X_val, y=y_val)\ntest_dataset = Dataset(X=X_test, y=y_test)\nprint (\"Datasets:\\n\"\n    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {train_dataset[0][0]}\\n\"\n    f\"  y: {train_dataset[0][1]}\")\n</code></pre> <pre>\nDatasets:\n  Train dataset: &lt;Dataset(N=1050)&gt;\n  Val dataset: &lt;Dataset(N=225)&gt;\n  Test dataset: &lt;Dataset(N=225)&gt;\nSample point:\n  X: [-1.47355106 -1.67417243]\n  y: 0\n</pre> <p>So far, we used batch gradient descent to update our weights. This means that we calculated the gradients using the entire training dataset. We also could've updated our weights using stochastic gradient descent (SGD) where we pass in one training example one at a time. The current standard is mini-batch gradient descent, which strikes a balance between batch and SGD, where we update the weights using a mini-batch of n (<code>BATCH_SIZE</code>) samples. This is where the <code>DataLoader</code> object comes in handy. <pre><code># Create dataloaders\nbatch_size = 64\ntrain_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\nval_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\ntest_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\nbatch_X, batch_y = next(iter(train_dataloader))\nprint (\"Sample batch:\\n\"\n    f\"  X: {list(batch_X.size())}\\n\"\n    f\"  y: {list(batch_y.size())}\\n\"\n    \"Sample point:\\n\"\n    f\"  X: {batch_X[0]}\\n\"\n    f\"  y: {batch_y[0]}\")\n</code></pre></p> <pre>\nSample batch:\n  X: [64, 2]\n  y: [64]\nSample point:\n  X: tensor([-1.4736, -1.6742])\n  y: 0\n</pre>"},{"location":"courses/foundations/utilities/#device","title":"Device","text":"<p>So far we've been running our operations on the CPU but when we have large datasets and larger models to train, we can benefit by parallelizing tensor operations on a GPU. In this notebook, you can use a GPU by going to <code>Runtime</code> &gt; <code>Change runtime type</code> &gt; Select <code>GPU</code> in the <code>Hardware accelerator</code> dropdown. We can what device we're using with the following line of code:</p> <p><pre><code># Set CUDA seeds\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED) # multi-GPU\n</code></pre> <pre><code># Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (device)\n</code></pre></p> <pre>\ncuda\n</pre>"},{"location":"courses/foundations/utilities/#model","title":"Model","text":"<p>Let's initialize the model we'll be using to show the capabilities of training utilities.</p> <p><pre><code>import math\nfrom torch import nn\nimport torch.nn.functional as F\n</code></pre> <pre><code>INPUT_DIM = X_train.shape[1] # 2D\nHIDDEN_DIM = 100\nDROPOUT_P = 0.1\nNUM_CLASSES = len(label_encoder.classes)\nNUM_EPOCHS = 10\n</code></pre> <pre><code>class MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout_p, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, inputs):\n        x_in, = inputs\n        z = F.relu(self.fc1(x_in))\n        z = self.dropout(z)\n        z = self.fc2(z)\n        return z\n</code></pre> <pre><code># Initialize model\nmodel = MLP(\n    input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel = model.to(device) # set device\nprint (model.named_parameters)\n</code></pre></p> <pre>\n&lt;bound method Module.named_parameters of MLP(\n  (fc1): Linear(in_features=2, out_features=100, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc2): Linear(in_features=100, out_features=3, bias=True)\n)&gt;\n</pre>"},{"location":"courses/foundations/utilities/#trainer","title":"Trainer","text":"<p>So far we've been writing training loops that train only using the train data split and then we perform evaluation on our test set. But in reality, we would follow this process:</p> <ol> <li>Train using mini-batches on one epoch of the train data split.</li> <li>Evaluate loss on the validation split and use it to adjust hyperparameters (ex. learning rate).</li> <li>After training ends (via stagnation in improvements, desired performance, etc.), evaluate your trained model on the test (hold-out) data split.</li> </ol> <p>We'll create a <code>Trainer</code> class to keep all of these processes organized.</p> <p>The first function in the class is <code>train_step</code> which will train the model using batches from one epoch of the train data split.</p> <pre><code>def train_step(self, dataloader):\n\"\"\"Train step.\"\"\"\n    # Set model to train mode\n    self.model.train()\n    loss = 0.0\n\n    # Iterate over train batches\n    for i, batch in enumerate(dataloader):\n\n        # Step\n        batch = [item.to(self.device) for item in batch]  # Set device\n        inputs, targets = batch[:-1], batch[-1]\n        self.optimizer.zero_grad()  # Reset gradients\n        z = self.model(inputs)  # Forward pass\n        J = self.loss_fn(z, targets)  # Define loss\n        J.backward()  # Backward pass\n        self.optimizer.step()  # Update weights\n\n        # Cumulative Metrics\n        loss += (J.detach().item() - loss) / (i + 1)\n\n    return loss\n</code></pre> <p>Next we'll define the <code>eval_step</code> which will be used for processing both the validation and test data splits. This is because neither of them require gradient updates and display the same metrics.</p> <pre><code>def eval_step(self, dataloader):\n\"\"\"Validation or test step.\"\"\"\n    # Set model to eval mode\n    self.model.eval()\n    loss = 0.0\n    y_trues, y_probs = [], []\n\n    # Iterate over val batches\n    with torch.inference_mode():\n        for i, batch in enumerate(dataloader):\n\n            # Step\n            batch = [item.to(self.device) for item in batch]  # Set device\n            inputs, y_true = batch[:-1], batch[-1]\n            z = self.model(inputs)  # Forward pass\n            J = self.loss_fn(z, y_true).item()\n\n            # Cumulative Metrics\n            loss += (J - loss) / (i + 1)\n\n            # Store outputs\n            y_prob = F.softmax(z).cpu().numpy()\n            y_probs.extend(y_prob)\n            y_trues.extend(y_true.cpu().numpy())\n\n    return loss, np.vstack(y_trues), np.vstack(y_probs)\n</code></pre> <p>The final function is the <code>predict_step</code> which will be used for inference. It's fairly similar to the <code>eval_step</code> except we don't calculate any metrics. We pass on the predictions which we can use to generate our performance scores.</p> <pre><code>def predict_step(self, dataloader):\n\"\"\"Prediction step.\"\"\"\n    # Set model to eval mode\n    self.model.eval()\n    y_probs = []\n\n    # Iterate over val batches\n    with torch.inference_mode():\n        for i, batch in enumerate(dataloader):\n\n            # Forward pass w/ inputs\n            inputs, targets = batch[:-1], batch[-1]\n            z = self.model(inputs)\n\n            # Store outputs\n            y_prob = F.softmax(z).cpu().numpy()\n            y_probs.extend(y_prob)\n\n    return np.vstack(y_probs)\n</code></pre>"},{"location":"courses/foundations/utilities/#lr-scheduler","title":"LR scheduler","text":"<p>As our model starts to optimize and perform better, the loss will reduce and we'll need to make smaller adjustments. If we keep using a fixed learning rate, we'll be overshooting back and forth. Therefore, we're going to add a learning rate scheduler to our optimizer to adjust our learning rate during training. There are many schedulers schedulers to choose from but a popular one is <code>ReduceLROnPlateau</code> which reduces the learning rate when a metric (ex. validation loss) stops improving. In the example below we'll reduce the learning rate by a factor of 0.1 (<code>factor=0.1</code>) when our metric of interest (<code>self.scheduler.step(val_loss)</code>) stops decreasing (<code>mode=\"min\"</code>) for three (<code>patience=3</code>) straight epochs.</p> <pre><code># Initialize the LR scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n...\ntrain_loop():\n    ...\n    # Steps\n    train_loss = trainer.train_step(dataloader=train_dataloader)\n    val_loss, _, _ = trainer.eval_step(dataloader=val_dataloader)\n    self.scheduler.step(val_loss)\n    ...\n</code></pre>"},{"location":"courses/foundations/utilities/#early-stopping","title":"Early stopping","text":"<p>We should never train our models for an arbitrary number of epochs but instead we should have explicit stopping criteria (even if you are bootstrapped by compute resources). Common stopping criteria include when validation performance stagnates for certain # of epochs (<code>patience</code>), desired performance is reached, etc.</p> <pre><code># Early stopping\nif val_loss &lt; best_val_loss:\n    best_val_loss = val_loss\n    best_model = trainer.model\n    _patience = patience  # reset _patience\nelse:\n    _patience -= 1\nif not _patience:  # 0\n    print(\"Stopping early!\")\n    break\n</code></pre>"},{"location":"courses/foundations/utilities/#training","title":"Training","text":"<p>Let's put all of this together now to train our model.</p> <p><pre><code>from torch.optim import Adam\n</code></pre> <pre><code>LEARNING_RATE = 1e-2\nNUM_EPOCHS = 100\nPATIENCE = 3\n</code></pre> <pre><code># Define Loss\nclass_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n</code></pre> <pre><code># Define optimizer &amp; scheduler\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", factor=0.1, patience=3)\n</code></pre> <pre><code>class Trainer(object):\n    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n\n        # Set params\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train_step(self, dataloader):\n\"\"\"Train step.\"\"\"\n        # Set model to train mode\n        self.model.train()\n        loss = 0.0\n\n        # Iterate over train batches\n        for i, batch in enumerate(dataloader):\n\n            # Step\n            batch = [item.to(self.device) for item in batch]  # Set device\n            inputs, targets = batch[:-1], batch[-1]\n            self.optimizer.zero_grad()  # Reset gradients\n            z = self.model(inputs)  # Forward pass\n            J = self.loss_fn(z, targets)  # Define loss\n            J.backward()  # Backward pass\n            self.optimizer.step()  # Update weights\n\n            # Cumulative Metrics\n            loss += (J.detach().item() - loss) / (i + 1)\n\n        return loss\n\n    def eval_step(self, dataloader):\n\"\"\"Validation or test step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        loss = 0.0\n        y_trues, y_probs = [], []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Step\n                batch = [item.to(self.device) for item in batch]  # Set device\n                inputs, y_true = batch[:-1], batch[-1]\n                z = self.model(inputs)  # Forward pass\n                J = self.loss_fn(z, y_true).item()\n\n                # Cumulative Metrics\n                loss += (J - loss) / (i + 1)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n                y_trues.extend(y_true.cpu().numpy())\n\n        return loss, np.vstack(y_trues), np.vstack(y_probs)\n\n    def predict_step(self, dataloader):\n\"\"\"Prediction step.\"\"\"\n        # Set model to eval mode\n        self.model.eval()\n        y_probs = []\n\n        # Iterate over val batches\n        with torch.inference_mode():\n            for i, batch in enumerate(dataloader):\n\n                # Forward pass w/ inputs\n                inputs, targets = batch[:-1], batch[-1]\n                z = self.model(inputs)\n\n                # Store outputs\n                y_prob = F.softmax(z).cpu().numpy()\n                y_probs.extend(y_prob)\n\n        return np.vstack(y_probs)\n\n    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n        best_val_loss = np.inf\n        for epoch in range(num_epochs):\n            # Steps\n            train_loss = self.train_step(dataloader=train_dataloader)\n            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n            self.scheduler.step(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                best_model = self.model\n                _patience = patience  # reset _patience\n            else:\n                _patience -= 1\n            if not _patience:  # 0\n                print(\"Stopping early!\")\n                break\n\n            # Logging\n            print(\n                f\"Epoch: {epoch+1} | \"\n                f\"train_loss: {train_loss:.5f}, \"\n                f\"val_loss: {val_loss:.5f}, \"\n                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n                f\"_patience: {_patience}\"\n            )\n        return best_model\n</code></pre> <pre><code># Trainer module\ntrainer = Trainer(\n    model=model, device=device, loss_fn=loss_fn,\n    optimizer=optimizer, scheduler=scheduler)\n</code></pre> <pre><code># Train\nbest_model = trainer.train(\n    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)\n</code></pre></p> <pre>\nEpoch: 1 | train_loss: 0.73999, val_loss: 0.58441, lr: 1.00E-02, _patience: 3\nEpoch: 2 | train_loss: 0.52631, val_loss: 0.41542, lr: 1.00E-02, _patience: 3\nEpoch: 3 | train_loss: 0.40919, val_loss: 0.30673, lr: 1.00E-02, _patience: 3\nEpoch: 4 | train_loss: 0.31421, val_loss: 0.22428, lr: 1.00E-02, _patience: 3\n...\nEpoch: 48 | train_loss: 0.04100, val_loss: 0.02100, lr: 1.00E-02, _patience: 2\nEpoch: 49 | train_loss: 0.04155, val_loss: 0.02008, lr: 1.00E-02, _patience: 3\nEpoch: 50 | train_loss: 0.05295, val_loss: 0.02094, lr: 1.00E-02, _patience: 2\nEpoch: 51 | train_loss: 0.04619, val_loss: 0.02179, lr: 1.00E-02, _patience: 1\nStopping early!\n</pre>"},{"location":"courses/foundations/utilities/#evaluation","title":"Evaluation","text":"<p><pre><code>import json\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code>def get_metrics(y_true, y_pred, classes):\n\"\"\"Per-class performance metrics.\"\"\"\n    # Performance\n    performance = {\"overall\": {}, \"class\": {}}\n\n    # Overall performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance[\"overall\"][\"precision\"] = metrics[0]\n    performance[\"overall\"][\"recall\"] = metrics[1]\n    performance[\"overall\"][\"f1\"] = metrics[2]\n    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n\n    # Per-class performance\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n    for i in range(len(classes)):\n        performance[\"class\"][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i]),\n        }\n\n    return performance\n</code></pre> <pre><code># Get predictions\ntest_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\ny_pred = np.argmax(y_prob, axis=1)\n</code></pre> <pre><code># Determine performance\nperformance = get_metrics(\n    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\nprint (json.dumps(performance[\"overall\"], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.9956140350877193,\n  \"recall\": 0.9955555555555556,\n  \"f1\": 0.9955553580159119,\n  \"num_samples\": 225.0\n}\n</pre>"},{"location":"courses/foundations/utilities/#saving-loading","title":"Saving &amp; loading","text":"<p>Many tutorials never show you how to save the components you created so you can load them for inference.</p> <p><pre><code>from pathlib import Path\n</code></pre> <pre><code># Save artifacts\ndir = Path(\"mlp\")\ndir.mkdir(parents=True, exist_ok=True)\nlabel_encoder.save(fp=Path(dir, \"label_encoder.json\"))\nX_scaler.save(fp=Path(dir, \"X_scaler.json\"))\ntorch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\nwith open(Path(dir, 'performance.json'), \"w\") as fp:\n    json.dump(performance, indent=2, sort_keys=False, fp=fp)\n</code></pre> <pre><code># Load artifacts\ndevice = torch.device(\"cpu\")\nlabel_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\nX_scaler = StandardScaler.load(fp=Path(dir, \"X_scaler.json\"))\nmodel = MLP(\n    input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\nmodel.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\nmodel.to(device)\n</code></pre></p> <pre>\nMLP(\n  (fc1): Linear(in_features=2, out_features=100, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc2): Linear(in_features=100, out_features=3, bias=True)\n)\n</pre> <p><pre><code># Initialize trainer\ntrainer = Trainer(model=model, device=device)\n</code></pre> <pre><code># Dataloader\nsample = [[0.106737, 0.114197]] # c1\nX = X_scaler.scale(sample)\ny_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\ndataset = Dataset(X=X, y=y_filler)\ndataloader = dataset.create_dataloader(batch_size=batch_size)\n</code></pre> <pre><code># Inference\ny_prob = trainer.predict_step(dataloader)\ny_pred = np.argmax(y_prob, axis=1)\nlabel_encoder.decode(y_pred)\n</code></pre></p> <pre>\n[\"c1\"]\n</pre>"},{"location":"courses/foundations/utilities/#miscellaneous","title":"Miscellaneous","text":"<p>There are lots of other utilities to cover, such as:</p> <ul> <li>Tokenizers to convert text to sequence of indices</li> <li>Various encoders to represent our data</li> <li>Padding to ensure uniform data shapes</li> <li>Experiment tracking to visualize and keep track of all experiments</li> <li>Hyperparameter optimization to tune our parameters (layers, learning rate, etc.)</li> <li>and many more!</li> </ul> <p>We'll explore these as we require them in future lessons including some in our MLOps course!</p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Utilities - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/api/","title":"APIs for Model Serving","text":""},{"location":"courses/mlops/api/#intuition","title":"Intuition","text":"<p>Our CLI application made it much easier to interact with our models, especially for fellow team members who may not want to delve into the codebase. But there are several limitations to serving our models with a CLI:</p> <ul> <li>users need access to the terminal, codebase, virtual environment, etc.</li> <li>CLI outputs on the terminal are not exportable</li> </ul> <p>To address these issues, we're going to develop an application programming interface (API) that will anyone to interact with our application with a simple request.</p> <p>The end user may not directly interact with our API but may use UI/UX components that send requests to our it.</p>"},{"location":"courses/mlops/api/#serving","title":"Serving","text":"<p>APIs allow different applications to communicate with each other in real-time. But when it comes to serving predictions, we need to first decide if we'll do that in batches or real-time, which is entirely based on the feature space (finite vs. unbound).</p>"},{"location":"courses/mlops/api/#batch-serving","title":"Batch serving","text":"<p>We can make batch predictions on a finite set of inputs which are then written to a database for low latency inference. When a user or downstream process sends an inference request in real-time, cached results from the database are returned.</p> <ul> <li>\u2705\u00a0 generate and cache predictions for very fast inference for users.</li> <li>\u2705\u00a0 the model doesn't need to be spun up as it's own service since it's never used in real-time.</li> <li>\u274c\u00a0 predictions can become stale if user develops new interests that aren\u2019t captured by the old data that the current predictions are based on.</li> <li>\u274c\u00a0 input feature space must be finite because we need to generate all the predictions before they're needed for real-time.</li> </ul> <p>Batch serving tasks</p> <p>What are some tasks where batch serving is ideal?</p> Show answer <p>Recommend content that existing users will like based on their viewing history. However, new users may just receive some generic recommendations based on their explicit interests until we process their history the next day. And even if we're not doing batch serving, it might still be useful to cache very popular sets of input features (ex. combination of explicit interests leads to certain recommended content) so that we can serve those predictions faster.</p>"},{"location":"courses/mlops/api/#real-time-serving","title":"Real-time serving","text":"<p>We can also serve live predictions, typically through a request to an API with the appropriate input data.</p> <ul> <li>\u2705\u00a0 can yield more up-to-date predictions which may yield a more meaningful user experience, etc.</li> <li>\u274c\u00a0 requires managed microservices to handle request traffic.</li> <li>\u274c\u00a0 requires real-time monitoring since input space in unbounded, which could yield erroneous predictions.</li> </ul> <p>In this lesson, we'll create the API required to enable real-time serving. The interactions in our situation involve the client (users, other applications, etc.) sending a request (ex. prediction request) with the appropriate inputs to the server (our application with a trained model) and receiving a response (ex. prediction) in return.</p>"},{"location":"courses/mlops/api/#request","title":"Request","text":"<p>Users will interact with our API in the form of a request. Let's take a look at the different components of a request:</p>"},{"location":"courses/mlops/api/#uri","title":"URI","text":"<p>A uniform resource identifier (URI) is an identifier for a specific resource.</p> <pre>\nhttps://localhost:8000/models/{modelId}/?filter=passed#details\n</pre> Parts of the URI Description scheme protocol definition domain address of the website port endpoint path location of the resource query string parameters to identify resources anchor location on webpage Parts of the path Description <code>/models</code> collection resource of all <code>models</code> <code>/models/{modelID}</code> single resource from the <code>models</code> collection <code>modelId</code> path parameters <code>filter</code> query parameter"},{"location":"courses/mlops/api/#method","title":"Method","text":"<p>The method is the operation to execute on the specific resource defined by the URI. There are many possible methods to choose from, but the four below are the most popular, which are often referred to as CRUD because they allow you to Create, Read, Update and Delete.</p> <ul> <li><code>GET</code>: get a resource.</li> <li><code>POST</code>: create or update a resource.</li> <li><code>PUT/PATCH</code>: create or update a resource.</li> <li><code>DELETE</code>: delete a resource.</li> </ul> <p>Note</p> <p>You could use either the <code>POST</code> or <code>PUT</code> request method to create and modify resources but the main difference is that <code>PUT</code> is idempotent which means you can call the method repeatedly and it'll produce the same state every time. Whereas, calling <code>POST</code> multiple times can result in creating multiple instance and so changes the overall state each time.</p> <pre><code>POST /models/&lt;new_model&gt; -d {}       # error since we haven't created the `new_model` resource yet\nPOST /models -d {}                   # creates a new model based on information provided in data\nPOST /models/&lt;existing_model&gt; -d {}  # updates an existing model based on information provided in data\n\nPUT /models/&lt;new_model&gt; -d {}        # creates a new model based on information provided in data\nPUT /models/&lt;existing_model&gt; -d {}   # updates an existing model based on information provided in data\n</code></pre> <p>We can use cURL to execute our API calls with the following options:</p> <pre><code>curl --help\n</code></pre> <pre>\nUsage: curl [options...] \n-X, --request  HTTP method (ie. GET)\n-H, --header   headers to be sent to the request (ex. authentication)\n-d, --data     data to POST, PUT/PATCH, DELETE (usually JSON)\n...\n\n\n<p>For example, if we want to GET all <code>models</code>, our cURL command would look like this:\n<pre><code>curl -X GET \"http://localhost:8000/models\"\n</code></pre></p>\n<p></p>"},{"location":"courses/mlops/api/#headers","title":"Headers","text":"<p>Headers contain information about a certain event and are usually found in both the client's request as well as the server's response. It can range from what type of format they'll send and receive, authentication and caching info, etc.\n<pre><code>curl -X GET \"http://localhost:8000/\" \\          # method and URI\n-H  \"accept: application/json\"  \\           # client accepts JSON\n-H  \"Content-Type: application/json\" \\      # client sends JSON\n</code></pre></p>\n<p></p>"},{"location":"courses/mlops/api/#body","title":"Body","text":"<p>The body contains information that may be necessary for the request to be processed. It's usually a JSON object sent during <code>POST</code>, <code>PUT</code>/<code>PATCH</code>, <code>DELETE</code> request methods.</p>\n<pre><code>curl -X POST \"http://localhost:8000/models\" \\   # method and URI\n-H  \"accept: application/json\" \\            # client accepts JSON\n-H  \"Content-Type: application/json\" \\      # client sends JSON\n-d \"{'name': 'RoBERTa', ...}\"               # request body\n</code></pre>\n<p></p>"},{"location":"courses/mlops/api/#response","title":"Response","text":"<p>The response we receive from our server is the result of the request we sent. The response also includes headers and a body which should include the proper HTTP status code as well as explicit messages, data, etc.</p>\n<pre><code>{\n\"message\": \"OK\",\n  \"method\": \"GET\",\n  \"status-code\": 200,\n  \"url\": \"http://localhost:8000/\",\n  \"data\": {}\n}\n</code></pre>\n<p>We may also want to include other metadata in the response such as model version, datasets used, etc. Anything that the downstream consumer may be interested in or metadata that might be useful for inspection.</p>\n<p>There are many HTTP status codes to choose from depending on the situation but here are the most common options:</p>\n<p>\nCode\nDescription\n<code>200 OK</code>\nmethod operation was successful.\n<code>201 CREATED</code>\n<code>POST</code> or <code>PUT</code> method successfully created a resource.\n<code>202 ACCEPTED</code>\nthe request was accepted for processing (but processing may not be done).\n<code>400 BAD REQUEST</code>\nserver cannot process the request because of a client side error.\n<code>401 UNAUTHORIZED</code>\nyou're missing required authentication.\n<code>403 FORBIDDEN</code>\nyou're not allowed to do this operation.\n<code>404 NOT FOUND</code>\nthe resource you're looking for was not found.\n<code>500 INTERNAL SERVER ERROR</code>\nthere was a failure somewhere in the system process.\n<code>501 NOT IMPLEMENTED</code>\nthis operation on the resource doesn't exist yet.\n<p></p>"},{"location":"courses/mlops/api/#best-practices","title":"Best practices","text":"<p>When designing our API, there are some best practices to follow:</p>\n<ul>\n<li>URI paths, messages, etc. should be as explicit as possible. Avoid using cryptic resource names, etc.</li>\n<li>Use nouns, instead of verbs, for naming resources. The request method already accounts for the verb (\u2705\u00a0 <code>GET /users</code> not \u274c\u00a0 <code>GET /get_users</code>).</li>\n<li>Plural nouns (\u2705\u00a0 <code>GET /users/{userId}</code> not \u274c\u00a0 <code>GET /user/{userID}</code>).</li>\n<li>Use dashes in URIs for resources and path parameters but use underscores for query parameters (<code>GET /nlp-models/?find_desc=bert</code>).</li>\n<li>Return appropriate HTTP and informative messages to the user.</li>\n</ul>"},{"location":"courses/mlops/api/#implementation","title":"Implementation","text":"<p>We're going to define our API in a separate <code>app</code> directory because, in the future, we may have additional packages like <code>tagifai</code> and we don't want our app to be attached to any one package. Inside our <code>app</code> directory, we'll create the follow scripts:</p>\n<pre><code>mkdir app\ncd app\ntouch api.py gunicorn.py schemas.py\ncd ../\n</code></pre>\n<pre><code>app/\n\u251c\u2500\u2500 api.py          - FastAPI app\n\u251c\u2500\u2500 gunicorn.py     - WSGI script\n\u2514\u2500\u2500 schemas.py      - API model schemas\n</code></pre>\n<ul>\n<li><code>api.py</code>: the main script that will include our API initialization and endpoints.</li>\n<li><code>gunicorn.py</code>: script for defining API worker configurations.</li>\n<li><code>schemas.py</code>: definitions for the different objects we'll use in our resource endpoints.</li>\n</ul>"},{"location":"courses/mlops/api/#fastapi","title":"FastAPI","text":"<p>We're going to use FastAPI as our framework to build our API service. There are plenty of other framework options out there such as Flask, Django and even non-Python based options like Node, Angular, etc. FastAPI combines many of the advantages across these frameworks and is maturing quickly and becoming more widely adopted. It's notable advantages include:</p>\n<ul>\n<li>development in Python</li>\n<li>highly performant</li>\n<li>data validation via pydantic</li>\n<li>autogenerated documentation</li>\n<li>dependency injection</li>\n<li>security via OAuth2</li>\n</ul>\n<pre><code>pip install fastapi==0.78.0\n</code></pre>\n<pre><code># Add to requirements.txt\nfastapi==0.78.0\n</code></pre>\n<p>Your choice of framework also depends on your team's existing systems and processes. However, with the wide adoption of microservices, we can wrap our specific application in any framework we choose and expose the appropriate resources so all other systems can easily communicate with it.</p>"},{"location":"courses/mlops/api/#initialization","title":"Initialization","text":"<p>The first step is to initialize our API in our <code>api.py</code> script` by defining metadata like the title, description and version:</p>\n<pre><code># app/api.py\nfrom fastapi import FastAPI\n\n# Define application\napp = FastAPI(\n    title=\"TagIfAI - Made With ML\",\n    description=\"Classify machine learning projects.\",\n    version=\"0.1\",\n)\n</code></pre>\n<p>Our first endpoint is going to be a simple one where we want to show that everything is working as intended. The path for the endpoint will just be <code>/</code> (when a user visit our base URI) and it'll be a <code>GET</code> request. This simple endpoint is often used as a health check to ensure that our application is indeed up and running properly.</p>\n<pre><code># app/api.py\nfrom http import HTTPStatus\nfrom typing import Dict\n@app.get(\"/\")\ndef _index() -&gt; Dict:\n\"\"\"Health check.\"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"data\": {},\n    }\n    return response\n</code></pre>\n<p>We let our application know that the endpoint is at <code>/</code> through the path operation decorator in line 4 and we return a JSON response with the <code>200 OK</code> HTTP status code.</p>\n<p>In our actual <code>api.py</code> script, you'll notice that even our index function looks different. Don't worry, we're slowly adding components to our endpoints and justifying them along the way.</p>"},{"location":"courses/mlops/api/#launching","title":"Launching","text":"<p>We're using Uvicorn, a fast ASGI server that can run asynchronous code in a single process to launch our application.</p>\n<pre><code>pip install uvicorn==0.17.6\n</code></pre>\n<pre><code># Add to requirements.txt\nuvicorn==0.17.6\n</code></pre>\n<p>We can launch our application with the following command:</p>\n<pre><code>uvicorn app.api:app \\       # location of app (`app` directory &gt; `api.py` script &gt; `app` object)\n--host 0.0.0.0 \\        # localhost\n--port 8000 \\           # port 8000\n--reload \\              # reload every time we update\n--reload-dir tagifai \\  # only reload on updates to `tagifai` directory\n--reload-dir app        # and the `app` directory\n</code></pre>\n<pre>\nINFO:     Will watch for changes in these directories: ['/Users/goku/Documents/madewithml/mlops/app', '/Users/goku/Documents/madewithml/mlops/tagifai']\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [57609] using statreload\nINFO:     Started server process [57611]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n</pre>\n\n<p>Notice that we only reload on changes to specific directories, as this is to avoid reloading on files that won't impact our application such as log files, etc.</p>\n<p>If we want to manage multiple uvicorn workers to enable parallelism in our application, we can use Gunicorn in conjunction with Uvicorn. This will usually be done in a production environment where we'll be dealing with meaningful traffic. We've included a <code>app/gunicorn.py</code> script with the customizable configuration and we can launch all the workers with the follow command:\n<pre><code>gunicorn -c config/gunicorn.py -k uvicorn.workers.UvicornWorker app.api:app\n</code></pre></p>\n<p>We'll add both of these commands to our <code>README.md</code> file as well:\n<pre><code>uvicorn app.api:app --host 0.0.0.0 --port 8000 --reload --reload-dir tagifai --reload-dir app  # dev\ngunicorn -c app/gunicorn.py -k uvicorn.workers.UvicornWorker app.api:app  # prod\n</code></pre></p>"},{"location":"courses/mlops/api/#requests","title":"Requests","text":"<p>Now that we have our application running, we can submit our <code>GET</code> request using several different methods:</p>\n<ul>\n<li>Visit the endpoint on a browser at http://localhost:8000/</li>\n<li>cURL\n<pre><code>curl -X GET http://localhost:8000/\n</code></pre></li>\n<li>Access endpoints via code. Here we show how to do it with the requests library in Python but it can be done with most popular languages. You can even use an online tool to convert your cURL commands into code!\n<pre><code>import json\nimport requests\n\nresponse = requests.get(\"http://localhost:8000/\")\nprint (json.loads(response.text))\n</code></pre></li>\n<li>Using external tools like Postman, which is great for managed tests that you can save and share with other, etc.</li>\n</ul>\n<p>For all of these, we'll see the exact same response from our API:</p>\n<pre>\n{\n  \"message\": \"OK\",\n  \"status-code\": 200,\n  \"data\": {}\n}\n</pre>"},{"location":"courses/mlops/api/#decorators","title":"Decorators","text":"<p>In our <code>GET \\</code> request's response above, there was not a whole lot of information about the actual request, but it's useful to have details such as URL, timestamp, etc. But we don't want to do this individually for each endpoint, so let's use decorators to automatically add relevant metadata to our responses</p>\n<pre><code># app/api.py\nfrom datetime import datetime\nfrom functools import wraps\nfrom fastapi import FastAPI, Request\n\ndef construct_response(f):\n\"\"\"Construct a JSON response for an endpoint.\"\"\"\n\n    @wraps(f)\ndef wrap(request: Request, *args, **kwargs) -&gt; Dict:\nresults = f(request, *args, **kwargs)\n        response = {\n            \"message\": results[\"message\"],\n            \"method\": request.method,\n            \"status-code\": results[\"status-code\"],\n            \"timestamp\": datetime.now().isoformat(),\n            \"url\": request.url._url,\n        }\n        if \"data\" in results:\n            response[\"data\"] = results[\"data\"]\n        return response\n\n    return wrap\n</code></pre>\n<p>We're passing in a Request instance in line 10 so we can access information like the request method and URL. Therefore, our endpoint functions also need to have this Request object as an input argument. Once we receive the results from our endpoint function <code>f</code>, we can append the extra details and return a more informative response. To use this decorator, we just have to wrap our functions accordingly.</p>\n<pre><code>@app.get(\"/\")\n@construct_response\ndef _index(request: Request) -&gt; Dict:\n\"\"\"Health check.\"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"data\": {},\n    }\n    return response\n</code></pre>\n<pre>\n{\n    message: \"OK\",\n    method: \"GET\",\n    status-code: 200,\n    timestamp: \"2021-02-08T13:19:11.343801\",\n    url: \"http://localhost:8000/\",\n    data: { }\n}\n</pre>\n\n<p>There are also some built-in decorators we should be aware of. We've already seen the path operation decorator (ex. <code>@app.get(\"/\")</code>) which defines the path for the endpoint as well as other attributes. There is also the events decorator (<code>@app.on_event()</code>) which we can use to startup and shutdown our application. For example, we use the (<code>@app.on_event(\"startup\")</code>) event to load the artifacts for the model to use for inference. The advantage of doing this as an event is that our service won't start until this is complete and so no requests will be prematurely processed and cause errors. Similarly, we can perform shutdown events with (<code>@app.on_event(\"shutdown\")</code>), such as saving logs, cleaning, etc.</p>\n<pre><code>from pathlib import Path\nfrom config import logger\nfrom tagifai import main\n\n@app.on_event(\"startup\")\ndef load_artifacts():\nglobal artifacts\n    run_id = open(Path(config.CONFIG_DIR, \"run_id.txt\")).read()\n    artifacts = main.load_artifacts(model_dir=config.MODEL_DIR)\n    logger.info(\"Ready for inference!\")\n</code></pre>"},{"location":"courses/mlops/api/#documentation","title":"Documentation","text":"<p>When we define an endpoint, FastAPI automatically generates some documentation (adhering to OpenAPI standards) based on the it's inputs, typing, outputs, etc. We can access the Swagger UI for our documentation by going to <code>/docs</code> endpoints on any browser while the api is running.</p>\n<p>Click on an endpoint &gt; <code>Try it out</code> &gt; <code>Execute</code> to see what the server's response will look like. Since this was a <code>GET</code> request without any inputs, our request body was empty but for other method's we'll need to provide some information (we'll illustrate this when we do a <code>POST</code> request).</p>\n<p>Notice that our endpoint is organized under sections in the UI. We can use <code>tags</code> when defining our endpoints in the script:\n<pre><code>@app.get(\"/\", tags=[\"General\"])\n@construct_response\ndef _index(request: Request) -&gt; Dict:\n\"\"\"Health check.\"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"data\": {},\n    }\n    return response\n</code></pre></p>\n<p>You can also use <code>/redoc</code> endpoint to view the ReDoc documentation or Postman to execute and manage tests that you can save and share with others.</p>"},{"location":"courses/mlops/api/#resources","title":"Resources","text":"<p>When designing the resources for our API , we need to think about the following questions:</p>\n<ul>\n<li>\n<p><code>[USERS]</code>: Who are the end users? This will define what resources need to be exposed.</p>\n<ul>\n<li>developers who want to interact with the API.</li>\n<li>product team who wants to test and inspect the model and it's performance.</li>\n<li>backend service that wants to classify incoming projects.</li>\n</ul>\n</li>\n<li>\n<p><code>[ACTIONS]</code>: What actions do our users want to be able to perform?</p>\n<ul>\n<li>prediction for a given set of inputs</li>\n<li>inspection of performance</li>\n<li>inspection of training arguments</li>\n</ul>\n</li>\n</ul>"},{"location":"courses/mlops/api/#query-parameters","title":"Query parameters","text":"<pre><code>@app.get(\"/performance\", tags=[\"Performance\"])\n@construct_response\ndef _performance(request: Request, filter: str = None) -&gt; Dict:\n\"\"\"Get the performance metrics.\"\"\"\n    performance = artifacts[\"performance\"]\n    data = {\"performance\":performance.get(filter, performance)}\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"data\": data,\n    }\n    return response\n</code></pre>\n<p>Notice that we're passing an optional query parameter <code>filter</code> here to indicate the subset of performance we care about. We can include this parameter in our <code>GET</code> request like so:</p>\n<pre><code>curl -X \"GET\" \\\n\"http://localhost:8000/performance?filter=overall\" \\\n-H \"accept: application/json\"\n</code></pre>\n<p>And this will only produce the subset of the performance we indicated through the query parameter:</p>\n<pre><code>{\n\"message\": \"OK\",\n\"method\": \"GET\",\n\"status-code\": 200,\n\"timestamp\": \"2021-03-21T13:12:01.297630\",\n\"url\": \"http://localhost:8000/performance?filter=overall\",\n\"data\": {\n\"performance\": {\n\"precision\": 0.8941372402587212,\n\"recall\": 0.8333333333333334,\n\"f1\": 0.8491658224308651,\n\"num_samples\": 144\n}\n}\n}\n</code></pre>"},{"location":"courses/mlops/api/#path-parameters","title":"Path parameters","text":"<p>Our next endpoint will be to <code>GET</code> the arguments used to train the model. This time, we're using a path parameter <code>args</code>, which is a required field in the URI.</p>\n<pre><code>@app.get(\"/args/{arg}\", tags=[\"Arguments\"])\n@construct_response\ndef _arg(request: Request, arg: str) -&gt; Dict:\n\"\"\"Get a specific parameter's value used for the run.\"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"data\": {\n            arg: vars(artifacts[\"args\"]).get(arg, \"\"),\n        },\n    }\n    return response\n</code></pre>\n<p>We can perform our <code>GET</code> request like so, where the <code>param</code> is part of the request URI's path as opposed to being part of it's query string.\n<pre><code>curl -X \"GET\" \\\n\"http://localhost:8000/args/learning_rate\" \\\n-H \"accept: application/json\"\n</code></pre></p>\n<p>And we'd receive a response like this:</p>\n<pre><code>{\n\"message\": \"OK\",\n\"method\": \"GET\",\n\"status-code\": 200,\n\"timestamp\": \"2021-03-21T13:13:46.696429\",\n\"url\": \"http://localhost:8000/params/hidden_dim\",\n\"data\": {\n\"learning_rate\": 0.14688087680118794\n}\n}\n</code></pre>\n<p>We can also create an endpoint to produce all the arguments that were used:</p>\nView <code>GET /args</code>\n<pre><code>@app.get(\"/args\", tags=[\"Arguments\"])\n@construct_response\ndef _args(request: Request) -&gt; Dict:\n\"\"\"Get all arguments used for the run.\"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"data\": {\n            \"args\": vars(artifacts[\"args\"]),\n        },\n    }\n    return response\n</code></pre>\n<p>We can perform our <code>GET</code> request like so, where the <code>param</code> is part of the request URI's path as opposed to being part of it's query string.</p>\n<pre><code>curl -X \"GET\" \\\n\"http://localhost:8000/args\" \\\n-H \"accept: application/json\"\n</code></pre>\n<p>And we'd receive a response like this:</p>\n<pre><code>{\n\"message\":\"OK\",\n\"method\":\"GET\",\n\"status-code\":200,\n\"timestamp\":\"2022-05-25T11:56:37.344762\",\n\"url\":\"http://localhost:8001/args\",\n\"data\":{\n\"args\":{\n\"shuffle\":true,\n\"subset\":null,\n\"min_freq\":75,\n\"lower\":true,\n\"stem\":false,\n\"analyzer\":\"char_wb\",\n\"ngram_max_range\":8,\n\"alpha\":0.0001,\n\"learning_rate\":0.14688087680118794,\n\"power_t\":0.158985493618746\n}\n}\n}\n</code></pre>"},{"location":"courses/mlops/api/#schemas","title":"Schemas","text":"<p>Now it's time to define our endpoint for prediction. We need to consume the inputs that we want to classify and so we need to define the schema that needs to be followed when defining those inputs.</p>\n<pre><code># app/schemas.py\nfrom typing import List\nfrom fastapi import Query\nfrom pydantic import BaseModel\n\nclass Text(BaseModel):\n    text: str = Query(None, min_length=1)\n\nclass PredictPayload(BaseModel):\n    texts: List[Text]\n</code></pre>\n<p>Here we're defining a <code>PredictPayload</code> object as a List of <code>Text</code> objects called <code>texts</code>. Each <code>Text</code> object is a string that defaults to <code>None</code> and must have a minimum length of 1 character.</p>\n<p>Note</p>\n<p>We could've just defined our <code>PredictPayload</code> like so:\n<pre><code>class PredictPayload(BaseModel):\n    texts: List[str] = Query(None, min_length=1)\n</code></pre>\nBut we wanted to create very explicit schemas in case we want to incorporate more validation or add additional parameters in the future.</p>\n<p>We can now use this payload in our predict endpoint:</p>\n<pre><code>from app.schemas import PredictPayload\nfrom tagifai import predict\n\n@app.post(\"/predict\", tags=[\"Prediction\"])\n@construct_response\ndef _predict(request: Request, payload: PredictPayload) -&gt; Dict:\n\"\"\"Predict tags for a list of texts.\"\"\"\n    texts = [item.text for item in payload.texts]\n    predictions = predict.predict(texts=texts, artifacts=artifacts)\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"data\": {\"predictions\": predictions},\n    }\n    return response\n</code></pre>\n<p>We need to adhere to the <code>PredictPayload</code> schema when we want to user our <code>/predict</code> endpoint:</p>\n<pre><code>curl -X 'POST' 'http://0.0.0.0:8000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"texts\": [\n        {\"text\": \"Transfer learning with transformers for text classification.\"},\n        {\"text\": \"Generative adversarial networks for image generation.\"}\n      ]\n    }'\n</code></pre>\n<pre>\n{\n  \"message\":\"OK\",\n  \"method\":\"POST\",\n  \"status-code\":200,\n  \"timestamp\":\"2022-05-25T12:23:34.381614\",\n  \"url\":\"http://0.0.0.0:8001/predict\",\n  \"data\":{\n    \"predictions\":[\n      {\n        \"input_text\":\"Transfer learning with transformers for text classification.\",\n        \"predicted_tag\":\"natural-language-processing\"\n      },\n      {\n        \"input_text\":\"Generative adversarial networks for image generation.\",\n        \"predicted_tag\":\"computer-vision\"\n      }\n    ]\n  }\n}\n</pre>"},{"location":"courses/mlops/api/#validation","title":"Validation","text":""},{"location":"courses/mlops/api/#built-in","title":"Built-in","text":"<p>We're using pydantic's <code>BaseModel</code> object here because it offers built-in validation for all of our schemas. In our case, if a <code>Text</code> instance is less than 1 character, then our service will return the appropriate error message and code:</p>\n<pre><code>curl -X POST \"http://localhost:8000/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"texts\\\":[{\\\"text\\\":\\\"\\\"}]}\"\n</code></pre>\n<pre>\n{\n  \"detail\": [\n    {\n      \"loc\": [\n        \"body\",\n        \"texts\",\n        0,\n        \"text\"\n      ],\n      \"msg\": \"ensure this value has at least 1 characters\",\n      \"type\": \"value_error.any_str.min_length\",\n      \"ctx\": {\n        \"limit_value\": 1\n      }\n    }\n  ]\n}\n</pre>"},{"location":"courses/mlops/api/#custom","title":"Custom","text":"<p>We can also add custom validation on a specific entity by using the <code>@validator</code> decorator, like we do to ensure that list of <code>texts</code> is not empty.</p>\n<pre><code>class PredictPayload(BaseModel):\n    texts: List[Text]\n\n@validator(\"texts\")\ndef list_must_not_be_empty(cls, value):\nif not len(value):\nraise ValueError(\"List of texts to classify cannot be empty.\")\nreturn value\n</code></pre>\n<pre><code>curl -X POST \"http://localhost:8000/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"texts\\\":[]}\"\n</code></pre>\n<pre>\n{\n  \"detail\":[\n    {\n      \"loc\":[\n        \"body\",\n        \"texts\"\n      ],\n      \"msg\": \"List of texts to classify cannot be empty.\",\n      \"type\": \"value_error\"\n    }\n  ]\n}\n</pre>"},{"location":"courses/mlops/api/#extras","title":"Extras","text":"<p>Lastly, we can add a <code>schema_extra</code> object under a <code>Config</code> class to depict what an example <code>PredictPayload</code> should look like. When we do this, it automatically appears in our endpoint's documentation (click <code>Try it out</code>).</p>\n<pre><code>class PredictPayload(BaseModel):\n    texts: List[Text]\n\n    @validator(\"texts\")\n    def list_must_not_be_empty(cls, value):\n        if not len(value):\n            raise ValueError(\"List of texts to classify cannot be empty.\")\n        return value\n\nclass Config:\nschema_extra = {\n\"example\": {\n\"texts\": [\n{\"text\": \"Transfer learning with transformers for text classification.\"},\n{\"text\": \"Generative adversarial networks in both PyTorch and TensorFlow.\"},\n]\n}\n}\n</code></pre>"},{"location":"courses/mlops/api/#product","title":"Product","text":"<p>To make our API a standalone product, we'll need to create and manage a database for our users and resources. These users will have credentials which they will use for authentication and use their privileges to be able to communicate with our service. And of course, we can display a rendered frontend to make all of this seamless with HTML forms, buttons, etc. This is exactly how the old MWML platform was built and we leveraged FastAPI to deliver high performance for 500K+ daily service requests.</p>\n<p>If you are building a product, then I highly recommending forking this generation template to get started. It includes the backbone architecture you need for your product:</p>\n<ul>\n<li>Databases (models, migrations, etc.)</li>\n<li>Authentication via JWT</li>\n<li>Asynchronous task queue with Celery</li>\n<li>Customizable frontend via Vue JS</li>\n<li>Docker integration</li>\n<li>so much more!</li>\n</ul>\n<p>However, for the majority of ML developers, thanks to the wide adoption of microservices, we don't need to do all of this. A well designed API service that can seamlessly communicate with all other services (framework agnostic) will fit into any process and add value to the overall product. Our main focus should be to ensure that our service is working as it should and constantly improve, which is exactly what the next cluster of lessons will focus on (testing and monitoring)</p>"},{"location":"courses/mlops/api/#model-server","title":"Model server","text":"<p>Besides wrapping our models as separate, scalable microservices, we can also have a purpose-built model server to host our models. Model servers provide a registry with an API layer to seamlessly inspect, update, serve, rollback, etc. multiple versions of models. They also offer automatic scaling to meet throughput and latency needs. Popular options include BentoML, MLFlow, TorchServe, RedisAI, Nvidia Triton Inference Server, etc.</p>\n<p>Model servers are experiencing a lot of adoption for their ability to standardize the model deployment and serving processes across the team -- enabling seamless upgrades, validation and integration.</p>\n<p>Upcoming live cohorts</p>\n<p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.\n\n</p>\n     Learn more\n<p></p>\n<p>To cite this content, please use:</p>\n<pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { APIs for Model Serving - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/augmentation/","title":"Data Augmentation","text":""},{"location":"courses/mlops/augmentation/#intuition","title":"Intuition","text":"<p>We'll often want to increase the size and diversity of our training data split through data augmentation. It involves using the existing samples to generate synthetic, yet realistic, examples.</p> <ol> <li> <p>Split the dataset. We want to split our dataset first because many augmentation techniques will cause a form of data leak if we allow the generated samples to be placed across different data splits.</p> <p>For example, some augmentation involves generating synonyms for certain key tokens in a sentence. If we allow the generated sentences from the same origin sentence to go into different splits, we could be potentially leaking samples with nearly identical embedding representations across our different splits.</p> </li> <li> <p>Augment the training split. We want to apply data augmentation on only the training set because our validation and testing splits should be used to provide an accurate estimate on actual data points.</p> </li> <li> <p>Inspect and validate.  It's useless to augment just for the same of increasing our training sample size if the augmented data samples are not probable inputs that our model could encounter in production.</p> </li> </ol> <p>The exact method of data augmentation depends largely on the type of data and the application. Here are a few ways different modalities of data can be augmented:</p> Data Augmentation with Snorkel <ul> <li>General: normalization, smoothing, random noise, synthetic oversampling (SMOTE), etc.</li> <li>Natural language processing (NLP): substitutions (synonyms, tfidf, embeddings, masked models), random noise, spelling errors, etc.</li> <li>Computer vision (CV): crop, flip, rotate, pad, saturate, increase brightness, etc.</li> </ul> <p>Warning</p> <p>While the transformations on some data modalities, such as images, are easy to inspect and validate, others may introduce silent errors. For example, shifting the order of tokens in text can significantly alter the meaning (\u201cthis is really cool\u201d \u2192 \u201cis this really cool\u201d). Therefore, it\u2019s important to measure the noise that our augmentation policies will introduce and do have granular control over the transformations that take place.</p>"},{"location":"courses/mlops/augmentation/#libraries","title":"Libraries","text":"<p>Depending on the feature types and tasks, there are many data augmentation libraries which allow us to extend our training data.</p>"},{"location":"courses/mlops/augmentation/#natural-language-processing-nlp","title":"Natural language processing (NLP)","text":"<ul> <li>NLPAug: data augmentation for NLP.</li> <li>TextAttack: a framework for adversarial attacks, data augmentation, and model training in NLP.</li> <li>TextAugment: text augmentation library.</li> </ul>"},{"location":"courses/mlops/augmentation/#computer-vision-cv","title":"Computer vision (CV)","text":"<ul> <li>Imgaug: image augmentation for machine learning experiments.</li> <li>Albumentations: fast image augmentation library.</li> <li>Augmentor: image augmentation library in Python for machine learning.</li> <li>Kornia.augmentation: a module to perform data augmentation in the GPU.</li> <li>SOLT: data augmentation library for Deep Learning, which supports images, segmentation masks, labels and key points.</li> </ul>"},{"location":"courses/mlops/augmentation/#other","title":"Other","text":"<ul> <li>Snorkel: system for generating training data with weak supervision.</li> <li>DeltaPy\u2060\u2060: tabular data augmentation and feature engineering.</li> <li>Audiomentations: a Python library for audio data augmentation.</li> <li>Tsaug: a Python package for time series augmentation.</li> </ul>"},{"location":"courses/mlops/augmentation/#implementation","title":"Implementation","text":"<p>Let's use the nlpaug library to augment our dataset and assess the quality of the generated samples.</p> <pre><code>pip install nlpaug==1.1.0 transformers==3.0.2 -q\npip install snorkel==0.9.8 -q\n</code></pre> <p><pre><code>import nlpaug.augmenter.word as naw\n</code></pre> <pre><code># Load tokenizers and transformers\nsubstitution = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\", action=\"substitute\")\ninsertion = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\", action=\"insert\")\ntext = \"Conditional image generation using Variational Autoencoders and GANs.\"\n</code></pre></p> <pre><code># Substitutions\nsubstitution.augment(text)\n</code></pre> <pre>\nhierarchical risk mapping using variational signals and gans.\n</pre> <p>Substitution doesn't seem like a great idea for us because there are certain keywords that provide strong signal for our tags so we don't want to alter those. Also, note that these augmentations are NOT deterministic and will vary every time we run them. Let's try insertion...</p> <pre><code># Insertions\ninsertion.augment(text)\n</code></pre> <pre>\nautomated conditional inverse image generation algorithms using multiple variational autoencoders and gans.\n</pre> <p>A little better but still quite fragile and now it can potentially insert key words that can influence false positive tags to appear. Maybe instead of substituting or inserting new tokens, let's try simply swapping machine learning related keywords with their aliases. We'll use Snorkel's transformation functions to easily achieve this.</p> <p><pre><code># Replace dashes from tags &amp; aliases\ndef replace_dash(x):\n    return x.replace(\"-\", \" \")\n</code></pre> <pre><code># Aliases\naliases_by_tag = {\n    \"computer-vision\": [\"cv\", \"vision\"],\n    \"mlops\": [\"production\"],\n    \"natural-language-processing\": [\"nlp\", \"nlproc\"]\n}\n</code></pre> <pre><code># Flatten dict\nflattened_aliases = {}\nfor tag, aliases in aliases_by_tag.items():\n    tag = replace_dash(x=tag)\n    if len(aliases):\n        flattened_aliases[tag] = aliases\n    for alias in aliases:\n        _aliases = aliases + [tag]\n        _aliases.remove(alias)\n        flattened_aliases[alias] = _aliases\n</code></pre> <pre><code>print (flattened_aliases[\"natural language processing\"])\nprint (flattened_aliases[\"nlp\"])\n</code></pre></p> <pre>\n['nlp', 'nlproc']\n['nlproc', 'natural language processing']\n</pre> <p>For now we'll use tags and aliases as they are in <code>aliases_by_tag</code> but we could account for plurality of tags using the inflect package or apply stemming before replacing aliases, etc.</p> <pre><code># We want to match with the whole word only\nprint (\"gan\" in \"This is a gan.\")\nprint (\"gan\" in \"This is gandalf.\")\n</code></pre> <pre><code># \\b matches spaces\ndef find_word(word, text):\n    word = word.replace(\"+\", \"\\+\")\n    pattern = re.compile(fr\"\\b({word})\\b\", flags=re.IGNORECASE)\n    return pattern.search(text)\n</code></pre> <pre><code># Correct behavior (single instance)\nprint (find_word(\"gan\", \"This is a gan.\"))\nprint (find_word(\"gan\", \"This is gandalf.\"))\n</code></pre> <pre>\n&lt;re.Match object; span=(10, 13), match='gan'&gt;\nNone\n</pre> <p>Now let's use snorkel's <code>transformation_function</code> to systematically apply this transformation to our data.</p> <pre><code>from snorkel.augmentation import transformation_function\n</code></pre> <pre><code>@transformation_function()\ndef swap_aliases(x):\n\"\"\"Swap ML keywords with their aliases.\"\"\"\n    # Find all matches\n    matches = []\n    for i, tag in enumerate(flattened_aliases):\n        match = find_word(tag, x.text)\n        if match:\n            matches.append(match)\n    # Swap a random match with a random alias\n    if len(matches):\n        match = random.choice(matches)\n        tag = x.text[match.start():match.end()]\n        x.text = f\"{x.text[:match.start()]}{random.choice(flattened_aliases[tag])}{x.text[match.end():]}\"\n    return x\n</code></pre> <pre><code># Swap\nfor i in range(3):\n    sample_df = pd.DataFrame([{\"text\": \"a survey of reinforcement learning for nlp tasks.\"}])\n    sample_df.text = sample_df.text.apply(clean_text, lower=True, stem=False)\n    print (swap_aliases(sample_df.iloc[0]).text)\n</code></pre> <pre><code># Undesired behavior (needs contextual insight)\nfor i in range(3):\n    sample_df = pd.DataFrame([{\"text\": \"Autogenerate your CV to apply for jobs using NLP.\"}])\n    sample_df.text = sample_df.text.apply(clean_text, lower=True, stem=False)\n    print (swap_aliases(sample_df.iloc[0]).text)\n</code></pre> <pre>\nautogenerate vision apply jobs using nlp\nautogenerate cv apply jobs using natural language processing\nautogenerate cv apply jobs using nlproc\n</pre> <p>Now we'll define a augmentation policy to apply our transformation functions with certain rules (how many samples to generate, whether to keep the original data point, etc.)</p> <pre><code>from snorkel.augmentation import ApplyOnePolicy, PandasTFApplier\n</code></pre> <pre><code># Transformation function (TF) policy\npolicy = ApplyOnePolicy(n_per_original=5, keep_original=True)\ntf_applier = PandasTFApplier([swap_aliases], policy)\ntrain_df_augmented = tf_applier.apply(train_df)\ntrain_df_augmented.drop_duplicates(subset=[\"text\"], inplace=True)\ntrain_df_augmented.head()\n</code></pre> text tags 0 laplacian pyramid reconstruction refinement se... computer-vision 1 extract stock sentiment news headlines project... natural-language-processing 2 big bad nlp database collection 400 nlp datasets... natural-language-processing 2 big bad natural language processing database c... natural-language-processing 2 big bad nlproc database collection 400 nlp dat... natural-language-processing <pre><code>len(train_df), len(train_df_augmented)\n</code></pre> <pre>\n(668, 913)\n</pre> <p>For now, we'll skip the data augmentation because it's quite fickle and empirically it doesn't improvement performance much. But we can see how this can be very effective once we can control what type of vocabulary to augment on and what exactly to augment with.</p> <p>Warning</p> <p>Regardless of what method we use, it's important to validate that we're not just augmenting for the sake of augmentation. We can do this by executing any existing data validation tests and even creating specific tests to apply on augmented data.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Data Augmentation - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/cicd/","title":"CI/CD for Machine Learning","text":""},{"location":"courses/mlops/cicd/#intuition","title":"Intuition","text":"<p>In the previous lesson, we learned how to manually execute our ML workloads with Jobs and Services. However, we want to be able to automatically execute these workloads when certain events occur (new data, performance regressions, elapsed time, etc.) to ensure that our models are always up to date and increasing in quality. In this lesson, we'll learn how to create continuous integration and delivery (CI/CD) pipelines to achieve an application that is capable of continual learning.</p>"},{"location":"courses/mlops/cicd/#github-actions","title":"GitHub Actions","text":"<p>We're going to use GitHub Actions to create our CI/CD pipelines. GitHub Actions allow us to define workflows that are triggered by events (pull request, push, etc.) and execute a series of actions.</p> <p>Our GitHub Actions are defined under our repository's <code>.github/workflows</code> directory where we have workflows for documentation (<code>documentation.yaml</code>), workloads (<code>workloads.yaml</code>) to train/validate a model and a final workflow for serving our model (<code>serve.yaml</code>). Let's start by understanding the structure of a workflow.</p>"},{"location":"courses/mlops/cicd/#events","title":"Events","text":"<p>Workflows are triggered by an event, which can be something that occurs on an event (like a push or pull request), schedule (cron), manually and many more. In our application, our <code>workloads</code> workflow is triggered on a pull request to the main branch and then our <code>serve</code> workflow and <code>documentation</code> workflows are triggered on a push to the main branch.</p> <pre><code># .github/workflows/workloads.yaml\nname: workloads\non:\nworkflow_dispatch:  # manual\npull_request:\nbranches:\n- main\n...\n</code></pre> <p>This creates for the following ideal workflow:</p> <ol> <li>We make changes to our code and submit a pull request to the <code>main</code> branch.</li> <li>Our <code>workloads</code> workflow is triggered and executes our model development workloads.</li> <li>If the performance of the new model is better, we can merge the pull request and push the changes to the <code>main</code> branch.</li> <li>Our <code>serve</code> workflow is triggered and deploys our application to production (along with an update to our <code>documentation</code>).</li> </ol>"},{"location":"courses/mlops/cicd/#jobs","title":"Jobs","text":"<p>Once the event is triggered, a set of <code>jobs</code> run on a <code>runner</code> (GitHub's infrastructure or self-hosted).</p> <pre><code># .github/workflows/workloads.yaml\n...\njobs:\nworkloads:\nruns-on: ubuntu-22.04\n...\n</code></pre> <p>Tip</p> <p>Each of our workflows only have one job but if we had multiple, the jobs would all run in parallel. If we wanted to create dependent jobs, where if a particular job fails all it's dependent jobs will be skipped, then we'd use the needs keyword.</p>"},{"location":"courses/mlops/cicd/#steps","title":"Steps","text":"<p>Each job contains a series of steps which are executed in order. Each step has a name, as well as actions to use from the GitHub Action marketplace and/or commands we want to run. For example, here's a look at one of the steps in our <code>workloads</code> job inside our <code>workloads.yaml</code> workflow:</p> <pre><code># .github/workflows/testing.yml\njobs:\nworkloads:\nruns-on: ubuntu-22.04\nsteps:\n...\n# Run workloads\n- name: Workloads\nrun: |\nexport ANYSCALE_HOST=$\\{\\{ secrets.ANYSCALE_HOST \\}\\}\nexport ANYSCALE_CLI_TOKEN=$\\{\\{ secrets.ANYSCALE_CLI_TOKEN \\}\\}\nanyscale jobs submit deploy/jobs/workloads.yaml --wait\n...\n</code></pre>"},{"location":"courses/mlops/cicd/#workflows","title":"Workflows","text":"<p>Now that we understand the basic components of a GitHub Actions workflow, let's take a closer look at each of our workflows. Most of our workflows will require access to our Anyscale credentials so we'll start by setting those up. We can set these secrets for our repository under the Settings tab.</p> <p>And our first workflow will be our <code>workloads</code> workflow which will be triggered on a pull request to the <code>main</code> branch. This means that we'll need to push our local code changes to Git and then submit a pull request to the <code>main</code> branch. But in order to push our code to GitHub, we'll need to first authenticate with our credentials before pushing to our repository:</p> <pre><code>git config --global user.name $GITHUB_USERNAME\ngit config --global user.email you@example.com  # &lt;-- CHANGE THIS to your email\ngit add .\ngit commit -m \"\"  # &lt;-- CHANGE THIS to your message\ngit push origin dev\n</code></pre> <p>Now you will be prompted to enter your username and password (personal access token). Follow these steps to get personal access token: New GitHub personal access token \u2192 Add a name \u2192 Toggle <code>repo</code> and <code>workflow</code> \u2192 Click <code>Generate token</code> (scroll down) \u2192 Copy the token and paste it when prompted for your password.</p> <p>Note that we should be on a <code>dev</code> branch, which we set up in our setup lesson. If you're not, go ahead and run <code>git checkout -b dev</code> first.</p> <p>And when any of our GitHub Actions workflows execute, we will be able to view them under the <code>Actions</code> tab of our repository. Here we'll find all the workflows that have been executed and we can inspect each one to see the details of the execution.</p>"},{"location":"courses/mlops/cicd/#workloads","title":"Workloads","text":"<p>Our <code>workloads</code> workflow is triggered on a pull request to the <code>main</code> branch. It contains a single job that runs our model development workloads with an Anyscale Job. The steps in this job are as follows:</p> <ol> <li>We start by configuring our AWS credentials so that we can push/pull from our S3 buckets. Recall that we store our model registry and results in S3 buckets so we need to be able to access them. We created an IAM role for this course so that only certain repositories can access our S3 buckets. <pre><code># Configure AWS credentials\n- name: Configure AWS credentials\nuses: aws-actions/configure-aws-credentials@v2\nwith:\nrole-to-assume: arn:aws:iam::593241322649:role/github-actions-madewithml\nrole-session-name: s3access\naws-region: us-west-2\n</code></pre></li> <li>Next, we checkout our repository code and install our Python dependencies so that we can execute our Anyscale Job. <pre><code># Set up dependencies\n- uses: actions/checkout@v3\n- uses: actions/setup-python@v4\nwith:\npython-version: '3.10.11'\ncache: 'pip'\n- run: python3 -m pip install anyscale==0.5.128 typer==0.9.0\n</code></pre></li> <li>Next, we can run our Anyscale Job but note that since this will be running on a GitHub hosted runner, we need to export our Anyscale credentials first (which we already set up earlier on our repository). <pre><code># Run workloads\n- name: Workloads\nrun: |\nexport ANYSCALE_HOST=$\\{\\{ secrets.ANYSCALE_HOST \\}\\}\nexport ANYSCALE_CLI_TOKEN=$\\{\\{ secrets.ANYSCALE_CLI_TOKEN \\}\\}\nanyscale jobs submit deploy/jobs/workloads.yaml --wait\n</code></pre></li> <li>Recall that our Anyscale Job in the previous step saves our model registry and results to S3 buckets. So in this step, we'll read the artifacts from S3 (from our unique path using our GitHub username) and save them locally on our GitHub runner. We have a small utility script called <code>.github/workflows/json_to_md.py</code> to convert our JSON results to markdown tables that we can comment on our PR. <pre><code># Read results from S3\n- name: Read results from S3\nrun: |\nmkdir results\naws s3 cp s3://madewithml/$\\{\\{ github.actor \\}\\}/results/ results/ --recursive\npython .github/workflows/json_to_md.py results/training_results.json results/training_results.md\npython .github/workflows/json_to_md.py results/evaluation_results.json results/evaluation_results.md\n</code></pre></li> <li>We use a GitHub Action from the marketplace to comment our results markdown tables on our PR. <pre><code># Comment results to PR\n- name: Comment training results on PR\nuses: thollander/actions-comment-pull-request@v2\nwith:\nfilePath: results/training_results.md\n- name: Comment evaluation results on PR\nuses: thollander/actions-comment-pull-request@v2\nwith:\nfilePath: results/evaluation_results.md\n</code></pre></li> </ol> <p>So when this <code>workloads</code> workflow completes, we'll have a comment on our PR (example) with our training and evaluation results. We can now collaboratively analyze the details and decide if we want to merge the PR.</p> <p>Tip</p> <p>We could easily extend this by retrieving evaluation results from our currently deployed model in production as well. Recall that we defined a <code>/evaluate/</code> endpoint for our service that expects a dataset location and returns the evaluation results. And we can submit this request as a step in our workflow and save the results to a markdown table that we can comment on our PR.</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $SECRET_TOKEN\" -d '{\n  \"dataset\": \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\n}' $SERVICE_ENDPOINT/evaluate/\n</code></pre> <pre><code>{\n\"results\": {\n\"timestamp\": \"July 24, 2023 11:43:37 PM\",\n\"run_id\": \"f1684a944d314bacabeaa90ff972775b\",\n\"overall\": {\n\"precision\": 0.9536309870079502,\n\"recall\": 0.9528795811518325,\n\"f1\": 0.9525489716579315,\n\"num_samples\": 191\n},\n}\n}\n</code></pre>"},{"location":"courses/mlops/cicd/#serve","title":"Serve","text":"<p>If we like the results and we want to merge the PR and push to the <code>main</code> branch, our <code>serve</code> workflow will be triggered.</p> <pre><code># .github/workflows/serve.yaml\nname: serve\non:\nworkflow_dispatch:  # manual\npush:\nbranches:\n- main\n...\n</code></pre> <p>It contains a single job that serves our model with Anyscale Services. The steps in this job are as follows:</p> <ol> <li>We start by configuring our AWS credentials so that we can push/pull from our S3 buckets. Recall that we store our model registry and results in S3 buckets so we need to be able to access them. <pre><code># Configure AWS credentials\n- name: Configure AWS credentials\nuses: aws-actions/configure-aws-credentials@v2\nwith:\nrole-to-assume: arn:aws:iam::593241322649:role/github-actions-madewithml\nrole-session-name: s3access\naws-region: us-west-2\n</code></pre></li> <li>Next, we checkout our repository code and install our Python dependencies so that we can execute our Anyscale Job. <pre><code># Set up dependencies\n- uses: actions/checkout@v3\n- uses: actions/setup-python@v4\nwith:\npython-version: '3.10.11'\ncache: 'pip'\n- run: python3 -m pip install anyscale==0.5.128 typer==0.9.0\n</code></pre></li> <li>Next, we can run our Anyscale Service but note that since this will be running on a GitHub hosted runner, we need to export our Anyscale credentials first (which we already set up earlier on our repository). <pre><code># Run workloads\n- name: Workloads\nrun: |\nexport ANYSCALE_HOST=$\\{\\{ secrets.ANYSCALE_HOST \\}\\}\nexport ANYSCALE_CLI_TOKEN=$\\{\\{ secrets.ANYSCALE_CLI_TOKEN \\}\\}\nanyscale service rollout --service-config-file deploy/services/serve_model.yaml\n</code></pre></li> </ol> <p>So when this <code>serve</code> workflow completes, our model will be deployed to production and we can start making inference requests with it.</p> <p>Note</p> <p>The <code>anyscale service rollout</code> command will update our existing service (if there was already one running) without changing the <code>SECRET_TOKEN</code> or <code>SERVICE_ENDPOINT</code>. So this means that our downstream applications that were making inference requests to our service can continue to do so without any changes.</p>"},{"location":"courses/mlops/cicd/#documentation","title":"Documentation","text":"<p>Our <code>documentation</code> workflow is also triggered on a push to the <code>main</code> branch. It contains a single job that builds our docs. The steps in this job are as follows:</p> <ol> <li>We checkout our repository code and install our Python dependencies so that we can build our documentation. <pre><code># Set up dependencies\n- uses: actions/checkout@v3\n- uses: actions/setup-python@v4\nwith:\npython-version: '3.10.11'\ncache: 'pip'\n- run: python3 -m pip install mkdocs==1.4.2 mkdocstrings==0.21.2 \"mkdocstrings[python]&gt;=0.18\"\n</code></pre></li> <li>And finally, we deploy our documentation. <pre><code># Deploy docs\n- name: Deploy documentation\nrun: mkdocs gh-deploy --force\n</code></pre></li> </ol>"},{"location":"courses/mlops/cicd/#continual-learning","title":"Continual learning","text":"<p>And with that, we're able to automatically update our ML application when ever we make changes to the code and want to trigger a new deployment. We have fully control because we can decide not to trigger an event (ex. push to <code>main</code> branch) if we're not satisfied with the results of our model development workloads. We can easily extend this to include other events (ex. new data, performance regressions, etc.) to trigger our workflows, as well as, integrate with more functionality around orchestration (ex. Prefect, Kubeflow, etc.), monitoring, etc.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { CI/CD workflows - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/cli/","title":"Command-Line Interface (CLI)","text":""},{"location":"courses/mlops/cli/#intuition","title":"Intuition","text":"<p>In the previous lesson, we organized our code from our notebook into individual Python scripts. We moved our functions and classes into their respective scripts and also created new workload functions to execute the main ML workloads (ex. <code>train_model</code> function from <code>madewithml/train.py</code> script). We now want to enable users to execute these workloads from the terminal without having to know anything about our code itself.</p>"},{"location":"courses/mlops/cli/#methods","title":"Methods","text":"<p>One way to execute these workloads is to import the functions in the Python script and execute them one at a time:</p> <pre><code>from madewithml import train\ntrain.train_model(experiment_name=\"llm\", ...)\n</code></pre> <p>Caution: Don't forget to run <code>export PYTHONPATH=$PYTHONPATH:$PWD</code> in your terminal to ensure that Python can find the modules in our project.</p> <p>While this may seem simple, it still requires us to import packages, identify the input arguments, etc. Therefore, another alternative is to place the main function call under a <code>if __name__ == \"__main__\"</code> conditional so that it's only executed when we run the script directly. Here we can pass in the input arguments directly into the function in the code.</p> <p><pre><code># madewithml/train.py\nif __name__ == \"__main__\":\n    train_model(experiment_name=\"llm\", ...)\n</code></pre> Which we can call from the terminal like so: <pre><code>python madewithml/train.py\n</code></pre></p> <p>However, the limitation here is that we can't choose which function from a particular script to execute. We have to set the one we want to execute under the <code>if __name__ == \"__main__\"</code> conditional. It's also very rigid since we have to set the input argument values in the code, unless we use a library like argparse.</p> <p><pre><code># madewithml/serve.py\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--run_id\", help=\"run ID to use for serving.\")\n    parser.add_argument(\"--threshold\", type=float, default=0.9, help=\"threshold for `other` class.\")\n    args = parser.parse_args()\n    ray.init()\n    serve.run(ModelDeployment.bind(run_id=args.run_id, threshold=args.threshold))\n</code></pre> Which we can call from the terminal like so (note that <code>--threshold</code> is optional since it has a default value): <pre><code>python madewithml/serve.py --run_id $RUN_ID\n</code></pre></p> <p>We use argparse in our <code>madewithml/serve.py</code> script because it's the only workload in the script and it's a one-line function call (<code>serve.run()</code>).</p> <p>Compared to using functions or the <code>__main__</code> conditional, a much better user experience would be to execute these workloads from the terminal. In this lesson, we'll learn how to build a command-line interface (CLI) so that execute our main ML workloads.</p>"},{"location":"courses/mlops/cli/#typer","title":"Typer","text":"<p>We're going to create our CLI using Typer. It's as simple as initializing the app and then adding the appropriate decorator to each function operation we wish to use as a CLI command in our script:</p> <pre><code>import typer\nfrom typing_extensions import Annotated\napp = typer.Typer()\n\n@app.command()\ndef train_model(\n    experiment_name: Annotated[str, typer.Option(help=\"name of the experiment.\")] = None,\n    ...):\n    pass\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"},{"location":"courses/mlops/cli/#inputs","title":"Inputs","text":"<p>You may notice that our function inputs have a lot of information besides just the input name. We'll cover typing (<code>str</code>, <code>List</code>, etc.) in our documentation lesson but for now, just know that <code>Annotated</code> allows us to specify metadata about the input argument's type and details about the (required) option (<code>typer.Option</code>).</p> <p>We make all of our input arguments optional so that we can explicitly define them in our CLI commands (ex. <code>--experiment-name</code>).</p> <p>We can also add some helpful information about the input parameter (with <code>typer.Option(help=\"...\")</code>) and a default value (ex. <code>None</code>).</p>"},{"location":"courses/mlops/cli/#usage","title":"Usage","text":"<p>With our CLI commands defined and our input arguments enriched, we can execute our workloads. Let's start by executing our <code>train_model</code> function by assuming that we don't know what the required input parameters are. Instead of having to look in the code, we can just do the following:</p> <pre><code>python madewithml/train.py --help\n</code></pre> <pre>\nUsage: train.py [OPTIONS]\nMain train function to train our model as a distributed workload.\n</pre> <p>We can follow this helpful message to execute our workload with the appropriate inputs.</p> <pre><code>export EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\npython madewithml/train.py \\\n--experiment-name \"$EXPERIMENT_NAME\" \\\n--dataset-loc \"$DATASET_LOC\" \\\n--train-loop-config \"$TRAIN_LOOP_CONFIG\" \\\n--num-workers 1 \\\n--cpu-per-worker 10 \\\n--gpu-per-worker 1 \\\n--num-epochs 10 \\\n--batch-size 256 \\\n--results-fp results/training_results.json\n</code></pre> <p>Be sure to check out our <code>README.md</code> file as it has examples of all the CLI commands for our ML workloads (train, tune, evaluate, inference and serve).</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { CLI - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/dashboard/","title":"Dashboard","text":""},{"location":"courses/mlops/dashboard/#intuition","title":"Intuition","text":"<p>When developing an application, there are a lot of technical decisions and results (preprocessing, performance, etc.) that are integral to our system. How can we effectively communicate this to other developers and business stakeholders? One option is a Jupyter notebook but it's often cluttered with code and isn't very easy for non-technical team members to access and run. We need to create a dashboard that can be accessed without any technical prerequisites and effectively communicates key findings. It would be even more useful if our dashboard was interactive such that it provides utility even for the technical developers.</p>"},{"location":"courses/mlops/dashboard/#streamlit","title":"Streamlit","text":"<p>There are some great tooling options, such as Dash, Gradio, Streamlit, Tableau, Looker, etc. for creating dashboards to deliver data oriented insights. Traditionally, interactive dashboards were exclusively created using front-end programming languages such as HTML Javascript, CSS, etc. However, given that many developers working in machine learning are using Python, the tooling landscape has evolved to bridge this gap. These tools now allow ML developers to create interactive dashboards and visualizations in Python while offering full customization via HTML, JS, and CSS. We'll be using Streamlit to create our dashboards because of it's intuitive API, sharing capabilities and increasing community adoption.</p>"},{"location":"courses/mlops/dashboard/#set-up","title":"Set up","text":"<p>With Streamlit, we can quickly create an empty application and as we develop, the UI will update as well. <pre><code># Setup\npip install streamlit==1.10.0\nmkdir streamlit\ntouch streamlit/app.py\nstreamlit run streamlit/app.py\n</code></pre></p> <pre>\nYou can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://10.0.1.93:8501\n</pre> <p>This will automatically open up the streamlit dashboard for us on http://localhost:8501.</p> <p>Be sure to add this package and version to our <code>requirements.txt</code> file.</p>"},{"location":"courses/mlops/dashboard/#api-reference","title":"API Reference","text":"<p>Before we create a dashboard for our specific application, we need to learn about the different Streamlit components. Instead of going through them all in this lesson, take a few minutes and go through the API reference. It's quite short and we promise you'll be amazed at how many UI components (styled text, latex, tables, plots, etc.) you can create using just Python. We'll explore the different components in detail as they apply to creating different interactions for our specific dashboard below.</p>"},{"location":"courses/mlops/dashboard/#sections","title":"Sections","text":"<p>We'll start by outlining the sections we want to have in our dashboard by editing our <code>streamlit/app.py</code> script:</p> <pre><code>import pandas as pd\nfrom pathlib import Path\nimport streamlit as st\n\nfrom config import config\nfrom tagifai import main, utils\n</code></pre> <pre><code># Title\nst.title(\"MLOps Course \u00b7 Made With ML\")\n\n# Sections\nst.header(\"\ud83d\udd22 Data\")\nst.header(\"\ud83d\udcca Performance\")\nst.header(\"\ud83d\ude80 Inference\")\n</code></pre> <p>To see these changes on our dashboard, we can refresh our dashboard page (press <code>R</code>) or set it <code>Always rerun</code> (press <code>A</code>).</p>"},{"location":"courses/mlops/dashboard/#data","title":"Data","text":"<p>We're going to keep our dashboard simple, so we'll just display the labeled projects.</p> <pre><code>st.header(\"Data\")\nprojects_fp = Path(config.DATA_DIR, \"labeled_projects.csv\")\ndf = pd.read_csv(projects_fp)\nst.text(f\"Projects (count: {len(df)})\")\nst.write(df)\n</code></pre>"},{"location":"courses/mlops/dashboard/#performance","title":"Performance","text":"<p>In this section, we'll display the performance of from our latest trained model. Again, we're going to keep it simple but we could also overlay more information such as improvements or regressions from previous deployments by accessing the model store.</p> <pre><code>st.header(\"\ud83d\udcca Performance\")\nperformance_fp = Path(config.CONFIG_DIR, \"performance.json\")\nperformance = utils.load_dict(filepath=performance_fp)\nst.text(\"Overall:\")\nst.write(performance[\"overall\"])\ntag = st.selectbox(\"Choose a tag: \", list(performance[\"class\"].keys()))\nst.write(performance[\"class\"][tag])\ntag = st.selectbox(\"Choose a slice: \", list(performance[\"slices\"].keys()))\nst.write(performance[\"slices\"][tag])\n</code></pre>"},{"location":"courses/mlops/dashboard/#inference","title":"Inference","text":"<p>With the inference section, we want to be able to quickly predict with the latest trained model.</p> <pre><code>st.header(\"\ud83d\ude80 Inference\")\ntext = st.text_input(\"Enter text:\", \"Transfer learning with transformers for text classification.\")\nrun_id = st.text_input(\"Enter run ID:\", open(Path(config.CONFIG_DIR, \"run_id.txt\")).read())\nprediction = main.predict_tag(text=text, run_id=run_id)\nst.write(prediction)\n</code></pre> <p>Tip</p> <p>Our dashboard is quite simple but we can also more comprehensive dashboards that reflect some of the core topics we covered in our machine learning canvas.</p> <ul> <li>Display findings from our labeling, EDA and preprocessing stages of development.</li> <li>View false +/- interactively and connect with annotation pipelines so that changes to the data can be reviewed and incorporated.</li> <li>Compare performances across multiple releases to visualize improvements/regressions over time (using model store, git tags, etc.)</li> </ul>"},{"location":"courses/mlops/dashboard/#caching","title":"Caching","text":"<p>Sometimes we may have views that involve computationally heavy operations, such as loading data or model artifacts. It's best practice to cache these operations by wrapping them as a separate function with the <code>@st.cache</code> decorator. This calls for Streamlit to cache the function by the specific combination of it's inputs to deliver the respective outputs when the function is invoked with the same inputs.</p> <pre><code>@st.cache()\ndef load_data():\n    projects_fp = Path(config.DATA_DIR, \"labeled_projects.csv\")\n    df = pd.read_csv(projects_fp)\n    return df\n</code></pre>"},{"location":"courses/mlops/dashboard/#deploy","title":"Deploy","text":"<p>We have several different options for deploying and managing our Streamlit dashboard. We could use Streamlit's sharing feature (beta) which allows us to seamlessly deploy dashboards straight from GitHub. Our dashboard will continue to stay updated as we commit changes to our repository. Another option is to deploy the Streamlit dashboard along with our API service. We can use docker-compose to spin up a separate container or simply add it to the API service's Dockerfile's ENTRYPOINT with the appropriate ports exposed. The later might be ideal, especially if your dashboard isn't meant to be public and it you want added security, performance, etc.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Dashboard - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/data-engineering/","title":"Data Engineering for Machine Learning","text":""},{"location":"courses/mlops/data-engineering/#intuition","title":"Intuition","text":"<p>So far we've had the convenience of using local CSV files as data source but in reality, our data can come from many disparate sources. Additionally, our processes around transforming and testing our data should ideally be moved upstream so that many different downstream processes can benefit from them. Our ML use case being just one among the many potential downstream applications. To address these shortcomings, we're going to learn about the fundamentals of data engineering and construct a modern data stack that can scale and provide high quality data for our applications.</p> <p>View the  data-engineering repository for all the code.</p> <p>At a high level, we're going to:</p> <ol> <li>Extract and Load data from sources to destinations.</li> <li>Transform data for downstream applications.</li> </ol> <p>This process is more commonly known as ELT, but there are variants such as ETL and reverse ETL, etc. They are all essentially the same underlying workflows but have slight differences in the order of data flow and where data is processed and stored.</p> <p>Utility and simplicity</p> <p>It can be enticing to set up a modern data stack in your organization, especially with all the hype. But it's very important to motivate utility and adding additional complexity:</p> <ul> <li>Start with a use case that we already have data sources for and has direct impact on the business' bottom line (ex. user churn).</li> <li>Start with the simplest infrastructure (source \u2192 database \u2192 report) and add complexity (in infrastructure, performance and team) as needed.</li> </ul>"},{"location":"courses/mlops/data-engineering/#data-systems","title":"Data systems","text":"<p>Before we start working with our data, it's important to understand the different types of systems that our data can live in. So far in this course we've worked with files, but there are several types of data systems that are widely adopted in industry for different purposes.</p>"},{"location":"courses/mlops/data-engineering/#data-lake","title":"Data lake","text":"<p>A data lake is a flat data management system that stores raw objects. It's a great option for inexpensive storage and has the capability to hold all types of data (unstructured, semi-structured and structured). Object stores are becoming the standard for data lakes with default options across the popular cloud providers. Unfortunately, because data is stored as objects in a data lake, it's not designed for operating on structured data.</p> <p>Popular data lake options include Amazon S3, Azure Blob Storage, Google Cloud Storage, etc.</p>"},{"location":"courses/mlops/data-engineering/#database","title":"Database","text":"<p>Another popular storage option is a database (DB), which is an organized collection of structured data that adheres to either:</p> <ul> <li>relational schema (tables with rows and columns) often referred to as a Relational Database Management System (RDBMS) or SQL database.</li> <li>non-relational (key/value, graph, etc.), often referred to as a non-relational database or NoSQL database.</li> </ul> <p>A database is an online transaction processing (OLTP) system because it's typically used for day-to-day CRUD (create, read, update, delete) operations where typically information is accessed by rows. However, they're generally used to store data from one application and is not designed to hold data from across many sources for the purpose of analytics.</p> <p>Popular database options include PostgreSQL, MySQL, MongoDB, Cassandra, etc.</p>"},{"location":"courses/mlops/data-engineering/#data-warehouse","title":"Data warehouse","text":"<p>A data warehouse (DWH) is a type of database that's designed for storing structured data from many different sources for downstream analytics and data science. It's an online analytical processing (OLAP) system that's optimized for performing operations across aggregating column values rather than accessing specific rows.</p> <p>Popular data warehouse options include SnowFlake, Google BigQuery, Amazon RedShift, Hive, etc.</p>"},{"location":"courses/mlops/data-engineering/#extract-and-load","title":"Extract and load","text":"<p>The first step in our data pipeline is to extract data from a source and load it into the appropriate destination. While we could construct custom scripts to do this manually or on a schedule, an ecosystem of data ingestion tools have already standardized the entire process. They all come equipped with connectors that allow for extraction, normalization, cleaning and loading between sources and destinations. And these pipelines can be scaled, monitored, etc. all with very little to no code.</p> <p>Popular data ingestion tools include Fivetran, Airbyte, Stitch, etc.</p> <p>We're going to use the open-source tool Airbyte to create connections between our data sources and destinations. Let's set up Airbyte and define our data sources. As we progress in this lesson, we'll set up our destinations and create connections to extract and load data.</p> <ol> <li>Ensure that we have Docker installed, but if not, download it here. For Windows users, be sure to have these configurations enabled.</li> <li>In a parent directory, outside our project directory for the MLOps course, execute the following commands to load the Airbyte repository locally and launch the service. <pre><code>git clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker-compose up\n</code></pre></li> <li>After a few minutes, visit http://localhost:8000/ to view the launched Airbyte service.</li> </ol>"},{"location":"courses/mlops/data-engineering/#sources","title":"Sources","text":"<p>Our data sources we want to extract from can be from anywhere. They could come from 3rd party apps, files, user click streams, physical devices, data lakes, databases, data warehouses, etc. But regardless of the source of our data, they type of data should fit into one of these categories:</p> <ul> <li><code>structured</code>: organized data stored in an explicit structure (ex. tables)</li> <li><code>semi-structured</code>: data with some structure but no formal schema or data types (web pages, CSV, JSON, etc.)</li> <li><code>unstructured</code>: qualitative data with no formal structure (text, images, audio, etc.)</li> </ul> <p>For our application, we'll define two data sources:</p> <ul> <li>projects.csv: data containing projects with their ID, create date, title and description.</li> <li>tags.csv: labels for each of project IDs in projects.csv</li> </ul> <p>Ideally, these data assets would be retrieved from a database that contains projects that we extracted and perhaps another database that stores labels from our labeling team's workflows. However, for simplicity we'll use CSV files to demonstrate how to define a data source.</p>"},{"location":"courses/mlops/data-engineering/#define-file-source-in-airbyte","title":"Define file source in Airbyte","text":"<p>We'll start our ELT process by defining the data source in Airbyte:</p> <ol> <li>On our Airbyte UI, click on <code>Sources</code> on the left menu. Then click the <code>+ New source</code> button on the top right corner.</li> <li>Click on the <code>Source type</code> dropdown and choose <code>File</code>. This will open a view to define our file data source. <pre><code>Name: Projects\nURL: https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\nFile Format: csv\nStorage Provider: HTTPS: Public Web\nDataset Name: projects\n</code></pre></li> <li>Click the <code>Set up source</code> button and our data source will be tested and saved.</li> <li>Repeat steps 1-3 for our tags data source as well: <pre><code>Name: Tags\nURL: https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\nFile Format: csv\nStorage Provider: HTTPS: Public Web\nDataset Name: tags\n</code></pre></li> </ol>"},{"location":"courses/mlops/data-engineering/#destinations","title":"Destinations","text":"<p>Once we know the source we want to extract data from, we need to decide the destination to load it. The choice depends on what our downstream applications want to be able to do with the data. And it's also common to store data in one location (ex. data lake) and move it somewhere else (ex. data warehouse) for specific processing.</p>"},{"location":"courses/mlops/data-engineering/#set-up-google-bigquery","title":"Set up Google BigQuery","text":"<p>Our destination will be a data warehouse since we'll want to use the data for downstream analytical and machine learning applications. We're going to use Google BigQuery which is free under Google Cloud's free tier for up to 10 GB storage and 1TB of queries (which is significantly more than we'll ever need for our purpose).</p> <ol> <li>Log into your Google account and then head over to Google CLoud. If you haven't already used Google Cloud's free trial, you'll have to sign up. It's free and you won't be autocharged unless you manually upgrade your account. Once the trial ends, we'll still have the free tier which is more than plenty for us.</li> <li>Go to the Google BigQuery page and click on the <code>Go to console</code> button.</li> <li>We can create a new project by following these instructions which will lead us to the create project page. <pre><code>Project name: made-with-ml  # Google will append a unique ID to the end of it\nLocation: No organization\n</code></pre></li> <li>Once the project has been created, refresh the page and we should see it (along with few other default projects from Google).</li> </ol> <pre><code># Google BigQuery projects\n\u251c\u2500\u2500 made-with-ml-XXXXXX   \ud83d\udc48 our project\n\u251c\u2500\u2500 bigquery-publicdata\n\u251c\u2500\u2500 imjasonh-storage\n\u2514\u2500\u2500 nyc-tlc\n</code></pre> <p>Console or code</p> <p>Most cloud providers will allow us to do everything via console but also programmatically via API, Python, etc. For example, we manually create a project but we could've also done so with code as shown here.</p>"},{"location":"courses/mlops/data-engineering/#define-bigquery-destination-in-airbyte","title":"Define BigQuery destination in Airbyte","text":"<p>Next, we need to establish the connection between Airbyte and BigQuery so that we can load the extracted data to the destination. In order to authenticate our access to BigQuery with Airbyte, we'll need to create a service account and generate a secret key. This is basically creating an identity with certain access that we can use for verification. Follow these instructions to create a service and generate the key file (JSON). Note down the location of this file because we'll be using it throughout this lesson. For example ours is <code>/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json</code>.</p> <ol> <li>On our Airbyte UI, click on <code>Destinations</code> on the left menu. Then click the <code>+ New destination</code> button on the top right corner.</li> <li>Click on the <code>Destination type</code> dropdown and choose <code>BigQuery</code>. This will open a view to define our file data source. <pre><code>Name: BigQuery\nDefault Dataset ID: mlops_course  # where our data will go inside our BigQuery project\nProject ID: made-with-ml-XXXXXX  # REPLACE this with your Google BiqQuery Project ID\nCredentials JSON: SERVICE-ACCOUNT-KEY.json  # REPLACE this with your service account JSON location\nDataset location: US  # select US or EU, all other options will not be compatible with dbt later\n</code></pre></li> <li>Click the <code>Set up destination</code> button and our data destination will be tested and saved.</li> </ol>"},{"location":"courses/mlops/data-engineering/#connections","title":"Connections","text":"<p>So we've set up our data sources (public CSV files) and destination (Google BigQuery data warehouse) but they haven't been connected yet. To create the connection, we need to think about a few aspects.</p>"},{"location":"courses/mlops/data-engineering/#frequency","title":"Frequency","text":"<p>How often do we want to extract data from the sources and load it into the destination?</p> <ul> <li><code>batch</code>: extracting data in batches, usually following a schedule (ex. daily) or when an event of interest occurs (ex. new data count)</li> <li><code>streaming</code>: extracting data in a continuous stream (using tools like Kafka, Kinesis, etc.)</li> </ul> <p>Micro-batch</p> <p>As we keep decreasing the time between batch ingestion (ex. towards 0), do we have stream ingestion? Not exactly. Batch processing is deliberately deciding to extract data from a source at a given interval. As that interval becomes &lt;15 minutes, it's referred to as a micro-batch (many data warehouses allow for batch ingestion every 5 minutes). However, with stream ingestion, the extraction process is continuously on and events will keep being ingested.</p> <p>Start simple</p> <p>In general, it's a good idea to start with batch ingestion for most applications and slowly add the complexity of streaming ingestion (and additional infrastructure). This was we can prove that downstream applications are finding value from the data source and evolving to streaming later should only improve things.</p> <p>We'll learn more about the different system design implications of batch vs. stream in our systems design lesson.</p>"},{"location":"courses/mlops/data-engineering/#connecting-file-source-to-bigquery-destination","title":"Connecting File source to BigQuery destination","text":"<p>Now we're ready to create the connection between our sources and destination:</p> <ol> <li>On our Airbyte UI, click on <code>Connections</code> on the left menu. Then click the <code>+ New connection</code> button on the top right corner.</li> <li>Under <code>Select a existing source</code>, click on the <code>Source</code> dropdown and choose <code>Projects</code> and click <code>Use existing source</code>.</li> <li>Under <code>Select a existing destination</code>, click on the <code>Destination</code> dropdown and choose <code>BigQuery</code> and click <code>Use existing destination</code>. <pre><code>Connection name: Projects &lt;&gt; BigQuery\nReplication frequency: Manual\nDestination Namespace: Mirror source structure\nNormalized tabular data: True  # leave this selected\n</code></pre></li> <li>Click the <code>Set up connection</code> button and our connection will be tested and saved.</li> <li>Repeat the same for our <code>Tags</code> source with the same <code>BigQuery</code> destination.</li> </ol> <p>Notice that our sync mode is <code>Full refresh | Overwrite</code> which means that every time we sync data from our source, it'll overwrite the existing data in our destination. As opposed to <code>Full refresh | Append</code> which will add entries from the source to bottom of the previous syncs.</p>"},{"location":"courses/mlops/data-engineering/#data-sync","title":"Data sync","text":"<p>Our replication frequency is <code>Manual</code> because we'll trigger the data syncs ourselves:</p> <ol> <li>On our Airbyte UI, click on <code>Connections</code> on the left menu. Then click the <code>Projects &lt;&gt; BigQuery</code> connection we set up earlier.</li> <li>Press the <code>\ud83d\udd04 Sync now</code> button and once it's completed we'll see that the projects are now in our BigQuery data warehouse.</li> <li>Repeat the same with our <code>Tags &lt;&gt; BigQuery</code> connection.</li> </ol> <pre><code># Inside our data warehouse\nmade-with-ml-XXXXXX               - Project\n\u2514\u2500\u2500 mlops_course                  - Dataset\n\u2502   \u251c\u2500\u2500 _airbyte_raw_projects     - table\n\u2502   \u251c\u2500\u2500 _airbyte_raw_tags         - table\n\u2502   \u251c\u2500\u2500 projects                  - table\n\u2502   \u2514\u2500\u2500 tags                      - table\n</code></pre> <p>In our orchestration lesson, we'll use Airflow to programmatically execute the data sync.</p> <p>We can easily explore and query this data using SQL directly inside our warehouse:</p> <ol> <li>On our BigQuery project page, click on the <code>\ud83d\udd0d QUERY</code> button and select <code>In new tab</code>.</li> <li>Run the following SQL statement and view the data: <pre><code>SELECT *\nFROM `made-with-ml-XXXXXX.mlops_course.projects`\nLIMIT 1000\n</code></pre></li> </ol> <pre>\nid\n      created_on\n      title\n      description\n    0\n      6\n      2020-02-20 06:43:18\n      Comparison between YOLO and RCNN on real world...\n      Bringing theory to experiment is cool. We can ...\n    1\n      7\n      2020-02-20 06:47:21\n      Show, Infer &amp; Tell: Contextual Inference for C...\n      The beauty of the work lies in the way it arch...\n    2\n      9\n      2020-02-24 16:24:45\n      Awesome Graph Classification\n      A collection of important graph embedding, cla...\n    3\n      15\n      2020-02-28 23:55:26\n      Awesome Monte Carlo Tree Search\n      A curated list of Monte Carlo tree search papers...\n    4\n      19\n      2020-03-03 13:54:31\n      Diffusion to Vector\n      Reference implementation of Diffusion2Vec (Com...\n    </pre>"},{"location":"courses/mlops/data-engineering/#best-practices","title":"Best practices","text":"<p>With the advent of cheap storage and cloud SaaS options to manage them, it's become a best practice to store raw data into data lakes. This allows for storage of raw, potentially unstructured, data without having to justify storage with downstream applications. When we do need to transform and process the data, we can move it to a data warehouse so can perform those operations efficiently.</p>"},{"location":"courses/mlops/data-engineering/#transform","title":"Transform","text":"<p>Once we've extracted and loaded our data, we need to transform the data so that it's ready for downstream applications. These transformations are different from the preprocessing we've seen before but are instead reflective of business logic that's agnostic to downstream applications. Common transformations include defining schemas, filtering, cleaning and joining data across tables, etc. While we could do all of these things with SQL in our data warehouse (save queries as tables or views), dbt delivers production functionality around version control, testing, documentation, packaging, etc. out of the box. This becomes crucial for maintaining observability and high quality data workflows.</p> <p>Popular transformation tools include dbt, Matillion, custom jinja templated SQL, etc.</p> <p>Note</p> <p>In addition to data transformations, we can also process the data using large-scale analytics engines like Spark, Flink, etc.</p>"},{"location":"courses/mlops/data-engineering/#dbt-cloud","title":"dbt Cloud","text":"<p>Now we're ready to transform our data in our data warehouse using dbt. We'll be using a developer account on dbt Cloud (free), which provides us with an IDE, unlimited runs, etc.</p> <p>We'll learn how to use the dbt-core in our orchestration lesson. Unlike dbt Cloud, dbt core is completely open-source and we can programmatically connect to our data warehouse and perform transformations.</p> <ol> <li>Create a free account and verify it.</li> <li>Go to https://cloud.getdbt.com/ to get set up.</li> <li>Click <code>continue</code> and choose <code>BigQuery</code> as the database.</li> <li>Click <code>Upload a Service Account JSON file</code> and upload our file to autopopulate everything.</li> <li>Click the <code>Test</code> &gt; <code>Continue</code>.</li> <li>Click <code>Managed</code> repository and name it <code>dbt-transforms</code> (or anything else you want).</li> <li>Click <code>Create</code> &gt; <code>Continue</code> &gt; <code>Skip and complete</code>.</li> <li>This will open the project page and click <code>&gt;_ Start Developing</code> button.</li> <li>This will open the IDE where we can click <code>\ud83d\uddc2 initialize your project</code>.</li> </ol> <p>Now we're ready to start developing our models:</p> <ol> <li>Click the <code>\u00b7\u00b7\u00b7</code> next to the <code>models</code> directory on the left menu.</li> <li>Click <code>New folder</code> called <code>models/labeled_projects</code>.</li> <li>Create a <code>New file</code> under <code>models/labeled_projects</code> called <code>labeled_projects.sql</code>.</li> <li>Repeat for another file under <code>models/labeled_projects</code> called <code>schema.yml</code>.</li> </ol> <pre><code>dbt-cloud-XXXXX-dbt-transforms\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 models\n\u2502   \u251c\u2500\u2500 example\n\u2502   \u2514\u2500\u2500 labeled_projects\n\u2502   \u2502   \u251c\u2500\u2500 labeled_projects.sql\n\u2502   \u2502   \u2514\u2500\u2500 schema.yml\n\u251c\u2500\u2500 ...\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"courses/mlops/data-engineering/#joins","title":"Joins","text":"<p>Inside our <code>models/labeled_projects/labeled_projects.sql</code> file we'll create a view that joins our project data with the appropriate tags. This will create the labeled data necessary for downstream applications such as machine learning models. Here we're joining based on the matching id between the projects and tags:</p> <pre><code>-- models/labeled_projects/labeled_projects.sql\nSELECT p.id, created_on, title, description, tag\nFROM `made-with-ml-XXXXXX.mlops_course.projects` p  -- REPLACE\nLEFT JOIN `made-with-ml-XXXXXX.mlops_course.tags` t  -- REPLACE\nON p.id = t.id\n</code></pre> <p>We can view the queried results by clicking the <code>Preview</code> button and view the data lineage as well.</p>"},{"location":"courses/mlops/data-engineering/#schemas","title":"Schemas","text":"<p>Inside our <code>models/labeled_projects/schema.yml</code> file we'll define the schemas for each of the features in our transformed data. We also define several tests that each feature should pass. View the full list of dbt tests but note that we'll use Great Expectations for more comprehensive tests when we orchestrate all these data workflows in our orchestration lesson.</p> <pre><code># models/labeled_projects/schema.yml\n\nversion: 2\n\nmodels:\n- name: labeled_projects\ndescription: \"Tags for all projects\"\ncolumns:\n- name: id\ndescription: \"Unique ID of the project.\"\ntests:\n- unique\n- not_null\n- name: title\ndescription: \"Title of the project.\"\ntests:\n- not_null\n- name: description\ndescription: \"Description of the project.\"\ntests:\n- not_null\n- name: tag\ndescription: \"Labeled tag for the project.\"\ntests:\n- not_null\n</code></pre>"},{"location":"courses/mlops/data-engineering/#runs","title":"Runs","text":"<p>At the bottom of the IDE, we can execute runs based on the transformations we've defined. We'll run each of the following commands and once they finish, we can see the transformed data inside our data warehouse.</p> <pre><code>dbt run\ndbt test\n</code></pre> <p>Once these commands run successfully, we're ready to move our transformations to a production environment where we can insert this view in our data warehouse.</p>"},{"location":"courses/mlops/data-engineering/#jobs","title":"Jobs","text":"<p>In order to apply these transformations to the data in our data warehouse, it's best practice to create an Environment and then define Jobs:</p> <ol> <li>Click <code>Environments</code> on the left menu &gt; <code>New Environment</code> button (top right corner) and fill out the details: <pre><code>Name: Production\nType: Deployment\n...\nDataset: mlops_course\n</code></pre></li> <li>Click <code>New Job</code> with the following details and then click <code>Save</code> (top right corner). <pre><code>Name: Transform\nEnvironment: Production\nCommands: dbt run\ndbt test\nSchedule: uncheck \"RUN ON SCHEDULE\"\n</code></pre></li> <li>Click <code>Run Now</code> and view the transformed data in our data warehouse under a view called <code>labeled_projects</code>.</li> </ol> <pre><code># Inside our data warehouse\nmade-with-ml-XXXXXX               - Project\n\u2514\u2500\u2500 mlops_course                  - Dataset\n\u2502   \u251c\u2500\u2500 _airbyte_raw_projects     - table\n\u2502   \u251c\u2500\u2500 _airbyte_raw_tags         - table\n\u2502   \u251c\u2500\u2500 labeled_projects          - view\n\u2502   \u251c\u2500\u2500 projects                  - table\n\u2502   \u2514\u2500\u2500 tags                      - table\n</code></pre> <p>There is so much more to dbt so be sure to check out their official documentation to really customize any workflows. And be sure to check out our orchestration lesson where we'll programmatically create and execute our dbt transformations.</p>"},{"location":"courses/mlops/data-engineering/#implementations","title":"Implementations","text":"<p>Hopefully we created our data stack for the purpose of gaining some actionable insight about our business, users, etc. Because it's these use cases that dictate which sources of data we extract from, how often and how that data is stored and transformed. Downstream applications of our data typically fall into one of these categories:</p> <ul> <li><code>data analytics</code>: use cases focused on reporting trends, aggregate views, etc. via charts, dashboards, etc.for the purpose of providing operational insight for business stakeholders. <p>\ud83d\udee0\u00a0 Popular tools: Tableau, Looker, Metabase, Superset, etc.</p> </li> <li><code>machine learning</code>: use cases centered around using the transformed data to construct predictive models (forecasting, personalization, etc.).</li> </ul> <p>While it's very easy to extract data from our data warehouse:</p> <p><pre><code>pip install google-cloud-bigquery==1.21.0\n</code></pre> <pre><code>from google.cloud import bigquery\nfrom google.oauth2 import service_account\n\n# Replace these with your own values\nproject_id = \"made-with-ml-XXXXXX\"  # REPLACE\nSERVICE_ACCOUNT_KEY_JSON = \"/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json\"  # REPLACE\n\n# Establish connection\ncredentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_KEY_JSON)\nclient = bigquery.Client(credentials= credentials, project=project_id)\n\n# Query data\nquery_job = client.query(\"\"\"\n   SELECT *\n   FROM mlops_course.labeled_projects\"\"\")\nresults = query_job.result()\nresults.to_dataframe().head()\n</code></pre></p> id created_on title description tag 0 1994.0 2020-07-29 04:51:30 Understanding the Effectivity of Ensembles in ... The report explores the ideas presented in Dee... computer-vision 1 1506.0 2020-06-19 06:26:17 Using GitHub Actions for MLOps &amp; Data Science A collection of resources on how to facilitate... mlops 2 807.0 2020-05-11 02:25:51 Introduction to Machine Learning Problem Framing This course helps you frame machine learning (... mlops 3 1204.0 2020-06-05 22:56:38 Snaked: Classifying Snake Species using Images Proof of concept that it is possible to identi... computer-vision 4 1706.0 2020-07-04 11:05:28 PokeZoo A deep learning based web-app developed using ... computer-vision <p>Warning</p> <p>Check out our notebook where we extract the transformed data from our data warehouse. We do this in a separate notebook because it requires the <code>google-cloud-bigquery</code> package and until dbt loosens it's Jinja versioning constraints... it'll have to be done in a separate environment. However, downstream applications are typically analytics or ML applications which have their own environments anyway so these conflicts are not inhibiting.</p> <p>many of the analytics (ex. dashboards) and machine learning solutions (ex. feature stores) allow for easy connection to our data warehouses so that workflows can be triggered when an event occurs or on a schedule. We're going to take this a step further in the next lesson where we'll use a central orchestration platform to control all these workflows.</p> <p>Analytics first, then ML</p> <p>It's a good idea for the first several applications to be analytics and reporting based in order to establish a robust data stack. These use cases typically just involve displaying data aggregations and trends, as opposed to machine learning systems that involve additional complex infrastructure and workflows.</p>"},{"location":"courses/mlops/data-engineering/#observability","title":"Observability","text":"<p>When we create complex data workflows like this, observability becomes a top priority. Data observability is the general  concept of understanding the condition of data in our system and it involves:</p> <ul> <li><code>data quality</code>: testing and monitoring our data quality after every step (schemas, completeness, recency, etc.).</li> <li><code>data lineage</code>: mapping the where data comes from and how it's being transformed as it moves through our pipelines.</li> <li><code>discoverability</code>: enabling discovery of the different data sources and features for downstream applications.</li> <li><code>privacy + security</code>: are the different data assets treated and restricted appropriately amongst the applications?</li> </ul> <p>Popular observability tools include Monte Carlo, Bigeye, etc.</p>"},{"location":"courses/mlops/data-engineering/#considerations","title":"Considerations","text":"<p>The data stack ecosystem to create the robust data workflows is growing and maturing. However, it can be overwhelming when it comes to choosing the best tooling options, especially as needs change over time. Here are a few important factors to consider when making a tooling decision in this space:</p> <ul> <li>What is the cost per time per employee? Some of the tooling options can rack up quite the bill!</li> <li>Does the tool have the proper connectors to integrate with our data sources and the rest of the stack?</li> <li>Does the tool fit with our team's technical aptitude (SQL, Spark, Python, etc.)?</li> <li>What kind of support does the tool offer (enterprise, community, etc.)?</li> </ul> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Data engineering - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/data-stack/","title":"Data Stack for Machine Learning","text":""},{"location":"courses/mlops/data-stack/#intuition","title":"Intuition","text":"<p>So far we've had the convenience of using local CSV files as data source but in reality, our data can come from many disparate sources. Additionally, our processes around transforming and testing our data should ideally be moved upstream so that many different downstream processes can benefit from them. Our ML use case being just one among the many potential downstream applications. To address these shortcomings, we're going to learn about the fundamentals of data engineering and construct a modern data stack that can scale and provide high quality data for our applications.</p> <p>View the  data-engineering repository for all the code.</p> <p>At a high level, we're going to:</p> <ol> <li>Extract and Load data from sources to destinations.</li> <li>Transform data for downstream applications.</li> </ol> <p>This process is more commonly known as ELT, but there are variants such as ETL and reverse ETL, etc. They are all essentially the same underlying workflows but have slight differences in the order of data flow and where data is processed and stored.</p> <p>Utility and simplicity</p> <p>It can be enticing to set up a modern data stack in your organization, especially with all the hype. But it's very important to motivate utility and adding additional complexity:</p> <ul> <li>Start with a use case that we already have data sources for and has direct impact on the business' bottom line (ex. user churn).</li> <li>Start with the simplest infrastructure (source \u2192 database \u2192 report) and add complexity (in infrastructure, performance and team) as needed.</li> </ul>"},{"location":"courses/mlops/data-stack/#data-systems","title":"Data systems","text":"<p>Before we start working with our data, it's important to understand the different types of systems that our data can live in. So far in this course we've worked with files, but there are several types of data systems that are widely adopted in industry for different purposes.</p>"},{"location":"courses/mlops/data-stack/#data-lake","title":"Data lake","text":"<p>A data lake is a flat data management system that stores raw objects. It's a great option for inexpensive storage and has the capability to hold all types of data (unstructured, semi-structured and structured). Object stores are becoming the standard for data lakes with default options across the popular cloud providers. Unfortunately, because data is stored as objects in a data lake, it's not designed for operating on structured data.</p> <p>Popular data lake options include Amazon S3, Azure Blob Storage, Google Cloud Storage, etc.</p>"},{"location":"courses/mlops/data-stack/#database","title":"Database","text":"<p>Another popular storage option is a database (DB), which is an organized collection of structured data that adheres to either:</p> <ul> <li>relational schema (tables with rows and columns) often referred to as a Relational Database Management System (RDBMS) or SQL database.</li> <li>non-relational (key/value, graph, etc.), often referred to as a non-relational database or NoSQL database.</li> </ul> <p>A database is an online transaction processing (OLTP) system because it's typically used for day-to-day CRUD (create, read, update, delete) operations where typically information is accessed by rows. However, they're generally used to store data from one application and is not designed to hold data from across many sources for the purpose of analytics.</p> <p>Popular database options include PostgreSQL, MySQL, MongoDB, Cassandra, etc.</p>"},{"location":"courses/mlops/data-stack/#data-warehouse","title":"Data warehouse","text":"<p>A data warehouse (DWH) is a type of database that's designed for storing structured data from many different sources for downstream analytics and data science. It's an online analytical processing (OLAP) system that's optimized for performing operations across aggregating column values rather than accessing specific rows.</p> <p>Popular data warehouse options include SnowFlake, Google BigQuery, Amazon RedShift, Hive, etc.</p>"},{"location":"courses/mlops/data-stack/#extract-and-load","title":"Extract and load","text":"<p>The first step in our data pipeline is to extract data from a source and load it into the appropriate destination. While we could construct custom scripts to do this manually or on a schedule, an ecosystem of data ingestion tools have already standardized the entire process. They all come equipped with connectors that allow for extraction, normalization, cleaning and loading between sources and destinations. And these pipelines can be scaled, monitored, etc. all with very little to no code.</p> <p>Popular data ingestion tools include Fivetran, Airbyte, Stitch, etc.</p> <p>We're going to use the open-source tool Airbyte to create connections between our data sources and destinations. Let's set up Airbyte and define our data sources. As we progress in this lesson, we'll set up our destinations and create connections to extract and load data.</p> <ol> <li>Ensure that we still have Docker installed from our Docker lesson but if not, download it here. For Windows users, be sure to have these configurations enabled.</li> <li>In a parent directory, outside our project directory for the MLOps course, execute the following commands to load the Airbyte repository locally and launch the service. <pre><code>git clone https://github.com/airbytehq/airbyte.git\ncd airbyte\ndocker-compose up\n</code></pre></li> <li>After a few minutes, visit http://localhost:8000/ to view the launched Airbyte service.</li> </ol>"},{"location":"courses/mlops/data-stack/#sources","title":"Sources","text":"<p>Our data sources we want to extract from can be from anywhere. They could come from 3rd party apps, files, user click streams, physical devices, data lakes, databases, data warehouses, etc. But regardless of the source of our data, they type of data should fit into one of these categories:</p> <ul> <li><code>structured</code>: organized data stored in an explicit structure (ex. tables)</li> <li><code>semi-structured</code>: data with some structure but no formal schema or data types (web pages, CSV, JSON, etc.)</li> <li><code>unstructured</code>: qualitative data with no formal structure (text, images, audio, etc.)</li> </ul> <p>For our application, we'll define two data sources:</p> <ul> <li>projects.csv: data containing projects with their ID, create date, title and description.</li> <li>tags.csv: labels for each of project IDs in projects.csv</li> </ul> <p>Ideally, these data assets would be retrieved from a database that contains projects that we extracted and perhaps another database that stores labels from our labeling team's workflows. However, for simplicity we'll use CSV files to demonstrate how to define a data source.</p>"},{"location":"courses/mlops/data-stack/#define-file-source-in-airbyte","title":"Define file source in Airbyte","text":"<p>We'll start our ELT process by defining the data source in Airbyte:</p> <ol> <li>On our Airbyte UI, click on <code>Sources</code> on the left menu. Then click the <code>+ New source</code> button on the top right corner.</li> <li>Click on the <code>Source type</code> dropdown and choose <code>File</code>. This will open a view to define our file data source. <pre><code>Name: Projects\nURL: https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\nFile Format: csv\nStorage Provider: HTTPS: Public Web\nDataset Name: projects\n</code></pre></li> <li>Click the <code>Set up source</code> button and our data source will be tested and saved.</li> <li>Repeat steps 1-3 for our tags data source as well: <pre><code>Name: Tags\nURL: https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\nFile Format: csv\nStorage Provider: HTTPS: Public Web\nDataset Name: tags\n</code></pre></li> </ol>"},{"location":"courses/mlops/data-stack/#destinations","title":"Destinations","text":"<p>Once we know the source we want to extract data from, we need to decide the destination to load it. The choice depends on what our downstream applications want to be able to do with the data. And it's also common to store data in one location (ex. data lake) and move it somewhere else (ex. data warehouse) for specific processing.</p>"},{"location":"courses/mlops/data-stack/#set-up-google-bigquery","title":"Set up Google BigQuery","text":"<p>Our destination will be a data warehouse since we'll want to use the data for downstream analytical and machine learning applications. We're going to use Google BigQuery which is free under Google Cloud's free tier for up to 10 GB storage and 1TB of queries (which is significantly more than we'll ever need for our purpose).</p> <ol> <li>Log into your Google account and then head over to Google CLoud. If you haven't already used Google Cloud's free trial, you'll have to sign up. It's free and you won't be autocharged unless you manually upgrade your account. Once the trial ends, we'll still have the free tier which is more than plenty for us.</li> <li>Go to the Google BigQuery page and click on the <code>Go to console</code> button.</li> <li>We can create a new project by following these instructions which will lead us to the create project page. <pre><code>Project name: made-with-ml  # Google will append a unique ID to the end of it\nLocation: No organization\n</code></pre></li> <li>Once the project has been created, refresh the page and we should see it (along with few other default projects from Google).</li> </ol> <pre><code># Google BigQuery projects\n\u251c\u2500\u2500 made-with-ml-XXXXXX   \ud83d\udc48 our project\n\u251c\u2500\u2500 bigquery-publicdata\n\u251c\u2500\u2500 imjasonh-storage\n\u2514\u2500\u2500 nyc-tlc\n</code></pre> <p>Console or code</p> <p>Most cloud providers will allow us to do everything via console but also programmatically via API, Python, etc. For example, we manually create a project but we could've also done so with code as shown here.</p>"},{"location":"courses/mlops/data-stack/#define-bigquery-destination-in-airbyte","title":"Define BigQuery destination in Airbyte","text":"<p>Next, we need to establish the connection between Airbyte and BigQuery so that we can load the extracted data to the destination. In order to authenticate our access to BigQuery with Airbyte, we'll need to create a service account and generate a secret key. This is basically creating an identity with certain access that we can use for verification. Follow these instructions to create a service and generate the key file (JSON). Note down the location of this file because we'll be using it throughout this lesson. For example ours is <code>/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json</code>.</p> <ol> <li>On our Airbyte UI, click on <code>Destinations</code> on the left menu. Then click the <code>+ New destination</code> button on the top right corner.</li> <li>Click on the <code>Destination type</code> dropdown and choose <code>BigQuery</code>. This will open a view to define our file data source. <pre><code>Name: BigQuery\nDefault Dataset ID: mlops_course  # where our data will go inside our BigQuery project\nProject ID: made-with-ml-XXXXXX  # REPLACE this with your Google BiqQuery Project ID\nCredentials JSON: SERVICE-ACCOUNT-KEY.json  # REPLACE this with your service account JSON location\nDataset location: US  # select US or EU, all other options will not be compatible with dbt later\n</code></pre></li> <li>Click the <code>Set up destination</code> button and our data destination will be tested and saved.</li> </ol>"},{"location":"courses/mlops/data-stack/#connections","title":"Connections","text":"<p>So we've set up our data sources (public CSV files) and destination (Google BigQuery data warehouse) but they haven't been connected yet. To create the connection, we need to think about a few aspects.</p>"},{"location":"courses/mlops/data-stack/#frequency","title":"Frequency","text":"<p>How often do we want to extract data from the sources and load it into the destination?</p> <ul> <li><code>batch</code>: extracting data in batches, usually following a schedule (ex. daily) or when an event of interest occurs (ex. new data count)</li> <li><code>streaming</code>: extracting data in a continuous stream (using tools like Kafka, Kinesis, etc.)</li> </ul> <p>Micro-batch</p> <p>As we keep decreasing the time between batch ingestion (ex. towards 0), do we have stream ingestion? Not exactly. Batch processing is deliberately deciding to extract data from a source at a given interval. As that interval becomes &lt;15 minutes, it's referred to as a micro-batch (many data warehouses allow for batch ingestion every 5 minutes). However, with stream ingestion, the extraction process is continuously on and events will keep being ingested.</p> <p>Start simple</p> <p>In general, it's a good idea to start with batch ingestion for most applications and slowly add the complexity of streaming ingestion (and additional infrastructure). This was we can prove that downstream applications are finding value from the data source and evolving to streaming later should only improve things.</p> <p>We'll learn more about the different system design implications of batch vs. stream in our systems design lesson.</p>"},{"location":"courses/mlops/data-stack/#connecting-file-source-to-bigquery-destination","title":"Connecting File source to BigQuery destination","text":"<p>Now we're ready to create the connection between our sources and destination:</p> <ol> <li>On our Airbyte UI, click on <code>Connections</code> on the left menu. Then click the <code>+ New connection</code> button on the top right corner.</li> <li>Under <code>Select a existing source</code>, click on the <code>Source</code> dropdown and choose <code>Projects</code> and click <code>Use existing source</code>.</li> <li>Under <code>Select a existing destination</code>, click on the <code>Destination</code> dropdown and choose <code>BigQuery</code> and click <code>Use existing destination</code>. <pre><code>Connection name: Projects &lt;&gt; BigQuery\nReplication frequency: Manual\nDestination Namespace: Mirror source structure\nNormalized tabular data: True  # leave this selected\n</code></pre></li> <li>Click the <code>Set up connection</code> button and our connection will be tested and saved.</li> <li>Repeat the same for our <code>Tags</code> source with the same <code>BigQuery</code> destination.</li> </ol> <p>Notice that our sync mode is <code>Full refresh | Overwrite</code> which means that every time we sync data from our source, it'll overwrite the existing data in our destination. As opposed to <code>Full refresh | Append</code> which will add entries from the source to bottom of the previous syncs.</p>"},{"location":"courses/mlops/data-stack/#data-sync","title":"Data sync","text":"<p>Our replication frequency is <code>Manual</code> because we'll trigger the data syncs ourselves:</p> <ol> <li>On our Airbyte UI, click on <code>Connections</code> on the left menu. Then click the <code>Projects &lt;&gt; BigQuery</code> connection we set up earlier.</li> <li>Press the <code>\ud83d\udd04 Sync now</code> button and once it's completed we'll see that the projects are now in our BigQuery data warehouse.</li> <li>Repeat the same with our <code>Tags &lt;&gt; BigQuery</code> connection.</li> </ol> <pre><code># Inside our data warehouse\nmade-with-ml-XXXXXX               - Project\n\u2514\u2500\u2500 mlops_course                  - Dataset\n\u2502   \u251c\u2500\u2500 _airbyte_raw_projects     - table\n\u2502   \u251c\u2500\u2500 _airbyte_raw_tags         - table\n\u2502   \u251c\u2500\u2500 projects                  - table\n\u2502   \u2514\u2500\u2500 tags                      - table\n</code></pre> <p>In our orchestration lesson, we'll use Airflow to programmatically execute the data sync.</p> <p>We can easily explore and query this data using SQL directly inside our warehouse:</p> <ol> <li>On our BigQuery project page, click on the <code>\ud83d\udd0d QUERY</code> button and select <code>In new tab</code>.</li> <li>Run the following SQL statement and view the data: <pre><code>SELECT *\nFROM `made-with-ml-XXXXXX.mlops_course.projects`\nLIMIT 1000\n</code></pre></li> </ol> <pre>\nid\n      created_on\n      title\n      description\n    0\n      6\n      2020-02-20 06:43:18\n      Comparison between YOLO and RCNN on real world...\n      Bringing theory to experiment is cool. We can ...\n    1\n      7\n      2020-02-20 06:47:21\n      Show, Infer &amp; Tell: Contextual Inference for C...\n      The beauty of the work lies in the way it arch...\n    2\n      9\n      2020-02-24 16:24:45\n      Awesome Graph Classification\n      A collection of important graph embedding, cla...\n    3\n      15\n      2020-02-28 23:55:26\n      Awesome Monte Carlo Tree Search\n      A curated list of Monte Carlo tree search papers...\n    4\n      19\n      2020-03-03 13:54:31\n      Diffusion to Vector\n      Reference implementation of Diffusion2Vec (Com...\n    </pre>"},{"location":"courses/mlops/data-stack/#best-practices","title":"Best practices","text":"<p>With the advent of cheap storage and cloud SaaS options to manage them, it's become a best practice to store raw data into data lakes. This allows for storage of raw, potentially unstructured, data without having to justify storage with downstream applications. When we do need to transform and process the data, we can move it to a data warehouse so can perform those operations efficiently.</p>"},{"location":"courses/mlops/data-stack/#transform","title":"Transform","text":"<p>Once we've extracted and loaded our data, we need to transform the data so that it's ready for downstream applications. These transformations are different from the preprocessing we've seen before but are instead reflective of business logic that's agnostic to downstream applications. Common transformations include defining schemas, filtering, cleaning and joining data across tables, etc. While we could do all of these things with SQL in our data warehouse (save queries as tables or views), dbt delivers production functionality around version control, testing, documentation, packaging, etc. out of the box. This becomes crucial for maintaining observability and high quality data workflows.</p> <p>Popular transformation tools include dbt, Matillion, custom jinja templated SQL, etc.</p> <p>Note</p> <p>In addition to data transformations, we can also process the data using large-scale analytics engines like Spark, Flink, etc.</p>"},{"location":"courses/mlops/data-stack/#dbt-cloud","title":"dbt Cloud","text":"<p>Now we're ready to transform our data in our data warehouse using dbt. We'll be using a developer account on dbt Cloud (free), which provides us with an IDE, unlimited runs, etc.</p> <p>We'll learn how to use the dbt-core in our orchestration lesson. Unlike dbt Cloud, dbt core is completely open-source and we can programmatically connect to our data warehouse and perform transformations.</p> <ol> <li>Create a free account and verify it.</li> <li>Go to https://cloud.getdbt.com/ to get set up.</li> <li>Click <code>continue</code> and choose <code>BigQuery</code> as the database.</li> <li>Click <code>Upload a Service Account JSON file</code> and upload our file to autopopulate everything.</li> <li>Click the <code>Test</code> &gt; <code>Continue</code>.</li> <li>Click <code>Managed</code> repository and name it <code>dbt-transforms</code> (or anything else you want).</li> <li>Click <code>Create</code> &gt; <code>Continue</code> &gt; <code>Skip and complete</code>.</li> <li>This will open the project page and click <code>&gt;_ Start Developing</code> button.</li> <li>This will open the IDE where we can click <code>\ud83d\uddc2 initialize your project</code>.</li> </ol> <p>Now we're ready to start developing our models:</p> <ol> <li>Click the <code>\u00b7\u00b7\u00b7</code> next to the <code>models</code> directory on the left menu.</li> <li>Click <code>New folder</code> called <code>models/labeled_projects</code>.</li> <li>Create a <code>New file</code> under <code>models/labeled_projects</code> called <code>labeled_projects.sql</code>.</li> <li>Repeat for another file under <code>models/labeled_projects</code> called <code>schema.yml</code>.</li> </ol> <pre><code>dbt-cloud-XXXXX-dbt-transforms\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 models\n\u2502   \u251c\u2500\u2500 example\n\u2502   \u2514\u2500\u2500 labeled_projects\n\u2502   \u2502   \u251c\u2500\u2500 labeled_projects.sql\n\u2502   \u2502   \u2514\u2500\u2500 schema.yml\n\u251c\u2500\u2500 ...\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"courses/mlops/data-stack/#joins","title":"Joins","text":"<p>Inside our <code>models/labeled_projects/labeled_projects.sql</code> file we'll create a view that joins our project data with the appropriate tags. This will create the labeled data necessary for downstream applications such as machine learning models. Here we're joining based on the matching id between the projects and tags:</p> <pre><code>-- models/labeled_projects/labeled_projects.sql\nSELECT p.id, created_on, title, description, tag\nFROM `made-with-ml-XXXXXX.mlops_course.projects` p  -- REPLACE\nLEFT JOIN `made-with-ml-XXXXXX.mlops_course.tags` t  -- REPLACE\nON p.id = t.id\n</code></pre> <p>We can view the queried results by clicking the <code>Preview</code> button and view the data lineage as well.</p>"},{"location":"courses/mlops/data-stack/#schemas","title":"Schemas","text":"<p>Inside our <code>models/labeled_projects/schema.yml</code> file we'll define the schemas for each of the features in our transformed data. We also define several tests that each feature should pass. View the full list of dbt tests but note that we'll use Great Expectations for more comprehensive tests when we orchestrate all these data workflows in our orchestration lesson.</p> <pre><code># models/labeled_projects/schema.yml\n\nversion: 2\n\nmodels:\n- name: labeled_projects\ndescription: \"Tags for all projects\"\ncolumns:\n- name: id\ndescription: \"Unique ID of the project.\"\ntests:\n- unique\n- not_null\n- name: title\ndescription: \"Title of the project.\"\ntests:\n- not_null\n- name: description\ndescription: \"Description of the project.\"\ntests:\n- not_null\n- name: tag\ndescription: \"Labeled tag for the project.\"\ntests:\n- not_null\n</code></pre>"},{"location":"courses/mlops/data-stack/#runs","title":"Runs","text":"<p>At the bottom of the IDE, we can execute runs based on the transformations we've defined. We'll run each of the following commands and once they finish, we can see the transformed data inside our data warehouse.</p> <pre><code>dbt run\ndbt test\n</code></pre> <p>Once these commands run successfully, we're ready to move our transformations to a production environment where we can insert this view in our data warehouse.</p>"},{"location":"courses/mlops/data-stack/#jobs","title":"Jobs","text":"<p>In order to apply these transformations to the data in our data warehouse, it's best practice to create an Environment and then define Jobs:</p> <ol> <li>Click <code>Environments</code> on the left menu &gt; <code>New Environment</code> button (top right corner) and fill out the details: <pre><code>Name: Production\nType: Deployment\n...\nDataset: mlops_course\n</code></pre></li> <li>Click <code>New Job</code> with the following details and then click <code>Save</code> (top right corner). <pre><code>Name: Transform\nEnvironment: Production\nCommands: dbt run\ndbt test\nSchedule: uncheck \"RUN ON SCHEDULE\"\n</code></pre></li> <li>Click <code>Run Now</code> and view the transformed data in our data warehouse under a view called <code>labeled_projects</code>.</li> </ol> <pre><code># Inside our data warehouse\nmade-with-ml-XXXXXX               - Project\n\u2514\u2500\u2500 mlops_course                  - Dataset\n\u2502   \u251c\u2500\u2500 _airbyte_raw_projects     - table\n\u2502   \u251c\u2500\u2500 _airbyte_raw_tags         - table\n\u2502   \u251c\u2500\u2500 labeled_projects          - view\n\u2502   \u251c\u2500\u2500 projects                  - table\n\u2502   \u2514\u2500\u2500 tags                      - table\n</code></pre> <p>There is so much more to dbt so be sure to check out their official documentation to really customize any workflows. And be sure to check out our orchestration lesson where we'll programmatically create and execute our dbt transformations.</p>"},{"location":"courses/mlops/data-stack/#implementations","title":"Implementations","text":"<p>Hopefully we created our data stack for the purpose of gaining some actionable insight about our business, users, etc. Because it's these use cases that dictate which sources of data we extract from, how often and how that data is stored and transformed. Downstream applications of our data typically fall into one of these categories:</p> <ul> <li><code>data analytics</code>: use cases focused on reporting trends, aggregate views, etc. via charts, dashboards, etc.for the purpose of providing operational insight for business stakeholders. <p>\ud83d\udee0\u00a0 Popular tools: Tableau, Looker, Metabase, Superset, etc.</p> </li> <li><code>machine learning</code>: use cases centered around using the transformed data to construct predictive models (forecasting, personalization, etc.).</li> </ul> <p>While it's very easy to extract data from our data warehouse:</p> <p><pre><code>pip install google-cloud-bigquery==1.21.0\n</code></pre> <pre><code>from google.cloud import bigquery\nfrom google.oauth2 import service_account\n\n# Replace these with your own values\nproject_id = \"made-with-ml-XXXXXX\"  # REPLACE\nSERVICE_ACCOUNT_KEY_JSON = \"/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json\"  # REPLACE\n\n# Establish connection\ncredentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_KEY_JSON)\nclient = bigquery.Client(credentials= credentials, project=project_id)\n\n# Query data\nquery_job = client.query(\"\"\"\n   SELECT *\n   FROM mlops_course.labeled_projects\"\"\")\nresults = query_job.result()\nresults.to_dataframe().head()\n</code></pre></p> id created_on title description tag 0 1994.0 2020-07-29 04:51:30 Understanding the Effectivity of Ensembles in ... The report explores the ideas presented in Dee... computer-vision 1 1506.0 2020-06-19 06:26:17 Using GitHub Actions for MLOps &amp; Data Science A collection of resources on how to facilitate... mlops 2 807.0 2020-05-11 02:25:51 Introduction to Machine Learning Problem Framing This course helps you frame machine learning (... mlops 3 1204.0 2020-06-05 22:56:38 Snaked: Classifying Snake Species using Images Proof of concept that it is possible to identi... computer-vision 4 1706.0 2020-07-04 11:05:28 PokeZoo A deep learning based web-app developed using ... computer-vision <p>Warning</p> <p>Check out our notebook where we extract the transformed data from our data warehouse. We do this in a separate notebook because it requires the <code>google-cloud-bigquery</code> package and until dbt loosens it's Jinja versioning constraints... it'll have to be done in a separate environment. However, downstream applications are typically analytics or ML applications which have their own environments anyway so these conflicts are not inhibiting.</p> <p>many of the analytics (ex. dashboards) and machine learning solutions (ex. feature stores) allow for easy connection to our data warehouses so that workflows can be triggered when an event occurs or on a schedule. We're going to take this a step further in the next lesson where we'll use a central orchestration platform to control all these workflows.</p> <p>Analytics first, then ML</p> <p>It's a good idea for the first several applications to be analytics and reporting based in order to establish a robust data stack. These use cases typically just involve displaying data aggregations and trends, as opposed to machine learning systems that involve additional complex infrastructure and workflows.</p>"},{"location":"courses/mlops/data-stack/#observability","title":"Observability","text":"<p>When we create complex data workflows like this, observability becomes a top priority. Data observability is the general  concept of understanding the condition of data in our system and it involves:</p> <ul> <li><code>data quality</code>: testing and monitoring our data quality after every step (schemas, completeness, recency, etc.).</li> <li><code>data lineage</code>: mapping the where data comes from and how it's being transformed as it moves through our pipelines.</li> <li><code>discoverability</code>: enabling discovery of the different data sources and features for downstream applications.</li> <li><code>privacy + security</code>: are the different data assets treated and restricted appropriately amongst the applications?</li> </ul> <p>Popular observability tools include Monte Carlo, Bigeye, etc.</p>"},{"location":"courses/mlops/data-stack/#considerations","title":"Considerations","text":"<p>The data stack ecosystem to create the robust data workflows is growing and maturing. However, it can be overwhelming when it comes to choosing the best tooling options, especially as needs change over time. Here are a few important factors to consider when making a tooling decision in this space:</p> <ul> <li>What is the cost per time per employee? Some of the tooling options can rack up quite the bill!</li> <li>Does the tool have the proper connectors to integrate with our data sources and the rest of the stack?</li> <li>Does the tool fit with our team's technical aptitude (SQL, Spark, Python, etc.)?</li> <li>What kind of support does the tool offer (enterprise, community, etc.)?</li> </ul> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Data Stack for Machine Learning - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/distributed-data/","title":"Distributed Data Processing","text":""},{"location":"courses/mlops/distributed-data/#intuition","title":"Intuition","text":"<p>So far we've performed our data processing operations on a single machine. Our dataset was able to fit into a single Pandas DataFrame and we were able to perform our operations in a single Python process. But what if our dataset was too large to fit into a single machine? We would need to distribute our data processing operations across multiple machines. And with the increasing trend in ML for larger unstructured datasets and larger models (LLMs), we can quickly outgrow our single machine constraints and will need to go distributed.</p> <p>Note</p> <p>Our dataset is intentionally small for this course so that we can quickly execute the code. But with our distributed set up in this lesson, we can easily switch to a mcuh larger dataset and the code will continue to execute perfectly. And if we add more compute resources, we can scale our data processing operations to be even faster with no changes to our code.</p>"},{"location":"courses/mlops/distributed-data/#implementation","title":"Implementation","text":"<p>There are many frameworks for distributed computing, such as Ray, Dask, Modin, Spark, etc. All of these are great options but for our application we want to choose a framework that is will allow us to scale our data processing operations with minimal changes to our existing code and all in Python. We also want to choose a framework that will integrate well when we want to distributed our downstream workloads (training, tuning, serving, etc.).</p> <p>To address these needs, we'll be using Ray, a distributed computing framework that makes it easy to scale your Python applications. It's a general purpose framework that can be used for a variety of applications but we'll be using it for our data processing operations first (and more later). And it also has great integrations with the previously mentioned distributed data processing frameworks (Dask, Modin, Spark).</p>"},{"location":"courses/mlops/distributed-data/#setup","title":"Setup","text":"<p>The only setup we have to do is set Ray to preserve order when acting on our data. This is important for ensuring reproducible and deterministic results.</p> <pre><code>ray.data.DatasetContext.get_current().execution_options.preserve_order = True  # deterministic\n</code></pre>"},{"location":"courses/mlops/distributed-data/#ingestion","title":"Ingestion","text":"<p>We'll start by ingesting our dataset. Ray has a range of input/output functions that supports all major data formats and sources.</p> <pre><code># Data ingestion\nds = ray.data.read_csv(DATASET_LOC)\nds = ds.random_shuffle(seed=1234)\nds.take(1)\n</code></pre> <pre>\n[{'id': 2166,\n  'created_on': datetime.datetime(2020, 8, 17, 5, 19, 41),\n  'title': 'Pix2Pix',\n  'description': 'Tensorflow 2.0 Implementation of the paper Image-to-Image Translation using Conditional GANs by Philip Isola, Jun-Yan Zhu, Tinghui Zhou and Alexei A. Efros.',\n  'tag': 'computer-vision'}]\n</pre>"},{"location":"courses/mlops/distributed-data/#splitting","title":"Splitting","text":"<p>Next, we'll split our dataset into our training and validation splits. Ray has a built-in <code>train_test_split</code> function but we're using a modified version so that we can stratify our split based on the <code>tag</code> column.</p> <pre><code>import sys\nsys.path.append(\"..\")\nfrom madewithml.data import stratify_split\n</code></pre> <pre><code># Split dataset\ntest_size = 0.2\ntrain_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)\n</code></pre>"},{"location":"courses/mlops/distributed-data/#preprocessing","title":"Preprocessing","text":"<p>And finally, we're ready to preprocess our data splits. One of the advantages of using Ray is that we won't have to change anything to our original Pandas-based preprocessing function we implemented in the previous lesson. Instead, we can use it directly with Ray's <code>map_batches</code> utility to map our preprocessing function across batches in our data in a distributed manner.</p> <pre><code># Mapping\ntags = train_ds.unique(column=\"tag\")\nclass_to_index = {tag: i for i, tag in enumerate(tags)}\n</code></pre> <pre><code># Distributed preprocessing\nsample_ds = train_ds.map_batches(\n  preprocess,\n  fn_kwargs={\"class_to_index\": class_to_index},\n  batch_format=\"pandas\")\nsample_ds.show(1)\n</code></pre> <pre>\n{'ids': array([  102,  5800, 14982,  1422,  4958, 14982,   437,  3294,  3577,\n       12574,  2747,  1262,  7222,   103,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0]), 'masks': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0]), 'targets': 2}\n</pre> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Distributed - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/docker/","title":"Docker","text":""},{"location":"courses/mlops/docker/#intuition","title":"Intuition","text":"<p>The last step in achieving reproducibility is to deploy our versioned code and artifacts in a reproducible environment. This goes well beyond the virtual environment we configured for our Python applications because there are system-level specifications (operating system, required implicit packages, etc.) we aren't capturing. We want to be able to encapsulate all the requirements we need so that there are no external dependencies that would prevent someone else from reproducing our exact application.</p>"},{"location":"courses/mlops/docker/#docker","title":"Docker","text":"<p>There are actually quite a few solutions for system-level reproducibility (VMs, container engines, etc.) but the Docker container engine is by far the most popular for several key advantages:</p> <ul> <li>reproducibility via Dockerfile with explicit instructions to deploy our application in a specific system.</li> <li>isolation via containers as to not affect other applications that may also run on the same underlying operating system.</li> <li>and many more advantages including size (no separate OS needed for each application), speed, Docker Hub, etc.</li> </ul> <p>We're going to use Docker to deploy our application locally in an isolated, reproducible and scalable fashion. Once we do this, any machine with the Docker engine installed can reproduce our work. However, there is so much more to Docker, which you can explore in the docs, that goes beyond what we'll need.</p>"},{"location":"courses/mlops/docker/#architecture","title":"Architecture","text":"<p>Before we install Docker, let's take a look at how the container engine works on top our operating system, which can be our local hardware or something managed on the cloud.</p> <p>The Docker container engine is responsible for spinning up configured containers, which contains our application and it's dependencies (binaries, libraries, etc.). The container engine is very efficient in that it doesn't need to create a separate operating system for each containerized application. This also means that our containers can share the system's resources via the Docker engine.</p>"},{"location":"courses/mlops/docker/#set-up","title":"Set up","text":"<p>Now we're ready to install Docker based on our operating system. Once installed, we can start the Docker Desktop which will allow us to create and deploy our containerized applications.</p> <pre><code>docker --version\n</code></pre> <pre>\nDocker version 20.10.8, build 3967b7d\n</pre>"},{"location":"courses/mlops/docker/#images","title":"Images","text":"<p>The first step is to build a docker image which has the application and all it's specified dependencies. We can create this image using a Dockerfile which outlines a set of instructions. These instructions essentially build read-only image layers on top of each other to construct our entire image. Let's take a look at our application's Dockerfile and the image layers it creates.</p>"},{"location":"courses/mlops/docker/#dockerfile","title":"Dockerfile","text":"<p>We'll start by creating a Dockerfile:</p> <pre><code>touch Dockerfile\n</code></pre> <p>The first line we'll write in our <code>Dockerfile</code> specifies the base image we want to pull FROM. Here we want to use the base image for running Python based applications and specifically for Python 3.7 with the slim variant. Since we're only deploying a Python application, this slim variant with minimal packages satisfies our requirements while keeping the size of the image layer low.</p> <pre><code># Base image\nFROM python:3.7-slim\n</code></pre> <p>Next we're going to install our application dependencies. First, we'll COPY the required files from our local file system so we can use them for installation. Alternatively, if we were running on some remote infrastructure, we could've pulled from a remote git host. Once we have our files, we can install the packages required to install our application's dependencies using the RUN command. Once we're done using the packages, we can remove them to keep our image layer's size to a minimum.</p> <pre><code># Install dependencies\nWORKDIR /mlops\nCOPY setup.py setup.py\nCOPY requirements.txt requirements.txt\nRUN apt-get update \\\n&amp;&amp; apt-get install -y --no-install-recommends gcc build-essential \\\n&amp;&amp; rm -rf /var/lib/apt/lists/* \\\n&amp;&amp; python3 -m pip install --upgrade pip setuptools wheel \\\n&amp;&amp; python3 -m pip install -e . --no-cache-dir \\\n&amp;&amp; python3 -m pip install protobuf==3.20.1 --no-cache-dir \\\n&amp;&amp; apt-get purge -y --auto-remove gcc build-essential\n</code></pre> <p>Next we're ready to COPY over the required files to actually RUN our application.</p> <pre><code># Copy\nCOPY tagifai tagifai\nCOPY app app\nCOPY data data\nCOPY config config\nCOPY stores stores\n\n# Pull assets from S3\nRUN dvc init --no-scm\nRUN dvc remote add -d storage stores/blob\nRUN dvc pull\n</code></pre> <p>Since our application (API) requires PORT 8000 to be open, we need to specify in our Dockerfile to expose it. <pre><code># Export ports\nEXPOSE 8000\n</code></pre></p> <p>The final step in building our image is to specify the executable to be run when a container is built from our image. For our application, we want to launch our API with gunicorn since this Dockerfile may be used to deploy our service to production at scale.</p> <pre><code># Start app\nENTRYPOINT [\"gunicorn\", \"-c\", \"app/gunicorn.py\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"app.api:app\"]\n</code></pre> <p>There are many more commands available for us to use in the Dockerfile, such as using environment variables (ENV) and arguments (ARG), command arguments (CMD), specifying volumes (VOLUME), setting the working directory (WORKDIR) and many more, all of which you can explore through the official docs.</p>"},{"location":"courses/mlops/docker/#build-images","title":"Build images","text":"<p>Once we're done composing the Dockerfile, we're ready to build our image using the build command which allows us to add a tag and specify the location of the Dockerfile to use.</p> <pre><code>docker build -t tagifai:latest -f Dockerfile .\n</code></pre> <p>We can inspect all built images and their attributes like so: <pre><code>docker images\n</code></pre></p> <pre>\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\ntagifai      latest    02c88c95dd4c   23 minutes ago   2.57GB\n</pre> <p>We can also remove any or all images based on their unique IDs.</p> <pre><code>docker rmi &lt;IMAGE_ID&gt;              # remove an image\ndocker rmi $(docker images -a -q)  # remove all images\n</code></pre>"},{"location":"courses/mlops/docker/#run-containers","title":"Run containers","text":"<p>Once we've built our image, we're ready to run a container using that image with the run command which allows us to specify the image, port forwarding, etc.</p> <pre><code>docker run -p 8000:8000 --name tagifai tagifai:latest\n</code></pre> <p>Once we have our container running, we can use the API thanks for the port we're sharing (8000):</p> <pre><code>curl -X 'POST' \\\n'http://localhost:8000/predict' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n  \"texts\": [\n    {\n      \"text\": \"Transfer learning with transformers for text classification.\"\n    }\n  ]\n}'\n</code></pre> <p>We can inspect all containers (running or stopped) like so: <pre><code>docker ps     # running containers\ndocker ps -a  # stopped containers\n</code></pre></p> <pre>\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS    PORTS                    NAMES\nee5f1b08abd5   tagifai:latest   \"gunicorn -c config\u2026\"    19 minutes ago   Created   0.0.0.0:8000-&gt;8000/tcp   tagifai\n</pre> <p>We can also stop and remove any or all containers based on their unique IDs:</p> <pre><code>docker stop &lt;CONTAINER_ID&gt;      # stop a running container\ndocker rm &lt;CONTAINER_ID&gt;        # remove a container\ndocker stop $(docker ps -a -q)  # stop all containers\ndocker rm $(docker ps -a -q)    # remove all containers\n</code></pre> <p>If our application required multiple containers for different services (API, database, etc.) then we can bring them all up at once using the docker compose functionality and scale and manage them using a container orchestration system like Kubernetes (K8s). If we're specifically deploying ML workflows, we can use a toolkit like KubeFlow to help us manage and scale.</p>"},{"location":"courses/mlops/docker/#debug","title":"Debug","text":"<p>In the event that we run into errors while building our image layers, a very easy way to debug the issue is to run the container with the image layers that have been build so far. We can do this by only including the commands that have ran successfully so far (and all COPY statements) in the <code>Dockerfile</code>. And then we need to rebuild the image (since we altered the Dockerfile) and run the container:</p> <pre><code>docker build -t tagifai:latest -f Dockerfile .\ndocker run -p 8000:8000 -it tagifai /bin/bash\n</code></pre> <p>Once we have our container running, we can use our application as we would on our local machine but now it's reproducible on any operating system that can run the Docker container engine. We've covered just what we need from Docker to deploy our application but there is so much more to Docker, which you can explore in the docs.</p>"},{"location":"courses/mlops/docker/#production","title":"Production","text":"<p>This <code>Dockerfile</code> is commonly the end artifact a data scientist or ML engineer delivers to their DevOps teams to deploy and scale their services, with a few changes:</p> <ul> <li>data assets would be pulled from a remote storage location (ex. S3).</li> <li>model artifacts would be loaded from a remote model registry.</li> <li>code would be loaded from a remote repository (ex. GitHub) via <code>git clone</code>.</li> </ul> <p>All of these changes would involve using the proper credentials (via encrypted secrets and can even be automatically deployed via CI/CD workflows. But, of course, there are subsequent responsibilities such as monitoring.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Docker - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/documentation/","title":"Documenting Code","text":""},{"location":"courses/mlops/documentation/#intuition","title":"Intuition","text":"<p>Code tells you how, comments tell you why. -- Jeff Atwood</p> <p>We can really improve the quality of our codebase by documenting it to make it easier for others (and our future selves) to easily navigate and extend it. We know our code base best the moment we finish writing it but fortunately documenting it will allow us to quickly get back to that familiar state of mind. Documentation can mean many different things to developers, so let's define the most common components:</p> <ul> <li><code>comments</code>: short descriptions as to why a piece of code exists.</li> <li><code>typing</code>: specification of a function's inputs and outputs' data types, providing information pertaining to what a function consumes and produces.</li> <li><code>docstrings</code>: meaningful descriptions for functions and classes that describe overall utility, arguments, returns, etc.</li> <li><code>docs</code>: rendered webpage that summarizes all the functions, classes, workflows, examples, etc.</li> </ul>"},{"location":"courses/mlops/documentation/#typing","title":"Typing","text":"<p>It's important to be as explicit as possible with our code. We've already discussed choosing explicit names for variables, functions but another way we can be explicit is by defining the types for our function's inputs and outputs by using the typing library.</p> <p>So far, our functions have looked like this: <pre><code>def some_function(a, b):\n    return c\n</code></pre></p> <p>But we can incorporate so much more information using typing: <pre><code>from typing import List\ndef some_function(a: List, b: int = 0) -&gt; np.ndarray:\n    return c\n</code></pre></p> <p>Here we've defined:</p> <ul> <li>input parameter <code>a</code> is a list</li> <li>input parameter <code>b</code> is an integer with default value 0</li> <li>output parameter <code>c</code> is a NumPy array</li> </ul> <p>There are many other data types that we can work with, including <code>List</code>, <code>Set</code>, <code>Dict</code>, <code>Tuple</code>, <code>Sequence</code> and more, as well as included types such as <code>int</code>, <code>float</code>, etc. You can also use types from packages we install (ex. <code>np.ndarray</code>) and even from our own defined classes (ex. <code>LabelEncoder</code>).</p> <p>Starting from Python 3.9+, common types are built in so we don't need to import them with <code>from typing import List, Set, Dict, Tuple, Sequence</code> anymore.</p>"},{"location":"courses/mlops/documentation/#docstrings","title":"Docstrings","text":"<p>We can make our code even more explicit by adding docstrings to describe overall utility, arguments, returns, exceptions and more. Let's take a look at an example:</p> <pre><code>from typing import List\ndef some_function(a: List, b: int = 0) -&gt; np.ndarray:\n\"\"\"Function description.\n\n    ```python\n    c = some_function(a=[], b=0)\n    print (c)\n    ```\n    &lt;pre&gt;\n    [[1 2]\n     [3 4]]\n    &lt;/pre&gt;\n\n    Args:\n        a (List): description of `a`.\n        b (int, optional): description of `b`. Defaults to 0.\n\n    Raises:\n        ValueError: Input list is not one-dimensional.\n\n    Returns:\n        np.ndarray: Description of `c`.\n\n    \"\"\"\n    return c\n</code></pre> <p>Let's unpack the different parts of this function's docstring:</p> <ul> <li><code>[Line 3]</code>: Summary of the overall utility of the function.</li> <li><code>[Lines 5-12]</code>: Example of how to use our function.</li> <li><code>[Lines 14-16]</code>: Description of the function's input arguments.</li> <li><code>[Lines 18-19]</code>: Any exceptions that may be raised in the function.</li> <li><code>[Lines 21-22]</code>: Description of the function's output(s).</li> </ul> <p>We'll render these docstrings in the docs section below to produce this:</p> <p>Take a look at the docstrings of different functions and classes in our repository.</p> <pre><code># madewithml/data.py\nfrom typing import List\n\ndef clean_text(text: str, stopwords: List = STOPWORDS) -&gt; str:\n\"\"\"Clean raw text string.\n    Args:\n        text (str): Raw text to clean.\n        stopwords (List, optional): list of words to filter out. Defaults to STOPWORDS.\n\n    Returns:\n        str: cleaned text.\n    \"\"\"\n    pass\n</code></pre> <p>Tip</p> <p>If using Visual Studio Code, be sure to use the Python Docstrings Generator extension so you can type <code>\"\"\"</code> under a function and then hit the Shift key to generate a template docstring. It will autofill parts of the docstring using the typing information and even exception in your code!</p> <p></p>"},{"location":"courses/mlops/documentation/#docs","title":"Docs","text":"<p>So we're going through all this effort of including typing and docstrings to our functions but it's all tucked away inside our scripts. What if we can collect all this effort and automatically surface it as documentation? Well that's exactly what we'll do with the following open-source packages \u2192 final result here.</p> <ol> <li> <p>Initialize mkdocs <pre><code>python3 -m mkdocs new .\n</code></pre> This will create the following files: <pre><code>.\n\u251c\u2500 docs/\n\u2502  \u2514\u2500 index.md\n\u2514\u2500 mkdocs.yml\n</code></pre></p> </li> <li> <p>We'll start by overwriting the default <code>index.md</code> file in our <code>docs</code> directory with information specific to our project: index.md<pre><code>## Documentation\n- [madewithml](madewithml/config.md): documentation for functions and classes.\n\n## Course\nLearn how to combine machine learning with software engineering to design, develop, deploy and iterate on production ML applications.\n\n- Lessons: [https://madewithml.com/](https://madewithml.com/#course)\n- Code: [GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)\n</code></pre></p> </li> <li> <p>Next we'll create documentation files for each script in our <code>madewithml</code> directory: <pre><code>mkdir docs/madewithml\ncd docs/madewithml\ntouch config.md data.md evaluate.md models.md predict.md serve.md train.md tune.md util.md\ncd ../../\n</code></pre></p> </li> </ol> <p>Tip</p> <p>It's helpful to have the <code>docs</code> directory structure mimic our project's structure as much as possible.</p> <ol> <li> <p>Next we'll add <code>madewithml.&lt;SCRIPT_NAME&gt;</code> to each file under <code>docs/madewithml</code>. This will populate the file with information about the functions and classes (using their docstrings) from <code>madewithml/&lt;SCRIPT_NAME&gt;.py</code> thanks to the <code>mkdocstrings</code> plugin.</p> <p>Be sure to check out the complete list of mkdocs plugins. <pre><code># docs/madewithml/data.md\n::: madewithml.data\n</code></pre></p> </li> <li> <p>Finally, we'll add some configurations to our <code>mkdocs.yml</code> file that mkdocs automatically created: <pre><code>site_name: Made With ML\nsite_url: https://madewithml.com/\nrepo_url: https://github.com/GokuMohandas/Made-With-ML/\nnav:\n- Home: index.md\n- madewithml:\n- data: madewithml/data.md\n- models: madewithml/models.md\n- train: madewithml/train.md\n- tune: madewithml/tune.md\n- evaluate: madewithml/evaluate.md\n- predict: madewithml/predict.md\n- serve: madewithml/serve.md\n- utils: madewithml/utils.md\ntheme: readthedocs\nplugins:\n- mkdocstrings\nwatch:\n- .  # reload docs for any file changes\n</code></pre></p> </li> <li> <p>Serve our documentation locally: <pre><code>python3 -m mkdocs serve\n</code></pre></p> </li> </ol> <p>This will serve our docs at http://localhost:8000/:</p>"},{"location":"courses/mlops/documentation/#publishing","title":"Publishing","text":"<p>We can easily serve our documentation for free using GitHub pages for public repositories as wells as private documentation for private repositories. And we can even host it on a custom domain (ex. company's subdomain).</p> <p>Be sure to check out the auto-generated documentation page for our repository. We'll learn how to automatically generate and update this docs page every time we make changes to our codebase later in our CI/CD lesson.</p> <p>In the next lesson, we'll learn how to style and format our codebase in a consistent manner.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Documentation - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/evaluation/","title":"Evaluating Machine Learning Models","text":""},{"location":"courses/mlops/evaluation/#intuition","title":"Intuition","text":"<p>Evaluation is an integral part of modeling and it's one that's often glossed over. We'll often find evaluation to involve simply computing the accuracy or other global metrics but for many real work applications, a much more nuanced evaluation process is required. However, before evaluating our model, we always want to:</p> <ul> <li>be clear about what metrics we are prioritizing</li> <li>be careful not to over optimize on any one metric because it may mean you're compromising something else</li> </ul>"},{"location":"courses/mlops/evaluation/#setup","title":"Setup","text":"<p>Let's start by setting up our metrics dictionary that we'll fill in as we go along and all the data we'll need for evaluation: grounds truth labels (<code>y_test</code>, predicted labels (<code>y_pred</code>) and predicted probabilities (<code>y_prob</code>).</p> <p><pre><code># Metrics\nmetrics = {\"overall\": {}, \"class\": {}}\n</code></pre> <pre><code># y_test\npreprocessor = predictor.get_preprocessor()\npreprocessed_ds = preprocessor.transform(test_ds)\nvalues = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\ny_test = np.stack([item[\"targets\"] for item in values])\n</code></pre> <pre><code># y_pred\ntest_df = test_ds.to_pandas()\nz = predictor.predict(data=test_df)[\"predictions\"]  # adds text column (in-memory)\ny_pred = np.stack(z).argmax(1)\n</code></pre> <pre><code># y_prob\ny_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\nprint (np.shape(y_test))\nprint (np.shape(y_prob))\n</code></pre> <pre><code># Add columns (for convenience)\ntest_df = test_ds.to_pandas()\ntest_df[\"text\"] = test_df[\"title\"] + \" \" + test_df[\"description\"]\ntest_df[\"prediction\"] = test_df.index.map(lambda i: preprocessor.index_to_class[y_pred[i]])\ntest_df.head()\n</code></pre></p> id created_on title description tag text prediction 0 19 2020-03-03 13:54:31 Diffusion to Vector Reference implementation of Diffusion2Vec (Com... other Diffusion to Vector Reference implementation o... other 1 26 2020-03-07 23:11:58 Graph Wavelet Neural Network A PyTorch implementation of \"Graph Wavelet Neu... other Graph Wavelet Neural Network A PyTorch impleme... other 2 44 2020-03-08 00:32:58 Capsule Graph Neural Network A PyTorch implementation of \"Capsule Graph Neu... other Capsule Graph Neural Network A PyTorch impleme... other 3 80 2020-03-20 05:59:32 NeRF: Neural Radiance Fields Representing scenes as neural radiance fields ... computer-vision NeRF: Neural Radiance Fields Representing scen... computer-vision 4 84 2020-03-20 15:18:43 Mention Classifier Category prediction model\\r\\nThis repo contain... natural-language-processing Mention Classifier Category prediction model\\r... natural-language-processing"},{"location":"courses/mlops/evaluation/#coarse-grained","title":"Coarse-grained","text":"<p>While we were developing our models, our evaluation process involved computing the coarse-grained metrics such as overall precision, recall and f1 metrics.</p> <ul> <li>True positives (TP): we correctly predicted class X.</li> <li>False positives (FP): we incorrectly predicted class X but it was another class.</li> <li>True negatives (TN): we correctly predicted that it's wasn't the class X.</li> <li>False negatives (FN): we incorrectly predicted that it wasn't the class X but it was.</li> </ul> \\[ \\text{precision} = \\frac{TP}{TP + FP} \\] \\[ \\text{recall} = \\frac{TP}{TP + FN} \\] \\[ \\text{f1} = \\frac{2 * precision * recall}{precision + recall} \\] <p><pre><code>from sklearn.metrics import precision_recall_fscore_support\n</code></pre> <pre><code># Overall metrics\noverall_metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\nmetrics[\"overall\"][\"precision\"] = overall_metrics[0]\nmetrics[\"overall\"][\"recall\"] = overall_metrics[1]\nmetrics[\"overall\"][\"f1\"] = overall_metrics[2]\nmetrics[\"overall\"][\"num_samples\"] = np.float64(len(y_test))\nprint (json.dumps(metrics[\"overall\"], indent=4))\n</code></pre></p> <pre>\n{\n    \"precision\": 0.916248340770615,\n    \"recall\": 0.9109947643979057,\n    \"f1\": 0.9110623702438432,\n    \"num_samples\": 191.0\n}\n</pre> <p>Note</p> <p>The precision_recall_fscore_support() function from scikit-learn has an input parameter called <code>average</code> which has the following options below. We'll be using the different averaging methods for different metric granularities.</p> <ul> <li><code>None</code>: metrics are calculated for each unique class.</li> <li><code>binary</code>: used for binary classification tasks where the <code>pos_label</code> is specified.</li> <li><code>micro</code>: metrics are calculated using global TP, FP, and FN.</li> <li><code>macro</code>: per-class metrics which are averaged without accounting for class imbalance.</li> <li><code>weighted</code>: per-class metrics which are averaged by accounting for class imbalance.</li> <li><code>samples</code>: metrics are calculated at the per-sample level.</li> </ul>"},{"location":"courses/mlops/evaluation/#fine-grained","title":"Fine-grained","text":"<p>Inspecting these coarse-grained, overall metrics is a start but we can go deeper by evaluating the same fine-grained metrics at the categorical feature levels.</p> <p><pre><code>from collections import OrderedDict\n</code></pre> <pre><code># Per-class metrics\nclass_metrics = precision_recall_fscore_support(y_test, y_pred, average=None)\nfor i, _class in enumerate(preprocessor.class_to_index):\n    metrics[\"class\"][_class] = {\n        \"precision\": class_metrics[0][i],\n        \"recall\": class_metrics[1][i],\n        \"f1\": class_metrics[2][i],\n        \"num_samples\": np.float64(class_metrics[3][i]),\n    }\n</code></pre> <pre><code># Metrics for a specific class\ntag=\"natural-language-processing\"\nprint (json.dumps(metrics[\"class\"][tag], indent=2))\n</code></pre></p> <pre>\n{\n  \"precision\": 0.9036144578313253,\n  \"recall\": 0.9615384615384616,\n  \"f1\": 0.9316770186335404,\n  \"num_samples\": 78.0\n}\n</pre> <pre><code># Sorted tags\nsorted_tags_by_f1 = OrderedDict(sorted(\n        metrics[\"class\"].items(), key=lambda tag: tag[1][\"f1\"], reverse=True))\nfor item in sorted_tags_by_f1.items():\n    print (json.dumps(item, indent=2))\n</code></pre> <pre>\n[\n  \"natural-language-processing\",\n  {\n    \"precision\": 0.9036144578313253,\n    \"recall\": 0.9615384615384616,\n    \"f1\": 0.9316770186335404,\n    \"num_samples\": 78.0\n  }\n]\n[\n  \"computer-vision\",\n  {\n    \"precision\": 0.9838709677419355,\n    \"recall\": 0.8591549295774648,\n    \"f1\": 0.9172932330827067,\n    \"num_samples\": 71.0\n  }\n]\n[\n  \"other\",\n  {\n    \"precision\": 0.8333333333333334,\n    \"recall\": 0.9615384615384616,\n    \"f1\": 0.8928571428571429,\n    \"num_samples\": 26.0\n  }\n]\n[\n  \"mlops\",\n  {\n    \"precision\": 0.8125,\n    \"recall\": 0.8125,\n    \"f1\": 0.8125,\n    \"num_samples\": 16.0\n  }\n]\n</pre>"},{"location":"courses/mlops/evaluation/#confusion-matrix","title":"Confusion matrix","text":"<p>Besides just inspecting the metrics for each class, we can also identify the true positives, false positives and false negatives. Each of these will give us insight about our model beyond what the metrics can provide.</p> <ul> <li>True positives (TP): learn about where our model performs well.</li> <li>False positives (FP): potentially identify samples which may need to be relabeled.</li> <li>False negatives (FN): identify the model's less performant areas to oversample later.</li> </ul> <p>It's a good to have our FP/FN samples feed back into our annotation pipelines in the event we want to fix their labels and have those changes be reflected everywhere.</p> <p><pre><code># TP, FP, FN samples\ntag = \"natural-language-processing\"\nindex = preprocessor.class_to_index[tag]\ntp, fp, fn = [], [], []\nfor i, true in enumerate(y_test):\n    pred = y_pred[i]\n    if index==true==pred:\n        tp.append(i)\n    elif index!=true and index==pred:\n        fp.append(i)\n    elif index==true and index!=pred:\n        fn.append(i)\n</code></pre> <pre><code>print (tp)\nprint (fp)\nprint (fn)\n</code></pre></p> <pre>\n[4, 9, 12, 17, 19, 23, 25, 26, 29, 30, 31, 32, 33, 34, 42, 47, 49, 50, 54, 56, 65, 66, 68, 71, 75, 76, 77, 78, 79, 82, 92, 94, 95, 97, 99, 101, 109, 113, 114, 118, 120, 122, 126, 128, 129, 130, 131, 133, 134, 135, 138, 139, 140, 141, 142, 144, 148, 149, 152, 159, 160, 161, 163, 166, 170, 172, 173, 174, 177, 179, 183, 184, 187, 189, 190]\n[41, 44, 73, 102, 110, 150, 154, 165]\n[16, 112, 115]\n</pre> <pre><code># Samples\nnum_samples = 3\ncm = [(tp, \"True positives\"), (fp, \"False positives\"), (fn, \"False negatives\")]\nfor item in cm:\n    if len(item[0]):\n        print (f\"\\n=== {item[1]} ===\")\n        for index in item[0][:num_samples]:\n            print (f\"{test_df.iloc[index].text}\")\n            print (f\"    true: {test_df.tag[index]}\")\n            print (f\"    pred: {test_df.prediction[index]}\\n\")\n</code></pre> <pre>\n=== True positives ===\nMention Classifier Category prediction model\nThis repo contains AllenNLP model for prediction of Named Entity categories by its mentions.\n    true: natural-language-processing\n    pred: natural-language-processing\n\nFinetune: Scikit-learn Style Model Finetuning for NLP Finetune is a library that allows users to leverage state-of-the-art pretrained NLP models for a wide variety of downstream tasks.\n    true: natural-language-processing\n    pred: natural-language-processing\n\nFinetuning Transformers with JAX + Haiku Walking through a port of the RoBERTa pre-trained model to JAX + Haiku, then fine-tuning the model to solve a downstream task.\n    true: natural-language-processing\n    pred: natural-language-processing\n\n\n=== False positives ===\nHow Docker Can Help You Become A More Effective Data Scientist A look at Docker from the perspective of a data scientist.\n    true: mlops\n    pred: natural-language-processing\n\nTransfer Learning &amp; Fine-Tuning With Keras Your 100% up-to-date guide to transfer learning &amp; fine-tuning with Keras.\n    true: computer-vision\n    pred: natural-language-processing\n\nExploratory Data Analysis on MS COCO Style Datasets A Simple Toolkit to do exploratory data analysis on MS COCO style formatted datasets.\n    true: computer-vision\n    pred: natural-language-processing\n\n\n=== False negatives ===\nThe Unreasonable Effectiveness of Recurrent Neural Networks A close look at how RNNs are able to perform so well.\n    true: natural-language-processing\n    pred: other\n\nMachine Learning Projects  This Repo contains projects done by me while learning the basics. All the familiar types of regression, classification, and clustering methods have been used.\n    true: natural-language-processing\n    pred: other\n\nBERT Distillation with Catalyst How to distill BERT with Catalyst.\n    true: natural-language-processing\n    pred: mlops\n\n</pre> <p>Tip</p> <p>It's a really good idea to do this kind of analysis using our rule-based approach to catch really obvious labeling errors.</p>"},{"location":"courses/mlops/evaluation/#confidence-learning","title":"Confidence learning","text":"<p>While the confusion-matrix sample analysis was a coarse-grained process, we can also use fine-grained confidence based approaches to identify potentially mislabeled samples. Here we\u2019re going to focus on the specific labeling quality as opposed to the final model predictions.</p> <p>Simple confidence based techniques include identifying samples whose:</p> <ul> <li> <p>Categorical</p> <ul> <li>prediction is incorrect (also indicate TN, FP, FN)</li> <li>confidence score for the correct class is below a threshold</li> <li>confidence score for an incorrect class is above a threshold</li> <li>standard deviation of confidence scores over top N samples is low</li> <li>different predictions from same model using different parameters</li> </ul> </li> <li> <p>Continuous</p> <ul> <li>difference between predicted and ground-truth values is above some %</li> </ul> </li> </ul> <pre><code># Tag to inspect\ntag = \"natural-language-processing\"\nindex = class_to_index[tag]\nindices = np.where(y_test==index)[0]\n</code></pre> <pre><code># Confidence score for the correct class is below a threshold\nlow_confidence = []\nmin_threshold = 0.5\nfor i in indices:\n    prob = y_prob[i][index]\n    if prob &lt;= 0.5:\n        low_confidence.append({\n            \"text\": f\"{test_df.iloc[i].text}\",\n            \"true\": test_df.tag[i],\n            \"pred\": test_df.prediction[i],\n            \"prob\": prob})\n</code></pre> <pre><code>low_confidence[0:3]\n</code></pre> <pre>\n[{'text': 'The Unreasonable Effectiveness of Recurrent Neural Networks A close look at how RNNs are able to perform so well.',\n  'true': 'natural-language-processing',\n  'pred': 'other',\n  'prob': 0.0023471832},\n {'text': 'Machine Learning Projects  This Repo contains projects done by me while learning the basics. All the familiar types of regression, classification, and clustering methods have been used.',\n  'true': 'natural-language-processing',\n  'pred': 'other',\n  'prob': 0.0027675298},\n {'text': 'BERT Distillation with Catalyst How to distill BERT with Catalyst.',\n  'true': 'natural-language-processing',\n  'pred': 'mlops',\n  'prob': 0.37908182}]\n</pre> <p>But these are fairly crude techniques because neural networks are easily overconfident and so their confidences cannot be used without calibrating them.</p> Modern (large) neural networks result in higher accuracies but are over confident.On Calibration of Modern Neural Networks <ul> <li>Assumption: \u201cthe probability associated with the predicted class label should reflect its ground truth correctness likelihood.\u201d</li> <li>Reality: \u201cmodern (large) neural networks are no longer well-calibrated\u201d</li> <li>Solution: apply temperature scaling (extension of Platt scaling) on model outputs</li> </ul> <p>Recent work on confident learning (cleanlab) focuses on identifying noisy labels (with calibration), which can then be properly relabeled and used for training.</p> <p><pre><code>import cleanlab\nfrom cleanlab.filter import find_label_issues\n</code></pre> <pre><code># Find label issues\nlabel_issues = find_label_issues(labels=y_test, pred_probs=y_prob, return_indices_ranked_by=\"self_confidence\")\ntest_df.iloc[label_issues].drop(columns=[\"text\"]).head()\n</code></pre></p> id created_on title description tag prediction 165 2137 2020-08-13 02:10:03 Unpopular Opinion - Data Scientists Should Be ... I believe data scientists can be more effectiv... mlops natural-language-processing 154 1976 2020-07-27 14:12:03 Close-Domain fine-tuning for table detection In this project, we show the benefits of using... computer-vision natural-language-processing 16 264 2020-04-06 21:33:32 The Unreasonable Effectiveness of Recurrent Ne... A close look at how RNNs are able to perform s... natural-language-processing other 103 1459 2020-06-16 03:06:10 SuperGlue: Learning Feature Matching with Grap... SuperGlue, a neural network that matches two s... other computer-vision 112 1524 2020-06-20 10:42:25 Machine Learning Projects This Repo contains projects done by me while l... natural-language-processing other <p>Not all of these are necessarily labeling errors but situations where the predicted probabilities were not so confident. Therefore, it will be useful to attach the predicted outcomes along side results. This way, we can know if we need to relabel, upsample, etc. as mitigation strategies to improve our performance.</p> <p>The operations in this section can be applied to entire labeled dataset to discover labeling errors via confidence learning.</p>"},{"location":"courses/mlops/evaluation/#slicing","title":"Slicing","text":"<p>Just inspecting the overall and class metrics isn't enough to deploy our new version to production. There may be key slices of our dataset that we need to do really well on:</p> <ul> <li>Target / predicted classes (+ combinations)</li> <li>Features (explicit and implicit)</li> <li>Metadata (timestamps, sources, etc.)</li> <li>Priority slices / experience (minority groups, large users, etc.)</li> </ul> <p>An easy way to create and evaluate slices is to define slicing functions.</p> <pre><code>from snorkel.slicing import PandasSFApplier\nfrom snorkel.slicing import slice_dataframe\nfrom snorkel.slicing import slicing_function\n</code></pre> <p><pre><code>@slicing_function()\ndef nlp_llm(x):\n\"\"\"NLP projects that use LLMs.\"\"\"\n    nlp_project = \"natural-language-processing\" in x.tag\n    llm_terms = [\"transformer\", \"llm\", \"bert\"]\n    llm_project = any(s.lower() in x.text.lower() for s in llm_terms)\n    return (nlp_project and llm_project)\n</code></pre> <pre><code>@slicing_function()\ndef short_text(x):\n\"\"\"Projects with short titles and descriptions.\"\"\"\n    return len(x.text.split()) &lt; 8  # less than 8 words\n</code></pre></p> <p>Here we're using Snorkel's <code>slicing_function</code> to create our different slices. We can visualize our slices by applying this slicing function to a relevant DataFrame using <code>slice_dataframe</code>.</p> <pre><code>nlp_llm_df = slice_dataframe(test_df, nlp_llm)\nnlp_llm_df[[\"text\", \"tag\"]].head()\n</code></pre> text tag 12 Finetuning Transformers with JAX + Haiku Walki... natural-language-processing 19 Question Answering with a Fine-Tuned BERT What... natural-language-processing 29 BertViz Tool for visualizing attention in the ... natural-language-processing 30 The Transformer Family This post presents how ... natural-language-processing 31 Pruning Bert to Accelerate Inference After pre... natural-language-processing <pre><code>short_text_df = slice_dataframe(test_df, short_text)\nshort_text_df[[\"text\", \"tag\"]].head()\n</code></pre> text tag 75 NLPAug Data augmentation for NLP natural-language-processing 123 Offline Reinforcement Learning Challenges, alg... other 127 Image Classifier Pure JavaScript Image Classifier computer-vision 132 imgaug Image augmentation for machine learning... computer-vision 140 QSVM Quantum SVM for sentiment analysis natural-language-processing <p>We can define even more slicing functions and create a slices record array using the <code>PandasSFApplier</code>. The slices array has N (# of data points) items and each item has S (# of slicing functions) items, indicating whether that data point is part of that slice. Think of this record array as a masking layer for each slicing function on our data.</p> <pre><code># Slices\nslicing_functions = [nlp_llm, short_text]\napplier = PandasSFApplier(slicing_functions)\nslices = applier.apply(test_df)\nslices\n</code></pre> <pre>\nrec.array([(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0),\n           (1, 0) (0, 0) (0, 1) (0, 0) (0, 0) (1, 0) (0, 0) (0, 0) (0, 1) (0, 0)\n           ...\n           (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1),\n           (0, 0), (0, 0)],\n    dtype=[('nlp_cnn', '&lt;i8'), ('short_text', '&lt;i8')])\n</pre> <p>To calculate metrics for our slices, we could use snorkel.analysis.Scorer but we've implemented a version that will work for multiclass or multilabel scenarios.</p> <pre><code># Score slices\nmetrics[\"slices\"] = {}\nfor slice_name in slices.dtype.names:\n    mask = slices[slice_name].astype(bool)\n    if sum(mask):\n        slice_metrics = precision_recall_fscore_support(\n            y_test[mask], y_pred[mask], average=\"micro\"\n        )\n        metrics[\"slices\"][slice_name] = {}\n        metrics[\"slices\"][slice_name][\"precision\"] = slice_metrics[0]\n        metrics[\"slices\"][slice_name][\"recall\"] = slice_metrics[1]\n        metrics[\"slices\"][slice_name][\"f1\"] = slice_metrics[2]\n        metrics[\"slices\"][slice_name][\"num_samples\"] = len(y_test[mask])\n</code></pre> <pre><code>print(json.dumps(metrics[\"slices\"], indent=2))\n</code></pre> <pre>\n{\n  \"nlp_llm\": {\n    \"precision\": 0.9642857142857143,\n    \"recall\": 0.9642857142857143,\n    \"f1\": 0.9642857142857143,\n    \"num_samples\": 28\n  },\n  \"short_text\": {\n    \"precision\": 1.0,\n    \"recall\": 1.0,\n    \"f1\": 1.0,\n    \"num_samples\": 7\n  }\n}\n</pre> <p>Slicing can help identify sources of bias in our data. For example, our model has most likely learned to associated algorithms with certain applications such as CNNs used for computer vision or transformers used for NLP projects. However, these algorithms are not being applied beyond their initial use cases. We\u2019d need ensure that our model learns to focus on the application over algorithm. This could be learned with:</p> <ul> <li>enough data (new or oversampling incorrect predictions)</li> <li>masking the algorithm (using text matching heuristics)</li> </ul>"},{"location":"courses/mlops/evaluation/#interpretability","title":"Interpretability","text":"<p>Besides just comparing predicted outputs with ground truth values, we can also inspect the inputs to our models. What aspects of the input are more influential towards the prediction? If the focus is not on the relevant features of our input, then we need to explore if there is a hidden pattern we're missing or if our model has learned to overfit on the incorrect features. We can use techniques such as SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) to inspect feature importance. On a high level, these techniques learn which features have the most signal by assessing the performance in their absence. These inspections can be performed on a global level (ex. per-class) or on a local level (ex. single prediction).</p> <pre><code>from lime.lime_text import LimeTextExplainer\nfrom sklearn.pipeline import make_pipeline\n</code></pre> <p><code>LimeTextExplainer.explain_instance</code> function requires a <code>classifier_fn</code> that takes in a list of strings and outputs the predicted probabilities.</p> <p><pre><code>def classifier_fn(texts):\n    df = pd.DataFrame({\"title\": texts, \"description\": \"\", \"tag\": \"other\"})\n    z = predictor.predict(data=df)[\"predictions\"]\n    y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n    return y_prob\n</code></pre> <pre><code># Explain instance\ntext = \"Using pretrained convolutional neural networks for object detection.\"\nexplainer = LimeTextExplainer(class_names=list(class_to_index.keys()))\nexplainer.explain_instance(text, classifier_fn=classifier_fn, top_labels=1).show_in_notebook(text=True)\n</code></pre></p> <p>We can also use model-specific approaches to interpretability we we did in our embeddings lesson to identify the most influential n-grams in our text.</p>"},{"location":"courses/mlops/evaluation/#behavioral-testing","title":"Behavioral testing","text":"<p>Besides just looking at metrics, we also want to conduct some behavioral sanity tests. Behavioral testing is the process of testing input data and expected outputs while treating the model as a black box. They don't necessarily have to be adversarial in nature but more along the types of perturbations we'll see in the real world once our model is deployed. A landmark paper on this topic is Beyond Accuracy: Behavioral Testing of NLP Models with CheckList which breaks down behavioral testing into three types of tests:</p> <ul> <li><code>invariance</code>: Changes should not affect outputs. <pre><code># INVariance via verb injection (changes should not affect outputs)\ntokens = [\"revolutionized\", \"disrupted\"]\ntexts = [f\"Transformers applied to NLP have {token} the ML field.\" for token in tokens]\n[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]\n</code></pre></li> </ul> <pre>\n['natural-language-processing', 'natural-language-processing']\n</pre> <ul> <li><code>directional</code>: Change should affect outputs. <pre><code># DIRectional expectations (changes with known outputs)\ntokens = [\"text classification\", \"image classification\"]\ntexts = [f\"ML applied to {token}.\" for token in tokens]\n[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]\n</code></pre></li> </ul> <pre>\n['natural-language-processing', 'computer-vision']\n</pre> <ul> <li><code>minimum functionality</code>: Simple combination of inputs and expected outputs. <pre><code># Minimum Functionality Tests (simple input/output pairs)\ntokens = [\"natural language processing\", \"mlops\"]\ntexts = [f\"{token} is the next big wave in machine learning.\" for token in tokens]\n[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]\n</code></pre></li> </ul> <pre>\n['natural-language-processing', 'mlops']\n</pre> <p>We'll learn how to systematically create tests in our testing lesson.</p>"},{"location":"courses/mlops/evaluation/#online-evaluation","title":"Online evaluation","text":"<p>Once we've evaluated our model's ability to perform on a static dataset we can run several types of online evaluation techniques to determine performance on actual production data. It can be performed using labels or, in the event we don't readily have labels, proxy signals.</p> <ul> <li>manually label a subset of incoming data to evaluate periodically.</li> <li>asking the initial set of users viewing a newly categorized content if it's correctly classified.</li> <li>allow users to report misclassified content by our model.</li> </ul> <p>And there are many different experimentation strategies we can use to measure real-time performance before committing to replace our existing version of the system.</p>"},{"location":"courses/mlops/evaluation/#ab-tests","title":"AB tests","text":"<p>AB testing involves sending production traffic to our current system (control group) and the new version (treatment group) and measuring if there is a statistical difference between the values for two metrics. There are several common issues with AB testing such as accounting for different sources of bias, such as the novelty effect of showing some users the new system. We also need to ensure that the same users continue to interact with the same systems so we can compare the results without contamination.</p> <p>In many cases, if we're simply trying to compare the different versions for a certain metric, AB testing can take while before we reach statical significance since traffic is evenly split between the different groups. In this scenario, multi-armed bandits will be a better approach since they continuously assign traffic to the better performing version.</p>"},{"location":"courses/mlops/evaluation/#canary-tests","title":"Canary tests","text":"<p>Canary tests involve sending most of the production traffic to the currently deployed system but sending traffic from a small cohort of users to the new system we're trying to evaluate. Again we need to make sure that the same users continue to interact with the same system as we gradually roll out the new system.</p>"},{"location":"courses/mlops/evaluation/#shadow-tests","title":"Shadow tests","text":"<p>Shadow testing involves sending the same production traffic to the different systems. We don't have to worry about system contamination and it's very safe compared to the previous approaches since the new system's results are not served. However, we do need to ensure that we're replicating as much of the production system as possible so we can catch issues that are unique to production early on. But overall, shadow testing is easy to monitor, validate operational consistency, etc.</p> <p>What can go wrong?</p> <p>If shadow tests allow us to test our updated system without having to actually serve the new results, why doesn't everyone adopt it?</p> Show answer <p>With shadow deployment, we'll miss out on any live feedback signals (explicit/implicit) from our users since users are not directly interacting with the product using our new version.</p> <p>We also need to ensure that we're replicating as much of the production system as possible so we can catch issues that are unique to production early on. This is rarely possible because, while your ML system may be a standalone microservice, it ultimately interacts with an intricate production environment that has many dependencies.</p>"},{"location":"courses/mlops/evaluation/#capability-vs-alignment","title":"Capability vs. alignment","text":"<p>We've seen the many different metrics that we'll want to calculate when it comes to evaluating our model but not all metrics mean the same thing. And this becomes very important when it comes to choosing the \"best\" model(s).</p> <ul> <li>capability: the ability of our model to perform a task, measured by the objective function we optimize for (ex. log loss)</li> <li>alignment: desired behavior of our model, measure by metrics that are not differentiable or don't account for misclassifications and probability differences (ex. accuracy, precision, recall, etc.)</li> </ul> <p>While capability (ex. loss) and alignment (ex. accuracy) metrics may seem to be aligned, their differences can indicate issues in our data:</p> <ul> <li>\u2193 accuracy, \u2191 loss = large errors on lots of data (worst case)</li> <li>\u2193 accuracy, \u2193 loss = small errors on lots of data, distributions are close but tipped towards misclassifications (misaligned)</li> <li>\u2191 accuracy, \u2191 loss = large errors on some data (incorrect predictions have very skewed distributions)</li> <li>\u2191 accuracy, \u2193 loss = no/few errors on some data (best case)</li> </ul>"},{"location":"courses/mlops/evaluation/#resources","title":"Resources","text":"<ul> <li>Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning</li> <li>On Calibration of Modern Neural Networks</li> <li>Confident Learning: Estimating Uncertainty in Dataset Labels</li> <li>Automated Data Slicing for Model Validation</li> <li>SliceLine: Fast, Linear-Algebra-based Slice Finding for ML Model Debugging</li> <li>Distributionally Robust Neural Networks for Group Shifts</li> <li>No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems</li> <li>Model Patching: Closing the Subgroup Performance Gap with Data Augmentation</li> </ul> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Evaluation - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/experiment-tracking/","title":"Experiment Tracking","text":""},{"location":"courses/mlops/experiment-tracking/#intuition","title":"Intuition","text":"<p>So far, we've been training and evaluating our different baselines but haven't really been tracking these experiments. We'll fix this but defining a proper process for experiment tracking which we'll use for all future experiments (including hyperparameter optimization). Experiment tracking is the process of managing all the different experiments and their components, such as parameters, metrics, models and other artifacts and it enables us to:</p> <ul> <li>Organize all the necessary components of a specific experiment. It's important to have everything in one place and know where it is so you can use them later.</li> <li>Reproduce past results (easily) using saved experiments.</li> <li>Log iterative improvements across time, data, ideas, teams, etc.</li> </ul>"},{"location":"courses/mlops/experiment-tracking/#tools","title":"Tools","text":"<p>There are many options for experiment tracking but we're going to use MLFlow (100% free and open-source) because it has all the functionality we'll need. We can run MLFlow on our own servers and databases so there are no storage cost / limitations, making it one of the most popular options and is used by Microsoft, Facebook, Databricks and others. There are also several popular options such as a Comet ML (used by Google AI, HuggingFace, etc.), Neptune (used by Roche, NewYorker, etc.), Weights and Biases (used by Open AI, Toyota Research, etc.). These are fully managed solutions that provide features like dashboards, reports, etc.</p>"},{"location":"courses/mlops/experiment-tracking/#setup","title":"Setup","text":"<p>We'll start by setting up our model registry where all of our experiments and their artifacts will be stores.</p> <pre><code>import mlflow\nfrom pathlib import Path\nfrom ray.air.integrations.mlflow import MLflowLoggerCallback\nimport time\n</code></pre> <pre><code># Config MLflow\nMODEL_REGISTRY = Path(\"/tmp/mlflow\")\nPath(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\nMLFLOW_TRACKING_URI = \"file://\" + str(MODEL_REGISTRY.absolute())\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\nprint (mlflow.get_tracking_uri())\n</code></pre> <pre>\nfile:///tmp/mlflow\n</pre> <p>On Windows, the tracking URI should have three forwards slashes: <pre><code>MLFLOW_TRACKING_URI = \"file:///\" + str(MODEL_REGISTRY.absolute())\n</code></pre></p> <p>Note</p> <p>In this course, our MLflow artifact and backend store will both be on our local machine. In a production setting, these would be remote such as S3 for the artifact store and a database service (ex. PostgreSQL RDS) as our backend store.</p>"},{"location":"courses/mlops/experiment-tracking/#integration","title":"Integration","text":"<p>While we could use MLflow directly to log metrics, artifacts and parameters:</p> <pre><code># Example mlflow calls\nmlflow.log_metrics({\"train_loss\": train_loss, \"val_loss\": val_loss}, step=epoch)\nmlflow.log_artifacts(dir)\nmlflow.log_params(config)\n</code></pre> <p>We'll instead use Ray to integrate with MLflow. Specifically we'll use the MLflowLoggerCallback which will automatically log all the necessary components of our experiments to the location specified in our <code>MLFLOW_TRACKING_URI</code>. We of course can still use MLflow directly if we want to log something that's not automatically logged by the callback. And if we're using other experiment trackers, Ray has integrations for those as well.</p> <pre><code># MLflow callback\nexperiment_name = f\"llm-{int(time.time())}\"\nmlflow_callback = MLflowLoggerCallback(\n    tracking_uri=MLFLOW_TRACKING_URI,\n    experiment_name=experiment_name,\n    save_artifact=True)\n</code></pre> <p>Once we have the callback defined, all we have to do is update our <code>RunConfig</code> to include it.</p> <pre><code># Run configuration with MLflow callback\nrun_config = RunConfig(\n    callbacks=[mlflow_callback],\n    checkpoint_config=checkpoint_config,\n)\n</code></pre>"},{"location":"courses/mlops/experiment-tracking/#training","title":"Training","text":"<p>With our updated <code>RunConfig</code>, with the MLflow callback, we can now train our model and all the necessary components will be logged to MLflow. This is the exact same training workflow we've been using so far from the training lesson.</p> <pre><code># Dataset\nds = load_data()\ntrain_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)\n\n# Preprocess\npreprocessor = CustomPreprocessor()\ntrain_ds = preprocessor.fit_transform(train_ds)\nval_ds = preprocessor.transform(val_ds)\ntrain_ds = train_ds.materialize()\nval_ds = val_ds.materialize()\n\n# Trainer\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,  # uses RunConfig with MLflow callback\n    datasets={\"train\": train_ds, \"val\": val_ds},\n    dataset_config=dataset_config,\n    preprocessor=preprocessor,\n)\n\n# Train\nresults = trainer.fit()\n</code></pre> Trial name              status    loc               iter  total time (s)  epoch    lr  train_loss TorchTrainer_8c960_00000TERMINATED10.0.18.44:68577    10         76.3089      90.0001 0.000549661 <pre><code>results.metrics_dataframe\n</code></pre> epoch lr train_loss val_loss timestamp time_this_iter_s should_checkpoint done training_iteration trial_id date time_total_s pid hostname node_ip time_since_restore iterations_since_restore 0 0 0.0001 0.005196 0.004071 1689030896 14.162520 True False 1 8c960_00000 2023-07-10_16-14-59 14.162520 68577 ip-10-0-18-44 10.0.18.44 14.162520 1 1 1 0.0001 0.004033 0.003898 1689030905 8.704429 True False 2 8c960_00000 2023-07-10_16-15-08 22.866948 68577 ip-10-0-18-44 10.0.18.44 22.866948 2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9 9 0.0001 0.000550 0.001182 1689030958 6.604867 True False 10 8c960_00000 2023-07-10_16-16-01 76.308887 68577 ip-10-0-18-44 10.0.18.44 76.308887 10 <p>We're going to use the <code>search_runs</code> function from the MLflow python API to identify the best run in our experiment so far (we' only done one run so far so it will be the run from above).</p> <pre><code># Sorted runs\nsorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\nsorted_runs\n</code></pre> <pre>\nrun_id                                                           8e473b640d264808a89914e8068587fb\nexperiment_id                                                                  853333311265913081\nstatus                                                                                   FINISHED\n...\ntags.mlflow.runName                                                      TorchTrainer_077f9_00000\nName: 0, dtype: object\n</pre>"},{"location":"courses/mlops/experiment-tracking/#dashboard","title":"Dashboard","text":"<p>Once we're done training, we can use the MLflow dashboard to visualize our results. To do so, we'll use the <code>mlflow server</code> command to launch the MLflow dashboard and navigate to the experiment we just created.</p> <pre><code>mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri /tmp/mlflow/\n</code></pre> <p>View the dashboard</p> AnyscaleLocal <p>If you're on Anyscale Workspaces, then we need to first expose the port of the MLflow server. Run the following command on your Anyscale Workspace terminal to generate the public URL to your MLflow server.</p> <pre><code>APP_PORT=8080\necho https://$APP_PORT-port-$ANYSCALE_SESSION_DOMAIN\n</code></pre> <p>If you're running this notebook on your local laptop then head on over to http://localhost:8080/ to view your MLflow dashboard.</p> <p>MLFlow creates a main dashboard with all your experiments and their respective runs. We can sort runs by clicking on the column headers.</p> <p></p> <p>And within each run, we can view metrics, parameters, artifacts, etc.</p> <p></p> <p>And we can even create custom plots to help us visualize our results.</p> <p></p>"},{"location":"courses/mlops/experiment-tracking/#loading","title":"Loading","text":"<p>After inspection and once we've identified an experiment that we like, we can load the model for evaluation and inference.</p> <pre><code>from ray.air import Result\nfrom urllib.parse import urlparse\n</code></pre> <p>We're going to create a small utility function that uses an MLflow run's artifact path to load a Ray <code>Result</code> object. We'll then use the <code>Result</code> object to load the best checkpoint.</p> <pre><code>def get_best_checkpoint(run_id):\n    artifact_dir = urlparse(mlflow.get_run(run_id).info.artifact_uri).path  # get path from mlflow\n    results = Result.from_path(artifact_dir)\n    return results.best_checkpoints[0][0]\n</code></pre> <p>With a particular run's best checkpoint, we can load the model from it and use it.</p> <pre><code># Evaluate on test split\nbest_checkpoint = get_best_checkpoint(run_id=best_run.run_id)\npredictor = TorchPredictor.from_checkpoint(best_checkpoint)\nperformance = evaluate(ds=test_ds, predictor=predictor)\nprint (json.dumps(performance, indent=2))\n</code></pre> <pre>\n{\n  \"precision\": 0.9281010510531216,\n  \"recall\": 0.9267015706806283,\n  \"f1\": 0.9269438615952555\n}\n</pre> <p>Before we can use our model for inference, we need to load the preprocessor from our predictor and apply it to our input data.</p> <p><pre><code># Preprocessor\npreprocessor = predictor.get_preprocessor()\n</code></pre> <pre><code># Predict on sample\ntitle = \"Transfer learning with transformers\"\ndescription = \"Using transformers for transfer learning on text classification tasks.\"\nsample_df = pd.DataFrame([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\npredict_with_proba(df=sample_df, predictor=predictor)\n</code></pre></p> <pre>\n[{'prediction': 'natural-language-processing',\n  'probabilities': {'computer-vision': 0.00038025028,\n   'mlops': 0.00038209034,\n   'natural-language-processing': 0.998792,\n   'other': 0.00044562898}}]\n</pre> <p>In the next lesson we'll learn how to tune our models and use our MLflow dashboard to compare the results.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Tracking - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/exploratory-data-analysis/","title":"Exploratory Data Analysis (EDA)","text":""},{"location":"courses/mlops/exploratory-data-analysis/#intuition","title":"Intuition","text":"<p>Exploratory data analysis (EDA) to understand the signals and nuances of our dataset. It's a cyclical process that can be done at various points of our development process (before/after labeling, preprocessing, etc. depending on how well the problem is defined. For example, if we're unsure how to label or preprocess our data, we can use EDA to figure it out.</p> <p>We're going to start our project with EDA, a vital (and fun) process that's often misconstrued. Here's how to think about EDA:</p> <ul> <li>not just to visualize a prescribed set of plots (correlation matrix, etc.).</li> <li>goal is to convince yourself that the data you have is sufficient for the task.</li> <li>use EDA to answer important questions and to make it easier to extract insight</li> <li>not a one time process; as your data grows, you want to revisit EDA to catch distribution shifts, anomalies, etc.</li> </ul> <p>Let's answer a few key questions using EDA.</p> <pre><code>from collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set_theme()\nimport warnings; warnings.filterwarnings(\"ignore\")\nfrom wordcloud import WordCloud, STOPWORDS\n</code></pre>"},{"location":"courses/mlops/exploratory-data-analysis/#tag-distribution","title":"Tag distribution","text":"<p>How many data points do we have per tag? We'll use the <code>Counter</code> class to get counts for all the different tags.</p> <pre><code># Most common tags\nall_tags = Counter(df.tag)\nall_tags.most_common()\n</code></pre> <pre>\n[('natural-language-processing', 310),\n ('computer-vision', 285),\n ('other', 106),\n ('mlops', 63)]\n</pre> <p>We can then separate the tags and from their respective counts and plot them using Plotly.</p> <pre><code># Plot tag frequencies\ntags, tag_counts = zip(*all_tags.most_common())\nplt.figure(figsize=(10, 3))\nax = sns.barplot(x=list(tags), y=list(tag_counts))\nax.set_xticklabels(tags, rotation=0, fontsize=8)\nplt.title(\"Tag distribution\", fontsize=14)\nplt.ylabel(\"# of projects\", fontsize=12)\nplt.show()\n</code></pre> <p>We do have some data imbalance but it's not too bad. If we did want to account for this, there are many strategies, including over-sampling less frequent classes and under-sampling popular classes, class weights in the loss function, etc.</p>"},{"location":"courses/mlops/exploratory-data-analysis/#wordcloud","title":"Wordcloud","text":"<p>Is there enough signal in the title and description that's unique to each tag? This is important to know because we want to verify our initial hypothesis that the project's title and description are high quality features for predicting the tag. And to visualize this, we're going to use a wordcloud. We also use a jupyter widget, which you can view in the notebook, to interactively select a tag and see the wordcloud for that tag.</p> <pre><code># Most frequent tokens for each tag\ntag=\"natural-language-processing\"\nplt.figure(figsize=(10, 3))\nsubset = df[df.tag==tag]\ntext = subset.title.values\ncloud = WordCloud(\n    stopwords=STOPWORDS, background_color=\"black\", collocations=False,\n    width=500, height=300).generate(\" \".join(text))\nplt.axis(\"off\")\nplt.imshow(cloud)\n</code></pre> <p>Looks like the <code>title</code> text feature has some good signal for the respective classes and matches our intuition. We can repeat this for the <code>description</code> text feature as well and see similar quality signals. This information will become useful when we decide how to use our features for modeling.</p> <p>There's a lot more exploratory data analysis that we can do but for now we've answered our questions around our class distributions and the quality of our text features. In the next lesson we'll preprocess our dataset in preparation for model training.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Exploration - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/feature-store/","title":"Feature Store","text":""},{"location":"courses/mlops/feature-store/#what-is-a-feature-store","title":"What is a feature store","text":"<p>Let's motivate the need for a feature store by chronologically looking at what challenges developers face in their current workflows. Suppose we had a task where we needed to predict something for an entity (ex. user) using their features.</p> <ol> <li>Duplication: feature development in isolation (for each unique ML application) can lead to duplication of efforts (setting up ingestion pipelines, feature engineering, etc.).<ul> <li><code>Solution</code>: create a central feature repository where the entire team contributes maintained features that anyone can use for any application.</li> </ul> </li> <li>Skew: we may have different pipelines for generating features for training and serving which can introduce skew through the subtle differences.<ul> <li><code>Solution</code>: create features using a unified pipeline and store them in a central location that the training and serving pipelines pull from.</li> </ul> </li> <li>Values: once we set up our data pipelines, we need to ensure that our input feature values are up-to-date so we aren't working with stale data, while maintaining point-in-time correctness so we don't introduce data leaks.<ul> <li><code>Solution</code>: retrieve input features for the respective outcomes by pulling what's available when a prediction would be made.</li> </ul> </li> </ol> <p>Point-in-time correctness refers to mapping the appropriately up-to-date input feature values to an observed outcome at \\(t_{n+1}\\). This involves knowing the time (\\(t_n\\)) that a prediction is needed so we can collect feature values (\\(X\\)) at that time.</p> <p>When actually constructing our feature store, there are several core components we need to have to address these challenges:</p> <ul> <li>data ingestion: ability to ingest data from various sources (databases, data warehouse, etc.) and keep them updated.</li> <li>feature definitions: ability to define entities and corresponding features</li> <li>historical features: ability to retrieve historical features to use for training.</li> <li>online features: ability to retrieve features from a low latency origin for inference.</li> </ul> <p>Each of these components is fairly easy to set up but connecting them all together requires a managed service, SDK layer for interactions, etc. Instead of building from scratch, it's best to leverage one of the production-ready, feature store options such as Feast, Hopsworks, Tecton, Rasgo, etc. And of course, the large cloud providers have their own feature store options as well (Amazon's SageMaker Feature Store, Google's Vertex AI, etc.)</p> <p>Tip</p> <p>We highly recommend that you explore this lesson after completing the previous lessons since the topics (and code) are iteratively developed. We did, however, create the  feature-store repository for a quick overview with an interactive notebook.</p>"},{"location":"courses/mlops/feature-store/#over-engineering","title":"Over-engineering","text":"<p>Not all machine learning platforms require a feature store. In fact, our use case is a perfect example of a task that does not benefit from a feature store. All of our data points are independent, stateless, from client-side and there is no entity that has changing features over time. The real utility of a feature store shines when we need to have up-to-date features for an entity that we continually generate predictions for. For example, a user's behavior (clicks, purchases, etc.) on an e-commerce platform or the deliveries a food runner recently made in the last hour, etc.</p>"},{"location":"courses/mlops/feature-store/#when-do-i-need-a-feature-store","title":"When do I need a feature store?","text":"<p>To answer this question, let's revisit the main challenges that a feature store addresses:</p> <ul> <li>Duplication: if we don't have too many ML applications/models, we don't really need to add the additional complexity of a feature store to manage transformations. All the feature transformations can be done directly inside the model processing or as a separate function. We could even organize these transformations in a separate central repository for other team members to use. But this quickly becomes difficult to use because developers still need to know which transformations to invoke and which are compatible with their specific models, etc.</li> </ul> <p>Note</p> <p>Additionally, if the transformations are compute intensive, then they'll incur a lot of costs by running on duplicate datasets across different applications (as opposed to having a central location with upt-o-date transformed features).</p> <ul> <li> <p>Skew: similar to duplication of efforts, if our transformations can be tied to the model or as a standalone function, then we can just reuse the same pipelines to produce the feature values for training and serving. But this becomes complex and compute intensive as the number of applications, features and transformations grow.</p> </li> <li> <p>Value: if we aren't working with features that need to be computed server-side (batch or streaming), then we don't have to worry about concepts like point-in-time, etc. However, if we are, a feature store can allow us to retrieve the appropriate feature values across all data sources without the developer having to worry about using disparate tools for different sources (batch, streaming, etc.)</p> </li> </ul>"},{"location":"courses/mlops/feature-store/#feast","title":"Feast","text":"<p>We're going to leverage Feast as the feature store for our application for it's ease of local setup, SDK for training/serving, etc.</p> <pre><code># Install Feast and dependencies\npip install feast==0.10.5 PyYAML==5.3.1 -q\n</code></pre> <p>\ud83d\udc49 \u00a0 Follow along interactive notebook in the  feature-store repository as we implement the concepts below.</p>"},{"location":"courses/mlops/feature-store/#set-up","title":"Set up","text":"<p>We're going to create a feature repository at the root of our project. Feast will create a configuration file for us and we're going to add an additional features.py file to define our features.</p> <p>Traditionally, the feature repository would be it's own isolated repository that other services will use to read/write features from.</p> <pre><code>mkdir -p stores/feature\nmkdir -p data\nfeast init --minimal --template local features\ncd features\ntouch features.py\n</code></pre> <pre>\nCreating a new Feast repository in /content/features.\n</pre> <p>The initialized feature repository (with the additional file we've added) will include:</p> <pre><code>features/\n\u251c\u2500\u2500 feature_store.yaml  - configuration\n\u2514\u2500\u2500 features.py         - feature definitions\n</code></pre> <p>We're going to configure the locations for our registry and online store (SQLite) in our <code>feature_store.yaml</code> file.</p> <ul> <li>registry: contains information about our feature repository, such as data sources, feature views, etc. Since it's in a DB, instead of a Python file, it can very quickly be accessed in production.</li> <li>online store: DB (SQLite for local) that stores the (latest) features for defined entities to be used for online inference.</li> </ul> <p>If all our feature definitions look valid, Feast will sync the metadata about Feast objects to the registry. The registry is a tiny database storing most of the same information you have in the feature repository. This step is necessary because the production feature serving infrastructure won't be able to access Python files in the feature repository at run time, but it will be able to efficiently and securely read the feature definitions from the registry.</p> <p>When we run Feast locally, the offline store is effectively represented via Pandas point-in-time joins. Whereas, in production, the offline store can be something more robust like Google BigQuery, Amazon RedShift, etc.</p> <p>We'll go ahead and paste this into our <code>features/feature_store.yaml</code> file (the notebook cell is automatically do this):</p> <pre><code>project: features\nregistry: ../stores/feature/registry.db\nprovider: local\nonline_store:\npath: ../stores/feature/online_store.db\n</code></pre>"},{"location":"courses/mlops/feature-store/#data-ingestion","title":"Data ingestion","text":"<p>The first step is to establish connections with our data sources (databases, data warehouse, etc.). Feast requires it's data sources to either come from a file (Parquet), data warehouse (BigQuery) or data stream (Kafka / Kinesis). We'll convert our generated features file from the DataOps pipeline (<code>features.json</code>) into a Parquet file, which is a column-major data format that allows fast feature retrieval and caching benefits (contrary to row-major data formats such as CSV where we have to traverse every single row to collect feature values).</p> <pre><code>import os\nimport pandas as pd\n</code></pre> <pre><code># Load labeled projects\nprojects = pd.read_csv(\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\")\ntags = pd.read_csv(\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\")\ndf = pd.merge(projects, tags, on=\"id\")\ndf[\"text\"] = df.title + \" \" + df.description\ndf.drop([\"title\", \"description\"], axis=1, inplace=True)\ndf.head(5)\n</code></pre> <pre>\nid\n      created_on\n      tag\n      text\n    0\n      6\n      2020-02-20 06:43:18\n      computer-vision\n      Comparison between YOLO and RCNN on real world...\n    1\n      7\n      2020-02-20 06:47:21\n      computer-vision\n      Show, Infer &amp; Tell: Contextual Inference for C...\n    2\n      9\n      2020-02-24 16:24:45\n      graph-learning\n      Awesome Graph Classification A collection of i...\n    3\n      15\n      2020-02-28 23:55:26\n      reinforcement-learning\n      Awesome Monte Carlo Tree Search A curated list...\n    4\n      19\n      2020-03-03 13:54:31\n      graph-learning\n      Diffusion to Vector Reference implementation o...\n    </pre> <pre><code># Format timestamp\ndf.created_on = pd.to_datetime(df.created_on)\n</code></pre> <pre><code># Convert to parquet\nDATA_DIR = Path(os.getcwd(), \"data\")\ndf.to_parquet(\n    Path(DATA_DIR, \"features.parquet\"),\n    compression=None,\n    allow_truncated_timestamps=True,\n)\n</code></pre>"},{"location":"courses/mlops/feature-store/#feature-definitions","title":"Feature definitions","text":"<p>Now that we have our data source prepared, we can define our features for the feature store.</p> <pre><code>from datetime import datetime\nfrom pathlib import Path\nfrom feast import Entity, Feature, FeatureView, ValueType\nfrom feast.data_source import FileSource\nfrom google.protobuf.duration_pb2 import Duration\n</code></pre> <p>The first step is to define the location of the features (FileSource in our case) and the timestamp column for each data point.</p> <pre><code># Read data\nSTART_TIME = \"2020-02-17\"\nproject_details = FileSource(\n    path=str(Path(DATA_DIR, \"features.parquet\")),\n    event_timestamp_column=\"created_on\",\n)\n</code></pre> <p>Next, we need to define the main entity that each data point pertains to. In our case, each project has a unique ID with features such as text and tags.</p> <pre><code># Define an entity\nproject = Entity(\n    name=\"id\",\n    value_type=ValueType.INT64,\n    description=\"project id\",\n)\n</code></pre> <p>Finally, we're ready to create a FeatureView that loads specific features (<code>features</code>), of various value types, from a data source (<code>input</code>) for a specific period of time (<code>ttl</code>).</p> <pre><code># Define a Feature View for each project\nproject_details_view = FeatureView(\n    name=\"project_details\",\n    entities=[\"id\"],\nttl=Duration(\nseconds=(datetime.today() - datetime.strptime(START_TIME, \"%Y-%m-%d\")).days * 24 * 60 * 60\n    ),\nfeatures=[\nFeature(name=\"text\", dtype=ValueType.STRING),\n        Feature(name=\"tag\", dtype=ValueType.STRING),\n    ],\n    online=True,\ninput=project_details,\ntags={},\n)\n</code></pre> <p>So let's go ahead and define our feature views by moving this code into our <code>features/features.py</code> script (the notebook cell is automatically do this):</p> Show code <pre><code>from datetime import datetime\nfrom pathlib import Path\n\nfrom feast import Entity, Feature, FeatureView, ValueType\nfrom feast.data_source import FileSource\nfrom google.protobuf.duration_pb2 import Duration\n\n\n# Read data\nSTART_TIME = \"2020-02-17\"\nproject_details = FileSource(\n    path=\"/content/data/features.parquet\",\n    event_timestamp_column=\"created_on\",\n)\n\n# Define an entity for the project\nproject = Entity(\n    name=\"id\",\n    value_type=ValueType.INT64,\n    description=\"project id\",\n)\n\n# Define a Feature View for each project\n# Can be used for fetching historical data and online serving\nproject_details_view = FeatureView(\n    name=\"project_details\",\n    entities=[\"id\"],\n    ttl=Duration(\n        seconds=(datetime.today() - datetime.strptime(START_TIME, \"%Y-%m-%d\")).days * 24 * 60 * 60\n    ),\n    features=[\n        Feature(name=\"text\", dtype=ValueType.STRING),\n        Feature(name=\"tag\", dtype=ValueType.STRING),\n    ],\n    online=True,\n    input=project_details,\n    tags={},\n)\n</code></pre> <p>Once we've defined our feature views, we can <code>apply</code> it to push a version controlled definition of our features to the registry for fast access. It will also configure our registry and online stores that we've defined in our <code>feature_store.yaml</code>.</p> <pre><code>cd features\nfeast apply\n</code></pre> <pre>\nRegistered entity id\nRegistered feature view project_details\nDeploying infrastructure for project_details\n</pre>"},{"location":"courses/mlops/feature-store/#historical-features","title":"Historical features","text":"<p>Once we've registered our feature definition, along with the data source, entity definition, etc., we can use it to fetch historical features. This is done via joins using the provided timestamps using pandas for our local setup or BigQuery, Hive, etc. as an offline DB for production.</p> <pre><code>import pandas as pd\nfrom feast import FeatureStore\n</code></pre> <pre><code># Identify entities\nproject_ids = df.id[0:3].to_list()\nnow = datetime.now()\ntimestamps = [datetime(now.year, now.month, now.day)]*len(project_ids)\nentity_df = pd.DataFrame.from_dict({\"id\": project_ids, \"event_timestamp\": timestamps})\nentity_df.head()\n</code></pre> id event_timestamp 0 6 2022-06-23 1 7 2022-06-23 2 9 2022-06-23 <pre><code># Get historical features\nstore = FeatureStore(repo_path=\"features\")\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    feature_refs=[\"project_details:text\", \"project_details:tag\"],\n).to_df()\ntraining_df.head()\n</code></pre> event_timestamp id project_details__text project_details__tag 0 2022-06-23 00:00:00+00:00 6 Comparison between YOLO and RCNN on real world... computer-vision 1 2022-06-23 00:00:00+00:00 7 Show, Infer &amp; Tell: Contextual Inference for C... computer-vision 2 2022-06-23 00:00:00+00:00 9 Awesome Graph Classification A collection of i... graph-learning"},{"location":"courses/mlops/feature-store/#materialize","title":"Materialize","text":"<p>For online inference, we want to retrieve features very quickly via our online store, as opposed to fetching them from slow joins. However, the features are not in our online store just yet, so we'll need to materialize them first.</p> <pre><code>cd features\nCURRENT_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%S\")\nfeast materialize-incremental $CURRENT_TIME\n</code></pre> <pre>\nMaterializing 1 feature views to 2022-06-23 19:16:05+00:00 into the sqlite online store.\nproject_details from 2020-02-17 19:16:06+00:00 to 2022-06-23 19:16:05+00:00:\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 955/955 [00:00&lt;00:00, 10596.97it/s]\n</pre> <p>This has moved the features for all of our projects into the online store since this was first time materializing to the online store. When we subsequently run the <code>materialize-incremental</code> command, Feast keeps track of previous materializations and so we'll only materialize the new data since the last attempt.</p>"},{"location":"courses/mlops/feature-store/#online-features","title":"Online features","text":"<p>Once we've materialized the features (or directly sent to the online store in the stream scenario), we can use the online store to retrieve features.</p> <pre><code># Get online features\nstore = FeatureStore(repo_path=\"features\")\nfeature_vector = store.get_online_features(\n    feature_refs=[\"project_details:text\", \"project_details:tag\"],\n    entity_rows=[{\"id\": 6}],\n).to_dict()\nfeature_vector\n</code></pre> <pre><code>{'id': [6],\n 'project_details__tag': ['computer-vision'],\n 'project_details__text': ['Comparison between YOLO and RCNN on real world videos Bringing theory to experiment is cool. We can easily train models in colab and find the results in minutes.']}\n</code></pre>"},{"location":"courses/mlops/feature-store/#architecture","title":"Architecture","text":""},{"location":"courses/mlops/feature-store/#batch-processing","title":"Batch processing","text":"<p>The feature store we implemented above assumes that our task requires batch processing. This means that inference requests on specific entity instances can use features that have been materialized from the offline store. Note that they may not be the most recent feature values for that entity.</p> <ol> <li>Application data is stored in a database and/or a data warehouse, etc. And it goes through the necessary pipelines to be prepared for downstream application (analytics, machine learning, etc.).</li> <li>These features are written to the offline store which can then be used to retrieve historical training data to train a model with. In our local set up, this was join via Pandas DataFrame joins for given timestamps and entity IDs but in a production setting, something like Google BigQuery or Hive would receive the feature requests.</li> <li>Once we have our training data, we can start the workflows to optimize, train and validate a model.</li> <li>We can incrementally materialize features to the online store so that we can retrieve an entity's feature values with low latency. In our local set up, this was join via SQLite for a given set of entities but in a production setting, something like Redis or DynamoDB would be used.</li> <li>These online features are passed on to the deployed model to generate predictions that would be used downstream.</li> </ol> <p>Warning</p> <p>Had our entity (projects) had features that change over time, we would materialize them to the online store incrementally. But since they don't, this would be considered over engineering but it's important to know how to leverage a feature store for entities with changing features over time.</p>"},{"location":"courses/mlops/feature-store/#stream-processing","title":"Stream processing","text":"<p>Some applications may require stream processing where we require near real-time feature values to deliver up-to-date predictions at low latency. While we'll still utilize an offline store for retrieving historical data, our application's real-time event data will go directly through our data streams to an online store for serving. An example where stream processing would be needed is when we want to retrieve real-time user session behavior (clicks, purchases) in an e-commerce platform so that we can recommend the appropriate items from our catalog.</p> <ol> <li>Real-time event data enters our running data streams (Kafka / Kinesis, etc.) where they can be processed to generate features.</li> <li>These features are written to the online store which can then be used to retrieve online features for serving at low latency. In our local set up, this was join via SQLite for a given set of entities but in a production setting, something like Redis or DynamoDB would be used.</li> <li>Streaming features are also written from the data stream to the batch data source (data warehouse, db, etc.) to be processed for generating training data later on.</li> <li>Historical data will be validated and used to generate features for training a model. This cadence for how often this happens depends on whether there are data annotation lags, compute constraints, etc.</li> </ol> <p>There are a few more components we're not visualizing here such as the unified ingestion layer (Spark), that connects data from the varied data sources (warehouse, DB, etc.) to the offline/online stores, or low latency serving (&lt;10 ms). We can read more about all of these in the official Feast Documentation, which also has guides to set up a feature store with Feast with AWS, GCP, etc.</p>"},{"location":"courses/mlops/feature-store/#additional-functionality","title":"Additional functionality","text":"<p>Additional functionality that many feature store providers are currently (or recently) trying to integrate within the feature store platform include:</p> <ul> <li>transform: ability to directly apply global preprocessing or feature engineering on top of raw data extracted from data sources.<ul> <li><code>Current solution</code>: apply transformations as a separate Spark, Python, etc. workflow task before writing to the feature store.</li> </ul> </li> <li>validate: ability to assert expectations and identify data drift on the feature values.<ul> <li><code>Current solution</code>: apply data testing and monitoring as upstream workflow tasks before they are written to the feature store.</li> </ul> </li> <li>discover: ability for anyone in our team to easily discover features that they can leverage for their application.<ul> <li><code>Current solution</code>: add a data discovery engine, such as Amundsen, on top of our feature store to enable others to search for features.</li> </ul> </li> </ul>"},{"location":"courses/mlops/feature-store/#reproducibility","title":"Reproducibility","text":"<p>Though we could continue to version our training data with DVC whenever we release a version of the model, it might not be necessary. When we pull data from source or compute features, should they save the data itself or just the operations?</p> <ul> <li>Version the data<ul> <li>This is okay if (1) the data is manageable, (2) if our team is small/early stage ML or (3) if changes to the data are infrequent.</li> <li>But what happens as data becomes larger and larger and we keep making copies of it.</li> </ul> </li> <li>Version the operations<ul> <li>We could keep snapshots of the data (separate from our projects) and provided the operations and timestamp, we can execute operations on those snapshots of the data to recreate the precise data artifact used for training. Many data systems use time-travel to achieve this efficiently.</li> <li>But eventually this also results in data storage bulk. What we need is an append-only data source where all changes are kept in a log instead of directly changing the data itself. So we can use the data system with the logs to produce versions of the data as they were without having to store separate snapshots of the the data itself.</li> </ul> </li> </ul> <p>Regardless of the choice above, feature stores are very useful here. Instead of coupling data pulls and feature compute with the time of modeling, we can separate these two processes so that features are up-to-date when we need them. And we can still achieve reproducibility via efficient point-in-time correctness, low latency snapshots, etc. This essentially creates the ability to work with any version of the dataset at any point in time.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Feature Store - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/git/","title":"Git","text":""},{"location":"courses/mlops/git/#intuition","title":"Intuition","text":"<p>Whether we're working individually or with a team, it's important that we have a system to track changes to our projects so that we can revert to previous versions and so that others can reproduce our work and contribute to it. Git is a distributed versional control system that allows us do exactly this. Git runs locally on our computer and it keeps track of our files and their histories. To enable collaboration with others, we can use a remote host (GitHub, GitLab, BitBucket, etc.) to host our files and their histories. We'll use git to push our local changes and pull other's changes to and from the remote host.</p> <p>Git is traditionally used to store and version small files &lt;100MB (scripts, READMEs, etc.), however, we can still version large artifacts (datasets, model weights, etc.) using text pointers pointing to blob stores. These pointers will contain information such as where the asset is located, it's specific contents/version (ex. via hashing), etc.</p>"},{"location":"courses/mlops/git/#set-up","title":"Set up","text":""},{"location":"courses/mlops/git/#initialize-git","title":"Initialize git","text":"<p>Initialize a local repository (<code>.git</code> directory) to track our files: <pre><code>git init\n</code></pre></p> <pre>\nInitialized empty Git repository in /Users/goku/Documents/madewithml/MLOps/.git/\n</pre> <p>We can see what files are untracked or yet to be committed:</p> <pre><code>git status\n</code></pre> <pre>\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add ...\" to include in what will be committed)\n        .flake8\n        .vscode/\n        Makefile\n        ..."},{"location":"courses/mlops/git/#gitignore","title":".gitignore","text":"<p>We can see that we have some files that we don't want to push to a remote host, such as our virtual environment, logs, large data files, etc. We can create a <code>.gitignore</code> file to make sure we aren't checking in these files.</p>\n<pre><code>touch .gitignore\n</code></pre>\n<p>We'll add the following files to the file:</p>\n<pre><code># Data\nlogs/\nstores/\ndata/\n\n# Packaging\nvenv/\n*.egg-info/\n__pycache__/\n\n# Misc\n.DS_Store\n</code></pre>\n<p>For now, we're going to add <code>data</code> to our <code>.gitignore</code> file as well but this means that others will not be able to produce the same data assets when they pull from our remote host. To address this, we'll push pointers to our data files in our versioning lesson so that the data too can be reproduced exactly as we have it locally.</p>\n<p>Tip</p>\n<p>Check out our project's .gitignore for a more complete example that also includes lots of other system artifacts that we would normally not want to push to a remote repository. Our complete <code>.gitignore</code> file is based on GitHub's Python template  and we're using a Mac, so we added the relevant global file names as well.</p>\n<p>If we run <code>git status</code> now, we should no longer see the files we've defined in our <code>.gitignore</code> file.</p>"},{"location":"courses/mlops/git/#add-to-stage","title":"Add to stage","text":"<p>Next, we'll add our work from the working directory to the staging area.</p>\n<ul>\n<li>We can add one file at a time:\n<pre><code>git add &lt;filename&gt;\n</code></pre></li>\n<li>We can add all files at once:\n<pre><code>git add .\n</code></pre></li>\n</ul>\n<p>Now running <code>git status</code> will show us all the staged files:</p>\n<pre><code>git status\n</code></pre>\n<pre>\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached ...\" to unstage)\n        new file:   .flake8\n        new file:   .gitignore\n        new file:   Makefile\n        ..."},{"location":"courses/mlops/git/#commit-to-repo","title":"Commit to repo","text":"<p>Now we're ready to commit the files in the staging area to the local repository. The default branch (a version of our project) will be called <code>main</code>.</p>\n<pre><code>git commit -m \"added project files\"\n</code></pre>\n<pre>\n[main (root-commit) 704d99c] added project files\n 47 files changed, 50651 insertions(+)\n create mode 100644 .flake8\n create mode 100644 .gitignore\n create mode 100644 Makefile\n ...\n</pre>\n\n<p>The commit requires a message indicating what changes took place. We can use <code>git commit --amend</code> to edit the commit message if needed. If we do a <code>git status</code> check we'll see that there is nothing else to commit from our staging area.</p>\n<pre><code>git status\n</code></pre>\n<pre>\nOn branch main\nnothing to commit, working tree clean\n</pre>"},{"location":"courses/mlops/git/#push-to-remote","title":"Push to remote","text":"<p>Now we're ready to push the updates from our local repository to a remote repository. Start by creating an account on GitHub (or any other remote repository) and follow the instructions to create a remote repository (it can be private or public). Inside our local repository, we're going to set our username and email credentials so that we can push changes from our local to the remote repository.</p>\n<p><pre><code># Set credentials via terminal\ngit config --global user.name &lt;USERNAME&gt;\ngit config --global user.email &lt;EMAIL&gt;\n</code></pre>\nWe can quickly validate that we set the proper credentials like so:\n<pre><code># Check credentials\ngit config --global user.name\ngit config --global user.email\n</code></pre></p>\n<p>Next, we need to establish the connection between our local and remote repositories:</p>\n<pre><code># Push to remote\ngit remote add origin https://github.com/&lt;USERNAME&gt;/&lt;REPOSITORY_NAME&gt;.git\ngit push -u origin main  # pushing the contents of our local repo to the remote repo\n# origin signifies the remote repository\n</code></pre>"},{"location":"courses/mlops/git/#developing","title":"Developing","text":"<p>Now we're ready to start adding to our project and committing the changes.</p>"},{"location":"courses/mlops/git/#cloning","title":"Cloning","text":"<p>If we (or someone else) doesn't already have the local repository set up and connected with the remote host, we can use the clone command:</p>\n<pre><code>git clone &lt;REMOTE_REPO_URL&gt; &lt;PATH_TO_PROJECT_DIR&gt;\n</code></pre>\n<p>And we can clone a specific branch of a repository as well:</p>\n<pre><code>git clone -b &lt;BRANCH&gt; &lt;REMOTE_REPO_URL&gt; &lt;PATH_TO_PROJECT_DIR&gt;\n</code></pre>\n<ul>\n<li><code>&lt;REMOTE_REPO_URL&gt;</code> is the location of the remote repo (ex. https://github.com/GokuMohandas/Made-With-ML).</li>\n<li><code>&lt;PATH_TO_PROJECT_DIR&gt;</code> is the name of the local directory you want to clone the project into.</li>\n</ul>"},{"location":"courses/mlops/git/#create-a-branch","title":"Create a branch","text":"<p>When we want to add or change something, such as adding a feature, fixing a bug, etc., it's always a best practice to create a separate branch before developing. This is especially crucial when working with a team so we can cleanly merge our work with the <code>main</code> branch after discussions and reviews.</p>\n<p>We'll start by creating a new branch:</p>\n<pre><code>git checkout -b &lt;NEW_BRANCH_NAME&gt;\n</code></pre>\n<p>We can see all the branches we've created with the following command where the * indicates our current branch:</p>\n<pre><code>git branch\n</code></pre>\n<pre>\n* convnet\nmain\n</pre>\n\n<p>We can easily switch between existing branches using:</p>\n<pre><code>git checkout &lt;BRANCH_NAME&gt;\n</code></pre>\n<p>Once we're in a branch, we can make changes to our project and commit those changes.</p>\n<pre><code>git add .\ngit commit -m \"update model to a convnet\"\ngit push origin convnet\n</code></pre>\n<p>Note that we are pushing this branch to our remote repository, which doesn't yet exist there, so GitHub will create it accordingly.</p>"},{"location":"courses/mlops/git/#pull-request-pr","title":"Pull request (PR)","text":"<p>When we push our new branch to the remote repository, we'll need to create a pull request (PR) to merge with another branch (ex. our <code>main</code> branch in this case). When merging our work with another branch (ex. main), it's called a pull request because we're requesting the branch to pull our committed work. We can create the pull request using steps outlined here: Creating a pull request.</p>\n<p>Note</p>\n<p>We can  merge branches and resolve conflicts using git CLI commands but it's preferred to use the online interface because we can easily visualize the changes, have discussion with teammates, etc.\n<pre><code># Merge via CLI\ngit push origin convnet\ngit checkout main\ngit merge convnet\ngit push origin main\n</code></pre></p>"},{"location":"courses/mlops/git/#pull","title":"Pull","text":"<p>Once we accepted the pull request, our <code>main</code> branch is now updated with our changes. However, the update only happened on the remote repository so we should pull those changes to our local <code>main</code> branch as well.</p>\n<pre><code>git checkout main\ngit pull origin main\n</code></pre>"},{"location":"courses/mlops/git/#delete-branches","title":"Delete branches","text":"<p>Once we're done working with a branch, we can delete it to prevent our repository from cluttering up. We can easily delete both the local and remote versions of the branch with the following commands:\n<pre><code># Delete branches\ngit branch -d &lt;BRANCH_NAME&gt;  # local\ngit push origin --delete &lt;BRANCH_NAME&gt;  # remote\n</code></pre></p>"},{"location":"courses/mlops/git/#collaboration","title":"Collaboration","text":"<p>So far, the workflows for integrating our iterative development has been very smooth but in a collaborative setting, we may need to resolve conflicts. Let's say there are two branches (<code>a</code> and <code>b</code>) that were created from the <code>main</code> branch. Here's what we're going to try and simulate:</p>\n<ol>\n<li>Developer A and B fork the <code>main</code> branch to make some changes</li>\n<li>Developer A makes a change and submits a PR to the <code>main</code> branch.</li>\n<li>Developer B makes a change to the same line as Developer A and submits a PR to <code>main</code>.</li>\n<li>We have a merge conflict now since both developers altered the same line.</li>\n<li>Choose which version of the code to keep and resolve the conflict.</li>\n</ol>\n<p>When we try to merge the second PR, we have to resolve the conflicts between this new PR and what already exists in the <code>main</code> branch.</p>\n<p>We can resolve the conflict by choosing which content (current <code>main</code> which merged with the <code>a</code> branch or this <code>b</code> branch) to keep and delete the other one. Then we can merge the PR successfully and update our local <code>main</code> branch.</p>\n<pre><code>&lt;&lt;&lt;&lt;&lt;&lt; BRANCH_A\n&lt;CHANGES FROM BRANCH A&gt;\n======\n&lt;CHANGES FROM BRANCH B&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; BRANCH_B\n</code></pre>\n<p>Once the conflicts have been resolved and we merge the PR, we can update our local repository to reflect the decisions.</p>\n<pre><code>git checkout main\ngit pull origin main\n</code></pre>\n<p>Note</p>\n<p>We only have a conflict because both branches were forked from a previous version of the <code>main</code> branch and they both happened to alter the same content. Had we created one branch first and then updated main before creating the second branch, we wouldn't have any conflicts. But in a collaborative setting, different developers may fork off the same version of the branch anytime.</p>\n<p>A few more important commands to know include rebase and stash.</p>"},{"location":"courses/mlops/git/#inspection","title":"Inspection","text":"<p>Git allows us to inspect the current and previous states of our work at many different levels. Let's explore the most commonly used commands.</p>"},{"location":"courses/mlops/git/#status","title":"Status","text":"<p>We've used the status command quite a bit already as it's very useful to quickly see the status of our working tree.</p>\n<pre><code># Status\ngit status\ngit status -s  # short format\n</code></pre>"},{"location":"courses/mlops/git/#log","title":"Log","text":"<p>If we want to see the log of all our commits, we can do so using the log command. We can also do the same by inspecting specific branch histories on the Git online interface.</p>\n<pre><code># Log\ngit log\ngit log --oneline  # short version\n</code></pre>\n<pre>\n704d99c (HEAD -&gt; main) added project files\n</pre>\n\n<p>Commit IDs are 40 characters long but we can represent them with the first few (seven digits is the default for a Git SHA). If there is ambiguity, Git will notify us and we can simply add more of the commit ID.</p>"},{"location":"courses/mlops/git/#diff","title":"Diff","text":"<p>If we want to know the difference between two commits, we can use the diff command.</p>\n<pre><code># Diff\ngit diff  # all changes between current working tree and previous commit\ngit diff &lt;COMMIT_A&gt; &lt;COMMIT_B&gt;  # diff b/w two commits\ngit diff &lt;COMMIT_A&gt;:&lt;PATH_TO_FILE&gt; &lt;COMMIT_B&gt;:&lt;PATH_TO_FILE&gt;  # file diff b/w two commits\n</code></pre>\n<pre>\ndiff --git a/.gitignore b/.gitignore\nindex 288973d..028aa13 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -1,7 +1,6 @@\n # Data\n logs/\n stores/\n-data/\n</pre>"},{"location":"courses/mlops/git/#blame","title":"Blame","text":"<p>One of the most useful inspection commands is blame, which allows us to see what commit was responsible for every single line in a file.\n<pre><code># Blame\ngit blame &lt;PATH_TO_FILE&gt;\ngit blame -L 1,3 &lt;PATH_TO_FILE&gt;  # blame for lines 1 and 3\n</code></pre></p>"},{"location":"courses/mlops/git/#time-travel","title":"Time travel","text":"<p>Sometimes we may have done something we wish we could change. It's not always possible to do this in life, but in the world of Git, it is!</p>"},{"location":"courses/mlops/git/#restore","title":"Restore","text":"<p>Sometimes we may just want to undo adding or staging a file, which we can easily do with the restore command.\n<pre><code># Restore\ngit restore -- &lt;PATH_TO_FILE&gt; &lt;PATH_TO_FILE&gt; # will undo any changes\ngit restore --stage &lt;PATH_TO_FILE&gt;  # will remove from stage (won't undo changes)\n</code></pre></p>"},{"location":"courses/mlops/git/#reset","title":"Reset","text":"<p>Now if we already made the commit but haven't pushed to remote yet, we can reset to the previous commit by moving the branch pointer to that commit. Note that this will undo all changes made since the previous commit.\n<pre><code># Reset\ngit reset &lt;PREVIOUS_COMMIT_ID&gt;  # or HEAD^\n</code></pre></p>\n<p><code>HEAD</code> is a quick way to refer to the previous commit. Both <code>HEAD</code> and any previous commit ID can be accompanied with a <code>^</code> or <code>~</code> symbol which acts as a relative reference. <code>^</code>n refers to the nth parent of the commit while <code>~</code>n refers to the nth grandparent. Of course we can always just explicitly use commit IDs but these short hands can come in handy for quick checks without doing <code>git log</code> to retrieve commit IDs."},{"location":"courses/mlops/git/#revert","title":"Revert","text":"<p>But instead of moving the branch pointer to a previous commit, we can continue to move forward by adding a new commit to revert certain previous commits.</p>\n<pre><code># Revert\ngit revert &lt;COMMIT_ID&gt; ...  # rollback specific commits\ngit revert &lt;COMMIT_TO_ROLLBACK_TO&gt;..&lt;COMMIT_TO_ROLLBACK_FROM&gt;  # range\n</code></pre>"},{"location":"courses/mlops/git/#checkout","title":"Checkout","text":"<p>Sometimes we may want to temporarily switch back to a previous commit just to explore or commit some changes. It's best practice to do this in a separate branch and if we want to save our changes, we need to create a separate PR. Note that if you do checkout a previous commit and submit a PR, you may override the commits in between.\n<pre><code># Checkout\ngit checkout -b &lt;BRANCH_NAME&gt; &lt;COMMIT_ID&gt;\n</code></pre></p>"},{"location":"courses/mlops/git/#best-practices","title":"Best practices","text":"<p>There so many different works to work with git and sometimes it can became quickly unruly when fellow developers follow different practices. Here are a few, widely accepted, best practices when it comes to working with commits and branches.</p>"},{"location":"courses/mlops/git/#commits","title":"Commits","text":"<ul>\n<li>Commit often such that each commit has a clear associated change which you can approve / rollback.</li>\n<li>Try and squash commits if you have multiple before pushing to the remote host.</li>\n<li>Avoid monolithic commits (even if you regularly stash and rebase) because it can cause many components to break and creates a code review nightmare.</li>\n<li>Attach meaningful messages to commits so developers know exactly what the PR entails.</li>\n<li>Use tags to represent meaningful and stable releases of your application.\n<pre><code># Tags\ngit tag -a v0.1 -m \"initial release\"\n</code></pre></li>\n<li>Don't delete commit history (reset), instead use revert to rollback and provide reasoning.</li>\n</ul>"},{"location":"courses/mlops/git/#branches","title":"Branches","text":"<ul>\n<li>Create branches when working on a feature, bug, etc. because it makes adding and reverting to the <code>main</code> branch very easy.</li>\n<li>Avoid using cryptic branch names.</li>\n<li>Maintain your <code>main</code> branch as the \"demo ready\" branch that always works.</li>\n<li>Protect branches with rules (especially the <code>main</code> branch).</li>\n</ul>"},{"location":"courses/mlops/git/#tags","title":"Tags","text":"<p>Leverage git tags to mark significant release commits. We can create tags either through the terminal or the online remote interface and this can be done to previous commits as well (in case we forgot).</p>\n<pre><code># Tags\ngit tag  # view all existing tags\ngit tag -a &lt;TAG_NAME&gt; -m \"SGD\"  # create a tag\ngit checkout -b &lt;BRANCH_NAME&gt; &lt;TAG_NAME&gt;  # checkout a specific tag\ngit tag -d &lt;TAG_NAME&gt;  # delete local tag\ngit push origin --delete &lt;TAG_NAME&gt;  # delete remote tag\ngit fetch --all --tags  # fetch all tags from remote\n</code></pre>\n<p>Tag names usually adhere to version naming conventions, such as <code>v1.4.2</code> where the numbers indicate major, minor and bug changes from left to right.</p>\n<p>Upcoming live cohorts</p>\n<p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.\n\n</p>\n     Learn more\n<p></p>\n<p>To cite this content, please use:</p>\n<pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Git - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/jobs-and-services/","title":"Jobs and Services","text":""},{"location":"courses/mlops/jobs-and-services/#intuition","title":"Intuition","text":"<p>Our ML workloads have been responsible for everything from data ingestion to model validation:</p> <p>We can execute these workloads as standalone CLI commands:</p> <pre><code># ML workloads (simplified)\npytest --dataset-loc=$DATASET_LOC tests/data ...          # test data\npython -m pytest tests/code --verbose --disable-warnings  # test code\npython madewithml/train.py --experiment-name \"llm\" ...    # train model\npython madewithml/evaluate.py --run-id $RUN_ID ...        # evaluate model\npytest --run-id=$RUN_ID tests/model ...                   # test model\npython madewithml/serve.py --run_id $RUN_ID               # serve model\n</code></pre> <p>With all of our ML workloads implemented (and tested), we're ready to go to production. In this lesson, we'll learn how to convert our ML workloads from CLI commands into a scalable, fault-tolerant and reproducible workflow.</p> <ol> <li>We'll combine our ML workloads up to (and including) model validation into a workflow.</li> <li>This workflow will then produce model artifacts, which will be saved to our model registry.</li> <li>And finally, we can serve that model behind an API endpoint to use in production.</li> </ol>"},{"location":"courses/mlops/jobs-and-services/#jobs","title":"Jobs","text":""},{"location":"courses/mlops/jobs-and-services/#script","title":"Script","text":"<p>Since we have our CLI commands for our ML workloads, we could just execute them one-by-one on our local machine or Workspace. But for efficiency, we're going to combine them all into one script. We'll organize this under a <code>workloads.sh</code> bash script inside our <code>deploy/jobs</code> directory. Here the workloads are very similar to our CLI commands but we have some additional steps to print and save the logs from each of our workloads. For example, our data validation workload looks like this:</p> <pre><code># deploy/jobs/workloads.sh\nexport RESULTS_FILE=results/test_data_results.txt\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings &gt; $RESULTS_FILE\ncat $RESULTS_FILE\n</code></pre> <p>At the end of our <code>workloads.sh</code> script, we save our model registry (with our saved model artifacts) and the results from the different workloads to S3. We'll use these artifacts and results later on when we deploy our model as a Service.</p> <pre><code># Save to S3\nexport MODEL_REGISTRY=$(python -c \"from madewithml import config; print(config.MODEL_REGISTRY)\")\naws s3 cp $MODEL_REGISTRY s3://madewithml/$GITHUB_USERNAME/mlflow/ --recursive\naws s3 cp results/ s3://madewithml/$GITHUB_USERNAME/results/ --recursive\n</code></pre> <p>Note</p> <p>If you're doing this lesson on your local laptop, you'll have to add the proper AWS credentials and up the S3 buckets for our workloads script to run successfully. <pre><code>export AWS_ACCESS_KEY_ID=\"\"\nexport AWS_SECRET_ACCESS_KEY=\"\"\nexport AWS_SESSION_TOKEN=\"\"\n</code></pre> If you don't want to set up all of this yourself, we highly recommend joining our upcoming live cohort where we'll provide an environment with all of this infrastructure already set up for you so that you just focused on the machine learning.</p>"},{"location":"courses/mlops/jobs-and-services/#configuration","title":"Configuration","text":"<p>Now that we have our single script to execute all workloads, we can execute it with one command (<code>./deploy/jobs/workloads.sh</code>). But even better way is to use Anyscale Jobs to get features like automatic failure handling, email alerts and persisted logs all out of the box for our workloads. And with our <code>cluster_env.yaml</code>, <code>cluster_compute.yaml</code> and <code>workloads.sh</code> files, we can create the configuration for our Anyscale Job with an <code>workloads.yaml</code> file:</p> <pre><code># deploy/jobs/workloads.yaml\nname: workloads\nproject_id: prj_v9izs5t1d6b512ism8c5rkq4wm\ncluster_env: madewithml-cluster-env\ncompute_config: madewithml-cluster-compute\nruntime_env:\nworking_dir: .\nupload_path: s3://madewithml/GokuMohandas/jobs  # &lt;--- CHANGE USERNAME (case-sensitive)\nenv_vars:\nGITHUB_USERNAME: GokuMohandas  # &lt;--- CHANGE USERNAME (case-sensitive)\nentrypoint: bash deploy/jobs/workloads.sh\nmax_retries: 0\n</code></pre> <ul> <li><code>Line 2</code>: name of our Anyscale Job</li> <li><code>Line 3</code>: name of our Anyscale Project (we're organizing it all under the same <code>madewithml</code> project we used for our Workspace setup)</li> <li><code>Line 4</code>: name of our cluster environment</li> <li><code>Line 5</code>: name of our compute configuration</li> <li><code>Line 6-10</code>: runtime environment for our Anyscale Job. The <code>runtime_env</code> here specifies that we should upload our current <code>working_dir</code> to an S3 bucket so that all of our workers when we execute an Anyscale Job have access to the code to use. We also set some environment variables that our workloads will have access to.</li> <li><code>Line 11</code>: entrypoint for our Anyscale Job. This is the command that will be executed when we submit our Anyscale Job.</li> <li><code>Line 12</code>: maximum number of retries for our Anyscale Job. If our Anyscale Job fails, it will automatically retry up to this number of times.</li> </ul> <p>Warning</p> <p>Be sure to update the <code>$GITHUB_USERNAME</code> slots inside our <code>deploy/jobs/workloads.yaml</code> configuration to your own GitHub username. This is used to save your model registry and results to a unique path on our shared S3 bucket (<code>s3://madewithml</code>).</p> <p>Because we're using the exact same cluster environment and compute configuration, what worked during development will work in production. This is a huge benefit of using Anyscale Jobs because we don't have to worry about any environment discrepanices when we deploy our workloads to production. This makes going to production much easier and faster!</p>"},{"location":"courses/mlops/jobs-and-services/#execution","title":"Execution","text":"<p>And now we can execute our Anyscale Job in one line:</p> <pre><code>anyscale job submit deploy/jobs/workloads.yaml\n</code></pre> <pre>\nAuthenticating\n\nOutput\n(anyscale +8.8s) Maximum uptime is disabled for clusters launched by this job.\n(anyscale +8.8s) Job prodjob_zqj3k99va8a5jtd895u3ygraup has been successfully submitted. Current state of job: PENDING.\n(anyscale +8.8s) Query the status of the job with `anyscale job list --job-id prodjob_zqj3k99va8a5jtd895u3ygraup`.\n(anyscale +8.8s) Get the logs for the job with `anyscale job logs --job-id prodjob_zqj3k99va8a5jtd895u3ygraup --follow`.\n(anyscale +8.8s) View the job in the UI at https://console.anyscale.com/jobs/prodjob_zqj3k99va8a5jtd895u3ygraup\n(anyscale +8.8s) Use --follow to stream the output of the job when submitting a job.\n</pre> <p>Tip</p> <p>When we run anyscale cli commands inside our Workspaces, we automatically have our credentials set up for us. But if we're running anyscale cli commands on our local laptop, we'll have to set up the appropriate credentials. <pre><code>export ANYSCALE_HOST=https://console.anyscale.com ANYSCALE_CLI_TOKEN=your_cli_token\n</code></pre></p> <p>We can now go to thie UI link that was provided to us to view the status, logs, etc. of our Anyscale Job.</p> <p>And if we inspect our S3 buckets, we'll can see all the artifacts that have been saved from this Anyscale Job.</p>"},{"location":"courses/mlops/jobs-and-services/#debugging","title":"Debugging","text":"<p>Since we use the exact same cluster (environment and compute) for production as we did for development, we're significantly less likely to run into the environment discrepancy issues that typically arise when going from development to production. However, there can always be small issues that arize from missing credentials, etc. We can easily debug our Anyscale Jobs by inspecting the jobs: Jobs page &gt; choose job &gt; View console logs at the bottom &gt; View Ray workers logs &gt; paste command &gt; Open <code>job-logs</code> directory &gt; View <code>job-driver-raysubmit_XYZ.log</code>. Alternatively, we can also run our Anyscale Job as a Workspace by clicking on the Duplicate as Workspace button the top of a particular Job's page.</p>"},{"location":"courses/mlops/jobs-and-services/#services","title":"Services","text":"<p>After we execute our Anyscale Job, we will have saved our model artifacts to a particular location. We'll now use Anyscale Services to pull from this location to serve our models in production behind a scalable rest endpoint</p>"},{"location":"courses/mlops/jobs-and-services/#script_1","title":"Script","text":"<p>Similar to Anyscale Jobs, we'll start by creating a <code>serve_model.py</code> and a <code>serve_model.yaml</code> configuration:</p> <pre><code># deploy/services/serve_model.py\n\nimport os\nimport subprocess\nfrom madewithml.config import MODEL_REGISTRY  # NOQA: E402\nfrom madewithml.serve import ModelDeployment  # NOQA: E402\n\n# Copy from S3\ngithub_username = os.environ.get(\"GITHUB_USERNAME\")\nsubprocess.check_output([\"aws\", \"s3\", \"cp\", f\"s3://madewithml/{github_username}/mlflow/\", str(MODEL_REGISTRY), \"--recursive\"])\nsubprocess.check_output([\"aws\", \"s3\", \"cp\", f\"s3://madewithml/{github_username}/results/\", \"./\", \"--recursive\"])\n\n# Entrypoint\nrun_id = [line.strip() for line in open(\"run_id.txt\")][0]\nentrypoint = ModelDeployment.bind(run_id=run_id, threshold=0.9)\n\n# Inference\ndata = {\"query\": \"What is the default batch size for map_batches?\"}\nresponse = requests.post(\"http://127.0.0.1:8000/query\", json=data)\nprint(response.json())\n\n\n# Inference\ndata = {\"query\": \"What is the default batch size for map_batches?\"}\nresponse = requests.post(\"http://127.0.0.1:8000/query\", json=data)\nprint(response.json())\n</code></pre> <p>In this script, we first pull our previously saved artifacts from our S3 bucket to our local storage and then define the entrypoint for our model.</p> <p>Tip</p> <p>Recall that we have the option to scale when we define our service inside out <code>madewithml/serve.py</code> script. And we can scale our compute configuration to meet those demands.</p> <pre><code># madewithml/serve.py\n@serve.deployment(route_prefix=\"/\", num_replicas=\"1\", ray_actor_options={\"num_cpus\": 8, \"num_gpus\": 0})\n@serve.ingress(app)\nclass ModelDeployment:\n    pass\n</code></pre>"},{"location":"courses/mlops/jobs-and-services/#configuration_1","title":"Configuration","text":"<p>We can now use this <code>entrypoint</code> that we defined to serve our application:</p> <pre><code># deploy/services/serve_model.yaml\nname: madewithml\nproject_id: prj_v9izs5t1d6b512ism8c5rkq4wm\ncluster_env: madewithml-cluster-env\ncompute_config: madewithml-cluster-compute\nray_serve_config:\nimport_path: deploy.services.serve_model:entrypoint\nruntime_env:\nworking_dir: .\nupload_path: s3://madewithml/GokuMohandas/services  # &lt;--- CHANGE USERNAME (case-sensitive)\nenv_vars:\nGITHUB_USERNAME: GokuMohandas  # &lt;--- CHANGE USERNAME (case-sensitive)\nrollout_strategy: ROLLOUT # ROLLOUT or IN_PLACE\n</code></pre> <ul> <li><code>Line 2</code>: name of our Anyscale Service</li> <li><code>Line 3</code>: name of our Anyscale Project (we're organizing it all under the same <code>madewithml</code> project we used for our Workspace setup)</li> <li><code>Line 4</code>: name of our cluster environment</li> <li><code>Line 5</code>: name of our compute configuration</li> <li><code>Line 6-12</code>: serving configuration that specifies our entry point and details about the working directory, environment variables, etc.</li> <li><code>Line 13</code>: rollout strategy for our Anyscale Service. We can either rollout a new version of our service or replace the existing version with the new one.</li> </ul> <p>Warning</p> <p>Be sure to update the <code>$GITHUB_USERNAME</code> slots inside our <code>deploy/services/serve_model.yaml</code> configuration to your own GitHub username. This is used to pull model artifacts and results from our shared S3 bucket (<code>s3://madewithml</code>).</p>"},{"location":"courses/mlops/jobs-and-services/#execution_1","title":"Execution","text":"<p>And now we can execute our Anyscale Service in one line:</p> <pre><code># Rollout service\nanyscale service rollout -f deploy/services/serve_model.yaml\n</code></pre> <pre>\nAuthenticating\n\nOutput\n(anyscale +7.3s) Service service2_xwmyv1wcm3i7qan2sahsmybymw has been deployed. Service is transitioning towards: RUNNING.\n(anyscale +7.3s) View the service in the UI at https://console.anyscale.com/services/service2_xwmyv1wcm3i7qan2sahsmybymw\n</pre> <p>Note</p> <p>If we chose the <code>ROLLOUT</code> strategy, we get a canary rollout (increasingly serving traffic to the new version of our service) by default.</p> <p>Once our service is up and running, we can query it:</p> <pre><code># Query\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $SECRET_TOKEN\" -d '{\n  \"title\": \"Transfer learning with transformers\",\n  \"description\": \"Using transformers for transfer learning on text classification tasks.\"\n}' $SERVICE_ENDPOINT/predict/\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"prediction\": \"natural-language-processing\",\n\"probabilities\": {\n\"computer-vision\": 3.175719175487757E-4,\n\"mlops\": 4.065348766744137E-4,\n\"natural-language-processing\": 0.9989110231399536,\n\"other\": 3.6489960621111095E-4\n}\n}\n]\n}\n</code></pre> <p>And we can just as easily rollback to a previous version of our service or terminate it altogether:</p> <pre><code># Rollback (to previous version of the Service)\nanyscale service rollback -f $SERVICE_CONFIG --name $SERVICE_NAME\n\n# Terminate\nanyscale service terminate --name $SERVICE_NAME\n</code></pre>"},{"location":"courses/mlops/jobs-and-services/#observability","title":"Observability","text":"<p>Once we rollout our service, we have several different dashboards that we can use to monitor our service. We can access these dashboards by going to the Services page &gt; choose service &gt; Click the <code>Dashboard</code> button (top right corner) &gt; Ray Dashboard. Here we'll able to see the logs from our Service, metrics, etc.</p> <p>On the same Dashboard button, we also have a Metrics option that will take us to a Grafana Dashboard. This view has a lot more metrics on incoming requests, latency, errors, etc.</p>"},{"location":"courses/mlops/jobs-and-services/#debugging_1","title":"Debugging","text":"<p>Serving our models may not always work as intended. Even if our model serving logic is correct, there are external dependencies that could causes errors --- such as our model not being stored where it should be, trouble accessing our model registry, etc. For all these cases and more, it's very important to know how to be able to debug our Services.</p> <p>Services page &gt; choose service &gt; Go to Resource usage section &gt; Click on the cluster link (<code>cluster_for_service_XYZ</code>) &gt; Ray logs (tab at bottom) &gt; paste command &gt; Open <code>worker-XYZ</code> directory &gt; View <code>combined_worker.log</code></p>"},{"location":"courses/mlops/jobs-and-services/#scaling","title":"Scaling","text":"<p>The combination of using Workspaces for development and Job &amp; Services for production make it extremely easy and fast to make the transition. The cluster environment and compute configurations are the exact same so the code that's executing runs on the same conditions. However, we may sometimes want to scale up our production compute configurations to execute Jobs faster or meet the availability/latency demands for our Services. We could address this by creating a new compute configuration:</p> <pre><code># Compute config\nexport CLUSTER_COMPUTE_NAME=\"madewithml-cluster-compute-prod\"\nanyscale cluster-compute create deploy/cluster_compute_prod.yaml --name $CLUSTER_COMPUTE_NAME  # uses new config with prod compute requirements\n</code></pre> <p>or by using a one-off configuration to specify the compute changes, where instead of pointing to a previously existing compute configuration, we can define it directly in our Jobs/Services yaml configuration:</p> <pre><code>name: madewithml\nproject_id: prj_v9izs5t1d6b512ism8c5rkq4wm\ncluster_env: madewithml-cluster-env\ncompute_config:\ncloud: anyscale-v2-cloud-fast-startup\nmax_workers: 20\nhead_node_type:\nname: head_node_type\ninstance_type: m5.4xlarge\nworker_node_types:\n- name: gpu_worker\ninstance_type: g4dn.4xlarge\nmin_workers: 1\nmax_workers: 8\naws:\nBlockDeviceMappings:\n- DeviceName: \"/dev/sda1\"\nEbs:\nVolumeSize: 500\nDeleteOnTermination: true\n...\n</code></pre> <p>And with that, we're able to completely productionize our ML workloads! We have a working service that we can use to make predictions using our trained model. However, what happens when we receive new data or our model's performance regresses over time? With our current approach here, we have to manually execute our Jobs and Services again to udpate our application. In the next lesson, we'll learn how to automate this process with CI/CD workflows that execute our Jobs and Services based on an event (e.g. new data).</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Jobs &amp; Services - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/labeling/","title":"Data Labeling","text":""},{"location":"courses/mlops/labeling/#what-is-data-labeling","title":"What is data labeling","text":"<p>Labeling (or annotation) is the process of identifying the inputs and outputs that are worth modeling (not just what could be modeled).</p> <ul> <li>use objective as a guide to determine the necessary signals.</li> <li>explore creating new signals (via combining features, collecting new data, etc.).</li> <li>iteratively add more features to justify complexity and effort.</li> </ul> <p>It's really important to get our labeling workflows in place before we start performing downstream tasks such as data augmentation, model training, etc.</p> <p>Warning</p> <p>Be careful not to include features in the dataset that will not be available during prediction, causing data leaks.</p> <p>What else can we learn?</p> <p>It's not just about identifying and labeling our initial dataset. What else can we learn from it?</p> Show answer <p>It's also the phase where we can use our deep understanding of the problem to:</p> <pre><code>- augment the training data split\n- enhance using auxiliary datasets\n- simplify using constraints\n- remove noisy samples\n- improve the labeling process\n</code></pre>"},{"location":"courses/mlops/labeling/#process","title":"Process","text":"<p>Regardless of whether we have a custom labeling platform or we choose a generalized platform, the process of labeling and all it's related workflows (QA, data import/export, etc.) follow a similar approach.</p>"},{"location":"courses/mlops/labeling/#preliminary-steps","title":"Preliminary steps","text":"<ul> <li><code>[WHAT]</code> Decide what needs to be labeled:<ul> <li>identify natural labels you may already have (ex. time-series)</li> <li>consult with domain experts to ensure you're labeling the appropriate signals</li> <li>decide on the appropriate labels (and hierarchy) for your task</li> </ul> </li> <li><code>[WHERE]</code> Design the labeling interface:<ul> <li>intuitive, data modality dependent and quick (keybindings are a must!)</li> <li>avoid option paralysis by allowing the labeler to dig deeper or suggesting likely labels</li> <li>measure and resolve inter-labeler discrepancy</li> </ul> </li> <li><code>[HOW]</code> Compose labeling instructions:<ul> <li>examples of each labeling scenario</li> <li>course of action for discrepancies</li> </ul> </li> </ul> Multi-label text classification for our task using  Prodigy (labeling + QA)"},{"location":"courses/mlops/labeling/#workflow-setup","title":"Workflow setup","text":"<ul> <li>Establish data pipelines:<ul> <li><code>[IMPORT]</code> new data for annotation</li> <li><code>[EXPORT]</code> annotated data for QA, testing, modeling, etc.</li> </ul> </li> <li>Create a quality assurance (QA) workflow:<ul> <li>separate from labeling workflow (no bias)</li> <li>communicates with labeling workflow to escalate errors</li> </ul> </li> </ul>"},{"location":"courses/mlops/labeling/#iterative-setup","title":"Iterative setup","text":"<ul> <li>Implement strategies to reduce labeling efforts<ul> <li>identify subsets of the data to label next using active learning</li> <li>auto-label entire or parts of a dataset using weak supervision</li> <li>focus labeling efforts on long tail of edge cases over time</li> </ul> </li> </ul>"},{"location":"courses/mlops/labeling/#labeled-data","title":"Labeled data","text":"<p>For the purpose of this course, our data is already labeled, so we'll perform a basic version of ELT (extract, load, transform) to construct the labeled dataset.</p> <p>In our data-stack and orchestration lessons, we'll construct a modern data stack and programmatically deliver high quality data via DataOps workflows.</p> <ul> <li>projects.csv: projects with id, created time, title and description.</li> <li>tags.csv: labels (tag category) for the projects by id.</li> </ul> <p>Recall that our objective was to classify incoming content so that the community can discover them easily. These data assets will act as the training data for our first model.</p>"},{"location":"courses/mlops/labeling/#extract","title":"Extract","text":"<p>We'll start by extracting data from our sources (external CSV files). Traditionally, our data assets will be stored, versioned and updated in a database, warehouse, etc. We'll learn more about these different data systems later, but for now, we'll load our data as a stand-alone CSV file.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code># Extract projects\nPROJECTS_URL = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\"\nprojects = pd.read_csv(PROJECTS_URL)\nprojects.head(5)\n</code></pre> <pre>\nid\n      created_on\n      title\n      description\n    0\n      6\n      2020-02-20 06:43:18\n      Comparison between YOLO and RCNN on real world...\n      Bringing theory to experiment is cool. We can ...\n    1\n      7\n      2020-02-20 06:47:21\n      Show, Infer &amp; Tell: Contextual Inference for C...\n      The beauty of the work lies in the way it arch...\n    2\n      9\n      2020-02-24 16:24:45\n      Awesome Graph Classification\n      A collection of important graph embedding, cla...\n    3\n      15\n      2020-02-28 23:55:26\n      Awesome Monte Carlo Tree Search\n      A curated list of Monte Carlo tree search papers...\n    4\n      19\n      2020-03-03 13:54:31\n      Diffusion to Vector\n      Reference implementation of Diffusion2Vec (Com...\n    </pre> <p>We'll also load the labels (tag category) for our projects.</p> <pre><code># Extract tags\nTAGS_URL = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\"\ntags = pd.read_csv(TAGS_URL)\ntags.head(5)\n</code></pre> <pre>\nid\n      tag\n    0\n      6\n      computer-vision\n    1\n      7\n      computer-vision\n    2\n      9\n      graph-learning\n    3\n      15\n      reinforcement-learning\n    4\n      19\n      graph-learning\n    </pre>"},{"location":"courses/mlops/labeling/#transform","title":"Transform","text":"<p>Apply basic transformations to create our labeled dataset.</p> <pre><code># Join projects and tags\ndf = pd.merge(projects, tags, on=\"id\")\ndf.head()\n</code></pre> <pre>\nid\n      created_on\n      title\n      description\n      tag\n    0\n      6\n      2020-02-20 06:43:18\n      Comparison between YOLO and RCNN on real world...\n      Bringing theory to experiment is cool. We can ...\n      computer-vision\n    1\n      7\n      2020-02-20 06:47:21\n      Show, Infer &amp; Tell: Contextual Inference for C...\n      The beauty of the work lies in the way it arch...\n      computer-vision\n    2\n      9\n      2020-02-24 16:24:45\n      Awesome Graph Classification\n      A collection of important graph embedding, cla...\n      graph-learning\n    3\n      15\n      2020-02-28 23:55:26\n      Awesome Monte Carlo Tree Search\n      A curated list of Monte Carlo tree search papers...\n      reinforcement-learning\n    4\n      19\n      2020-03-03 13:54:31\n      Diffusion to Vector\n      Reference implementation of Diffusion2Vec (Com...\n      graph-learning\n    </pre> <pre><code>df = df[df.tag.notnull()]  # remove projects with no tag\n</code></pre>"},{"location":"courses/mlops/labeling/#load","title":"Load","text":"<p>Finally, we'll load our transformed data locally so that we can use it for our machine learning application.</p> <pre><code># Save locally\ndf.to_csv(\"labeled_projects.csv\", index=False)\n</code></pre>"},{"location":"courses/mlops/labeling/#libraries","title":"Libraries","text":"<p>We could have used the user provided tags as our labels but what if the user added a wrong tag or forgot to add a relevant one. To remove this dependency on the user to provide the gold standard labels, we can leverage labeling tools and platforms. These tools allow for quick and organized labeling of the dataset to ensure its quality. And instead of starting from scratch and asking our labeler to provide all the relevant tags for a given project, we can provide the author's original tags and ask the labeler to add / remove as necessary. The specific labeling tool may be something that needs to be custom built or leverages something from the ecosystem.</p> <p>As our platform grows, so too will our dataset and labeling needs so it's imperative to use the proper tooling that supports the workflows we'll depend on.</p>"},{"location":"courses/mlops/labeling/#general","title":"General","text":"<ul> <li>Labelbox: the data platform for high quality training and validation data for AI applications.</li> <li>Scale AI: data platform for AI that provides high quality training data.</li> <li>Label Studio: a multi-type data labeling and annotation tool with standardized output format.</li> <li>Universal Data Tool: collaborate and label any type of data, images, text, or documents in an easy web interface or desktop app.</li> <li>Prodigy: recipes for the Prodigy, our fully scriptable annotation tool.</li> <li>Superintendent: an ipywidget-based interactive labelling tool for your data to enable active learning.</li> </ul>"},{"location":"courses/mlops/labeling/#natural-language-processing","title":"Natural language processing","text":"<ul> <li>Doccano: an open source text annotation tool for text classification, sequence labeling and sequence to sequence tasks.</li> <li>BRAT: a rapid annotation tool for all your textual annotation needs.</li> </ul>"},{"location":"courses/mlops/labeling/#computer-vision","title":"Computer vision","text":"<ul> <li>LabelImg: a graphical image annotation tool and label object bounding boxes in images.</li> <li>CVAT: a free, online, interactive video and image annotation tool for computer vision.</li> <li>VoTT: an electron app for building end-to-end object detection models from images and videos.</li> <li>makesense.ai: a free to use online tool for labelling photos.</li> <li>remo: an app for annotations and images management in computer vision.</li> <li>Labelai: an online tool designed to label images, useful for training AI models.</li> </ul>"},{"location":"courses/mlops/labeling/#audio","title":"Audio","text":"<ul> <li>Audino: an open source audio annotation tool for voice activity detection (VAD), diarization, speaker identification, automated speech recognition, emotion recognition tasks, etc.</li> <li>audio-annotator: a JavaScript interface for annotating and labeling audio files.</li> <li>EchoML: a web app to play, visualize, and annotate your audio files for machine learning.</li> </ul>"},{"location":"courses/mlops/labeling/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>MedCAT: a medical concept annotation tool that can extract information from Electronic Health Records (EHRs) and link it to biomedical ontologies like SNOMED-CT and UMLS.</li> </ul> <p>Generalized labeling solutions</p> <p>What criteria should we use to evaluate what labeling platform to use?</p> Show answer <p>It's important to pick a generalized platform that has all the major labeling features for your data modality with the capability to easily customize the experience.</p> <ul> <li>how easy is it to connect to our data sources (DB, QA, etc.)?</li> <li>how easy was it to make changes (new features, labeling paradigms)?</li> <li>how securely is our data treated (on-prem, trust, etc.)</li> </ul> <p>However, as an industry trend, this balance between generalization and specificity is difficult to strike. So many teams put in the upfront effort to create bespoke labeling platforms or used industry specific, niche,  labeling tools.</p>"},{"location":"courses/mlops/labeling/#active-learning","title":"Active learning","text":"<p>Even with a powerful labeling tool and established workflows, it's easy to see how involved and expensive labeling can be. Therefore, many teams employ active learning to iteratively label the dataset and evaluate the model.</p> <ol> <li>Label a small, initial dataset to train a model.</li> <li>Ask the trained model to predict on some unlabeled data.</li> <li>Determine which new data points to label from the unlabeled data based on:<ul> <li>entropy over the predicted class probabilities</li> <li>samples with lowest predicted, calibrated, confidence (uncertainty sampling)</li> <li>discrepancy in predictions from an ensemble of trained models</li> </ul> </li> <li>Repeat until the desired performance is achieved.</li> </ol> <p>This can be significantly more cost-effective and faster than labeling the entire dataset.</p> Active Learning Literature Survey"},{"location":"courses/mlops/labeling/#libraries_1","title":"Libraries","text":"<ul> <li>modAL: a modular active learning framework for Python.</li> <li>libact: pool-based active learning in Python.</li> <li>ALiPy: active learning python toolbox, which allows users to conveniently evaluate, compare and analyze the performance of active learning methods.</li> </ul>"},{"location":"courses/mlops/labeling/#weak-supervision","title":"Weak supervision","text":"<p>If we had samples that needed labeling or if we simply wanted to validate existing labels, we can use weak supervision to generate labels as opposed to hand labeling all of them. We could utilize weak supervision via labeling functions to label our existing and new data, where we can create constructs based on keywords, pattern expressions, knowledge bases, etc. And we can add to the labeling functions over time and even mitigate conflicts amongst the different labeling functions. We'll use these labeling functions to create and evaluate slices of our data in the evaluation lesson.</p> <pre><code>from snorkel.labeling import labeling_function\n\n@labeling_function()\ndef contains_tensorflow(text):\n    condition = any(tag in text.lower() for tag in (\"tensorflow\", \"tf\"))\n    return \"tensorflow\" if condition else None\n</code></pre> <p>An easy way to validate our labels (before modeling) is to use the aliases in our auxillary datasets to create labeling functions for the different classes. Then we can look for false positives and negatives to identify potentially mislabeled samples. We'll actually implement a similar kind of inspection approach, but using a trained model as a heuristic, in our dashboards lesson.</p>"},{"location":"courses/mlops/labeling/#iteration","title":"Iteration","text":"<p>Labeling isn't just a one time event or something we repeat identically. As new data is available, we'll want to strategically label the appropriate samples and improve slices of our data that are lacking in quality. Once new data is labeled, we can have workflows that are triggered to start the (re)training process to deploy a new version of our system.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Data Labeling - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/logging/","title":"Logging for ML Systems","text":""},{"location":"courses/mlops/logging/#intuition","title":"Intuition","text":"<p>Logging is the process of tracking and recording key events that occur in our applications for the purpose of inspection, debugging, etc. They're a whole lot more powerful than <code>print</code> statements because they allow us to send specific pieces of information to specific locations with custom formatting, shared interfaces, etc. This makes logging a key proponent in being able to surface insightful information from the internal processes of our application.</p>"},{"location":"courses/mlops/logging/#components","title":"Components","text":"<p>There are a few overarching concepts to be aware of:</p> <ul> <li><code>Logger</code>: emits the log messages from our application.</li> <li><code>Handler</code>: sends log records to a specific location.</li> <li><code>Formatter</code>: formats and styles the log records.</li> </ul> <p>There is so much more to logging such as filters, exception logging, etc. but these basics will allows us to do everything we need for our application.</p>"},{"location":"courses/mlops/logging/#levels","title":"Levels","text":"<p>Before we create our specialized, configured logger, let's look at what logged messages look like by using the basic configuration. <pre><code>import logging\nimport sys\n\n# Create super basic logger\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\n# Logging levels (from lowest to highest priority)\nlogging.debug(\"Used for debugging your code.\")\nlogging.info(\"Informative messages from your code.\")\nlogging.warning(\"Everything works but there is something to be aware of.\")\nlogging.error(\"There's been a mistake with the process.\")\nlogging.critical(\"There is something terribly wrong and process may terminate.\")\n</code></pre></p> <pre>\nDEBUG:root:Used for debugging your code.\nINFO:root:Informative messages from your code.\nWARNING:root:Everything works but there is something to be aware of.\nERROR:root:There's been a mistake with the process.\nCRITICAL:root:There is something terribly wrong and process may terminate.\n</pre> <p>These are the basic levels of logging, where <code>DEBUG</code> is the lowest priority and <code>CRITICAL</code> is the highest. We defined our logger using <code>basicConfig</code> to emit log messages to stdout (ie. our terminal console), but we also could've written to any other stream or even a file. We also defined our logging to be sensitive to log messages starting from level <code>DEBUG</code>. This means that all of our logged messages will be displayed since <code>DEBUG</code> is the lowest level. Had we made the level <code>ERROR</code>, then only <code>ERROR</code> and <code>CRITICAL</code> log message would be displayed.</p> <pre><code>import logging\nimport sys\n\n# Create super basic logger\nlogging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n# Logging levels (from lowest to highest priority)\nlogging.debug(\"Used for debugging your code.\")\nlogging.info(\"Informative messages from your code.\")\nlogging.warning(\"Everything works but there is something to be aware of.\")\nlogging.error(\"There's been a mistake with the process.\")\nlogging.critical(\"There is something terribly wrong and process may terminate.\")\n</code></pre> <pre>\nERROR:root:There's been a mistake with the process.\nCRITICAL:root:There is something terribly wrong and process may terminate.\n</pre>"},{"location":"courses/mlops/logging/#configuration","title":"Configuration","text":"<p>First we'll set the location of our logs in our <code>config.py</code> script:</p> <pre><code># madewithml/config.py\nLOGS_DIR = Path(BASE_DIR, \"logs\")\nLOGS_DIR.mkdir(parents=True, exist_ok=True)\n</code></pre> <p>Next, we'll configure the logger for our application:</p> <pre><code># madewithml/config.py\nimport logging\nimport sys\nlogging_config = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"formatters\": {\n        \"minimal\": {\"format\": \"%(message)s\"},\n        \"detailed\": {\n            \"format\": \"%(levelname)s %(asctime)s [%(name)s:%(filename)s:%(funcName)s:%(lineno)d]\\n%(message)s\\n\"\n        },\n    },\n    \"handlers\": {\n        \"console\": {\n            \"class\": \"logging.StreamHandler\",\n            \"stream\": sys.stdout,\n            \"formatter\": \"minimal\",\n            \"level\": logging.DEBUG,\n        },\n        \"info\": {\n            \"class\": \"logging.handlers.RotatingFileHandler\",\n            \"filename\": Path(LOGS_DIR, \"info.log\"),\n            \"maxBytes\": 10485760,  # 1 MB\n            \"backupCount\": 10,\n            \"formatter\": \"detailed\",\n            \"level\": logging.INFO,\n        },\n        \"error\": {\n            \"class\": \"logging.handlers.RotatingFileHandler\",\n            \"filename\": Path(LOGS_DIR, \"error.log\"),\n            \"maxBytes\": 10485760,  # 1 MB\n            \"backupCount\": 10,\n            \"formatter\": \"detailed\",\n            \"level\": logging.ERROR,\n        },\n    },\n    \"root\": {\n        \"handlers\": [\"console\", \"info\", \"error\"],\n        \"level\": logging.INFO,\n        \"propagate\": True,\n    },\n}\n</code></pre> <ol> <li><code>[Lines 6-11]</code>: define two different Formatters (determine format and style of log messages), minimal and detailed, which use various LogRecord attributes to create a formatting template for log messages.</li> <li><code>[Lines 12-35]</code>: define the different Handlers (details about location of where to send log messages):<ul> <li><code>console</code>: sends log messages (using the <code>minimal</code> formatter) to the <code>stdout</code> stream for messages above level <code>DEBUG</code> (ie. all logged messages).</li> <li><code>info</code>: send log messages (using the <code>detailed</code> formatter) to <code>logs/info.log</code> (a file that can be up to <code>1 MB</code> and we'll backup the last <code>10</code> versions of it) for messages above level <code>INFO</code>.</li> <li><code>error</code>: send log messages (using the <code>detailed</code> formatter) to <code>logs/error.log</code> (a file that can be up to <code>1 MB</code> and we'll backup the last <code>10</code> versions of it) for messages above level <code>ERROR</code>.</li> </ul> </li> <li><code>[Lines 36-40]</code>: attach our different handlers to our root Logger.</li> </ol> <p>We chose to use a dictionary to configure our logger but there are other ways such as Python script, configuration file, etc. Click on the different options below to expand and view the respective implementation.</p> Python script <pre><code>import logging\nfrom rich.logging import RichHandler\n\n# Get root logger\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\n\n# Create handlers\nconsole_handler = RichHandler(markup=True)\nconsole_handler.setLevel(logging.DEBUG)\ninfo_handler = logging.handlers.RotatingFileHandler(\n    filename=Path(LOGS_DIR, \"info.log\"),\n    maxBytes=10485760,  # 1 MB\n    backupCount=10,\n)\ninfo_handler.setLevel(logging.INFO)\nerror_handler = logging.handlers.RotatingFileHandler(\n    filename=Path(LOGS_DIR, \"error.log\"),\n    maxBytes=10485760,  # 1 MB\n    backupCount=10,\n)\nerror_handler.setLevel(logging.ERROR)\n\n# Create formatters\nminimal_formatter = logging.Formatter(fmt=\"%(message)s\")\ndetailed_formatter = logging.Formatter(\n    fmt=\"%(levelname)s %(asctime)s [%(name)s:%(filename)s:%(funcName)s:%(lineno)d]\\n%(message)s\\n\"\n)\n\n# Hook it all up\nconsole_handler.setFormatter(fmt=minimal_formatter)\ninfo_handler.setFormatter(fmt=detailed_formatter)\nerror_handler.setFormatter(fmt=detailed_formatter)\nlogger.addHandler(hdlr=console_handler)\nlogger.addHandler(hdlr=info_handler)\nlogger.addHandler(hdlr=error_handler)\n</code></pre> Configuration file <ol> <li> <p>Place this inside a <code>logging.config</code> file: <pre><code>[formatters]\nkeys=minimal,detailed\n\n[formatter_minimal]\nformat=%(message)s\n\n[formatter_detailed]\nformat=\n    %(levelname)s %(asctime)s [%(name)s:%(filename)s:%(funcName)s:%(lineno)d]\n    %(message)s\n\n[handlers]\nkeys=console,info,error\n\n[handler_console]\nclass=StreamHandler\nlevel=DEBUG\nformatter=minimal\nargs=(sys.stdout,)\n\n[handler_info]\nclass=handlers.RotatingFileHandler\nlevel=INFO\nformatter=detailed\nbackupCount=10\nmaxBytes=10485760\nargs=(\"logs/info.log\",)\n\n[handler_error]\nclass=handlers.RotatingFileHandler\nlevel=ERROR\nformatter=detailed\nbackupCount=10\nmaxBytes=10485760\nargs=(\"logs/error.log\",)\n\n[loggers]\nkeys=root\n\n[logger_root]\nlevel=INFO\nhandlers=console,info,error\n</code></pre></p> </li> <li> <p>Place this inside your Python script: <pre><code>import logging\nimport logging.config\nfrom rich.logging import RichHandler\n\n# Use config file to initialize logger\nlogging.config.fileConfig(Path(CONFIG_DIR, \"logging.config\"))\nlogger = logging.getLogger()\nlogger.handlers[0] = RichHandler(markup=True)  # set rich handler\n</code></pre></p> </li> </ol> <p>We can load our logger configuration dict like so:</p> <pre><code># madewithml/config.py\nimport logging\n\n# Logger\nlogging_config = {...}\nlogging.config.dictConfig(logging_config)\nlogger = logging.getLogger()\n\n# Sample messages (note that we use configured `logger` now)\nlogger.debug(\"Used for debugging your code.\")\nlogger.info(\"Informative messages from your code.\")\nlogger.warning(\"Everything works but there is something to be aware of.\")\nlogger.error(\"There's been a mistake with the process.\")\nlogger.critical(\"There is something terribly wrong and process may terminate.\")\n</code></pre> <pre>\nDEBUG    Used for debugging your code.                                 config.py:71\nINFO     Informative messages from your code.                          config.py:72\nWARNING  Everything works but there is something to be aware of.       config.py:73\nERROR    There's been a mistake with the process.                      config.py:74\nCRITICAL There is something terribly wrong and process may terminate.  config.py:75\n</pre> <p>Our logged messages become stored inside the respective files in our logs directory:</p> <pre><code>logs/\n    \u251c\u2500\u2500 info.log\n    \u2514\u2500\u2500 error.log\n</code></pre> <p>And since we defined a detailed formatter, we would see informative log messages like these:</p> <pre>\nINFO 2020-10-21 11:18:42,102 [config.py:module:72]\nInformative messages from your code.\n</pre>"},{"location":"courses/mlops/logging/#implementation","title":"Implementation","text":"<p>In our project, we can replace all of our print statements into logging statements:</p> <pre><code>print(\"\u2705 Training complete!\")\n</code></pre>      \u2500\u2500\u2500\u2500 \u00a0 becomes: \u00a0 \u2500\u2500\u2500\u2500  <pre><code>from config import logger\nlogger.info(\"\u2705 Training complete!\")\n</code></pre> <p>All of our log messages are at the <code>INFO</code> level but while developing we may have had to use <code>DEBUG</code> levels and we also add some <code>ERROR</code> or <code>CRITICAL</code> log messages if our system behaves in an unintended manner.</p> <ul> <li> <p>what: log all the necessary details you want to surface from our application that will be useful during development and afterwards for retrospective inspection.</p> </li> <li> <p>where: a best practice is to not clutter our modular functions with log statements. Instead we should log messages outside of small functions and inside larger workflows. For example, there are no log messages inside any of our scripts except the <code>main.py</code> and <code>train.py</code> files. This is because these scripts use the smaller functions defined in the other scripts (data.py, evaluate.py, etc.). If we ever feel that we the need to log within our other functions, then it usually indicates that the function needs to be broken down further.</p> </li> </ul> <p>When it comes to saving our logs, we could simply upload our logs to a cloud blog storage (ex. S3 or Google Cloud Storage). Or for a more production-grade logging option, we could consider the Elastic stack.</p> <p>In the next lesson, we'll learn how to document our code and automatically generate high quality docs for our application.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Logging - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/makefile/","title":"Makefiles","text":""},{"location":"courses/mlops/makefile/#intuition","title":"Intuition","text":"<p>Throughout our development so far, there are so many different commands to keep track of. To help organize everything, we're going to use a <code>Makefile</code> which is a automation tool that organizes our commands. We'll start by create this file in our project's root directory.</p> <pre><code>touch Makefile\n</code></pre> <p>At the top of our <code>Makefile</code> we need to specify the shell environment we want all of our commands to execute in:</p> <pre><code># Makefile\nSHELL = /bin/bash\n</code></pre>"},{"location":"courses/mlops/makefile/#components","title":"Components","text":"<p>Inside our Makefile, we'll be creating a list of rules. These rules have a <code>target</code> which can sometimes have <code>prerequisites</code> that need to be met (can be other targets) and on the next line a Tab followed by a <code>recipe</code> which specifies how to create the target.</p> <pre><code># Makefile\ntarget: prerequisites\n&lt;TAB&gt; recipe\n</code></pre> <p>For example, if we wanted to create a rule for styling our files, we would add the following to our <code>Makefile</code>:</p> <pre><code># Styling\nstyle:\n    black .\n    flake8\n    python3 -m isort .\n</code></pre> <p>Tabs vs. spaces</p> <p>Makefiles require that indention be done with a , instead of spaces where we'll receive an error: <pre>\nMakefile:: *** missing separator.  Stop.\n\nLuckily, editors like VSCode automatically change indentation to tabs even if other files use spaces."},{"location":"courses/mlops/makefile/#targets","title":"Targets","text":"<p>We can execute any of the rules by typing <code>make &lt;target&gt;</code> in the terminal:</p>\n<pre><code># Make a target\n$ make style\n</code></pre>\n<pre>\nblack .\nAll done! \u2728 \ud83c\udf70 \u2728\n8 files left unchanged.\nflake8\npython3 -m isort .\nSkipped 1 files\n</pre>\n\n<p>Similarly, we can set up our <code>Makefile</code> for creating a virtual environment:</p>\n<pre><code># Environment\nvenv:\n    python3 -m venv venv\n    source venv/bin/activate &amp;&amp; \\\npython3 -m pip install pip setuptools wheel &amp;&amp; \\\npython3 -m pip install -e .\n</code></pre>\n<p><code>&amp;&amp;</code> signifies that we want these commands to execute in one shell (more on this below).</p>"},{"location":"courses/mlops/makefile/#phony","title":"PHONY","text":"<p>A Makefile is called as such because traditionally the <code>targets</code> are supposed to be files we can make. However, Makefiles are also commonly used as command shortcuts, which can lead to confusion when a Makefile target and a file share the same name! For example if we have a file called <code>venv</code> (which we do) and a <code>target</code> in your Makefile called <code>venv</code>, when you run <code>make venv</code> we'll get this message:</p>\n<pre><code>$ make venv\n</code></pre>\n<pre>\nmake: `venv' is up to date.\n</pre>\n\n<p>In this situation, this is the intended behavior because if a virtual environment already exists, then we don't want ot make that target again. However, sometimes, we'll name our targets and want them to execute whether it exists as an actual file or not. In these scenarios, we want to define a <code>PHONY</code> target in our makefile by adding this line above the target:\n<pre><code>.PHONY: &lt;target_name&gt;\n</code></pre></p>\n<p>Most of the rules in our Makefile will require the <code>PHONY</code> target because we want them to execute even if there is a file sharing the target's name.</p>\n<pre><code># Styling\n.PHONY: style\nstyle:\n    black .\n    flake8\n    isort .\n</code></pre>"},{"location":"courses/mlops/makefile/#prerequisites","title":"Prerequisites","text":"<p>Before we make a target, we can attach prerequisites to them. These can either be file targets that must exist or PHONY target commands that need to be executed prior to making this target. For example, we'll set the style target as a prerequisite for the clean target so that all files are formatted appropriately prior to cleaning them.</p>\n<pre><code># Cleaning\n.PHONY: clean\nclean: style\n    find . -type f -name \"*.DS_Store\" -ls -delete\n    find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n    find . | grep -E \".pytest_cache\" | xargs rm -rf\n    find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n    find . | grep -E \".trash\" | xargs rm -rf\n    rm -f .coverage\n</code></pre>"},{"location":"courses/mlops/makefile/#variables","title":"Variables","text":"<p>We can also set and use variables inside our Makefile to organize all of our rules.</p>\n<ul>\n<li>\n<p>We can set the variables directly inside the Makefile. If the variable isn't defined in the Makefile, then it would default to any environment variable with the same name.\n<pre><code># Set variable\nMESSAGE := \"hello world\"\n\n# Use variable\ngreeting:\n    @echo ${MESSAGE}\n</code></pre></p>\n</li>\n<li>\n<p>We can also use variables passed in when executing the rule like so (ensure that the variable is not overridden inside the Makefile):\n<pre><code>make greeting MESSAGE=\"hi\"\n</code></pre></p>\n</li>\n</ul>"},{"location":"courses/mlops/makefile/#shells","title":"Shells","text":"<p>Each line in a recipe for a rule will execute in a separate sub-shell. However for certain recipes such as activating a virtual environment and loading packages, we want to execute all steps in one shell. To do this, we can add the <code>.ONESHELL</code> special target above any target.</p>\n<pre><code># Environment\n.ONESHELL:\nvenv:\n    python3 -m venv venv\n    source venv/bin/activate\n    python3 -m pip install pip setuptools wheel\n    python3 -m pip install -e .\n</code></pre>\n<p>However this is only available in Make version 3.82 and above and most Macs currently use version 3.81. You can either update to the current version or chain your commands with <code>&amp;&amp;</code> as we did previously:</p>\n<pre><code># Environment\nvenv:\n    python3 -m venv venv\n    source venv/bin/activate &amp;&amp; \\\npython3 -m pip install pip setuptools wheel &amp;&amp; \\\npython3 -m pip install -e .\n</code></pre>"},{"location":"courses/mlops/makefile/#help","title":"Help","text":"<p>The last thing we'll add to our <code>Makefile</code> (for now at least) is a <code>help</code> target to the very top. This rule will provide an informative message for this Makefile's capabilities:</p>\n<pre><code>.PHONY: help\nhelp:\n    @echo \"Commands:\"\n@echo \"venv    : creates a virtual environment.\"\n@echo \"style   : executes style formatting.\"\n@echo \"clean   : cleans all unnecessary files.\"\n</code></pre>\n<pre><code>make help\n</code></pre>\n<pre>\nCommands:\nvenv    : creates a virtual environment.\nstyle   : executes style formatting.\nclean   : cleans all unnecessary files.\n</pre>\n\n<p>There's a whole lot more to Makefiles but this is plenty for most applied ML projects.</p>\n<p>Upcoming live cohorts</p>\n<p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.\n\n</p>\n     Learn more\n<p></p>\n<p>To cite this content, please use:</p>\n<pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Makefiles - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/monitoring/","title":"Monitoring Machine Learning Systems","text":""},{"location":"courses/mlops/monitoring/#intuition","title":"Intuition","text":"<p>Even though we've trained and thoroughly evaluated our model, the real work begins once we deploy to production. This is one of the fundamental differences between traditional software engineering and ML development. Traditionally, with rule based, deterministic, software, the majority of the work occurs at the initial stage and once deployed, our system works as we've defined it. But with machine learning, we haven't explicitly defined how something works but used data to architect a probabilistic solution. This approach is subject to natural performance degradation over time, as well as unintended behavior, since the data exposed to the model will be different from what it has been trained on. This isn't something we should be trying to avoid but rather understand and mitigate as much as possible. In this lesson, we'll understand the short comings from attempting to capture performance degradation in order to motivate the need for drift detection.</p> <p>Tip</p> <p>We highly recommend that you explore this lesson after completing the previous lessons since the topics (and code) are iteratively developed. We did, however, create the  monitoring-ml repository for a quick overview with an interactive notebook.</p>"},{"location":"courses/mlops/monitoring/#system-health","title":"System health","text":"<p>The first step to insure that our model is performing well is to ensure that the actual system is up and running as it should. This can include metrics specific to service requests such as latency, throughput, error rates, etc. as well as infrastructure utilization such as CPU/GPU utilization, memory, etc.</p> <p>Fortunately, most cloud providers and even orchestration layers will provide this insight into our system's health for free through a dashboard. In the event we don't, we can easily use Grafana, Datadog, etc. to ingest system performance metrics from logs to create a customized dashboard and set alerts.</p>"},{"location":"courses/mlops/monitoring/#performance","title":"Performance","text":"<p>Unfortunately, just monitoring the system's health won't be enough to capture the underlying issues with our model. So, naturally, the next layer of metrics to monitor involves the model's performance. These could be quantitative evaluation metrics that we used during model evaluation (accuracy, precision, f1, etc.) but also key business metrics that the model influences (ROI, click rate, etc.).</p> <p>It's usually never enough to just analyze the cumulative performance metrics across the entire span of time since the model has been deployed. Instead, we should also inspect performance across a period of time that's significant for our application (ex. daily). These sliding metrics might be more indicative of our system's health and we might be able to identify issues faster by not obscuring them with historical data.</p> <p>\ud83d\udc49 \u00a0 Follow along interactive notebook in the  monitoring-ml repository as we implement the concepts below.</p> <p><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set_theme()\n</code></pre> <pre><code># Generate data\nhourly_f1 = list(np.random.randint(low=94, high=98, size=24*20)) + \\\n            list(np.random.randint(low=92, high=96, size=24*5)) + \\\n            list(np.random.randint(low=88, high=96, size=24*5)) + \\\n            list(np.random.randint(low=86, high=92, size=24*5))\n</code></pre> <pre><code># Cumulative f1\ncumulative_f1 = [np.mean(hourly_f1[:n]) for n in range(1, len(hourly_f1)+1)]\nprint (f\"Average cumulative f1 on the last day: {np.mean(cumulative_f1[-24:]):.1f}\")\n</code></pre></p> <pre>\nAverage cumulative f1 on the last day: 93.7\n</pre> <pre><code># Sliding f1\nwindow_size = 24\nsliding_f1 = np.convolve(hourly_f1, np.ones(window_size)/window_size, mode=\"valid\")\nprint (f\"Average sliding f1 on the last day: {np.mean(sliding_f1[-24:]):.1f}\")\n</code></pre> <pre>\nAverage sliding f1 on the last day: 88.6\n</pre> <pre><code>plt.ylim([80, 100])\nplt.hlines(y=90, xmin=0, xmax=len(hourly_f1), colors=\"blue\", linestyles=\"dashed\", label=\"threshold\")\nplt.plot(cumulative_f1, label=\"cumulative\")\nplt.plot(sliding_f1, label=\"sliding\")\nplt.legend()\n</code></pre> <p>We may need to monitor metrics at various window sizes to catch performance degradation as soon as possible. Here we're monitoring the overall f1 but we can do the same for slices of data, individual classes, etc. For example, if we monitor the performance on a specific tag, we may be able to quickly catch new algorithms that were released for that tag (ex. new transformer architecture).</p>"},{"location":"courses/mlops/monitoring/#delayed-outcomes","title":"Delayed outcomes","text":"<p>We may not always have the ground-truth outcomes available to determine the model's performance on production inputs. This is especially true if there is significant lag or annotation is required on the real-world data. To mitigate this, we could:</p> <ul> <li>devise an approximate signal that can help us estimate the model's performance. For example, in our tag prediction task, we could use the actual tags that an author attributes to a project as the intermediary labels until we have verified labels from an annotation pipeline.</li> <li>label a small subset of our live dataset to estimate performance. This subset should try to be representative of the various distributions in the live data.</li> </ul>"},{"location":"courses/mlops/monitoring/#importance-weighting","title":"Importance weighting","text":"<p>However, approximate signals are not always available for every situation because there is no feedback on the ML system\u2019s outputs or it\u2019s too delayed. For these situations, a recent line of research relies on the only component that\u2019s available in all situations: the input data.</p> Mandoline: Model Evaluation under Distribution Shift <p>The core idea is to develop slicing functions that may potentially capture the ways our data may experience distribution shift. These slicing functions should capture obvious slices such as class labels or different categorical feature values but also slices based on implicit metadata (hidden aspects of the data that are not explicit feature columns). These slicing functions are then applied to our labeled dataset to create matrices with the corresponding labels. The same slicing functions are applied to our unlabeled production data to approximate what the weighted labels would be. With this, we can determine the approximate performance! The intuition here is that we can better approximate performance on our unlabeled dataset based on the similarity between the labeled slice matrix and unlabeled slice matrix. A core dependency of this assumption is that our slicing functions are comprehensive enough that they capture the causes of distributional shift.</p> <p>Warning</p> <p>If we wait to catch the model decay based on the performance, it may have already caused significant damage to downstream business pipelines that are dependent on it. We need to employ more fine-grained monitoring to identify the sources of model drift prior to actual performance degradation.</p>"},{"location":"courses/mlops/monitoring/#drift","title":"Drift","text":"<p>We need to first understand the different types of issues that can cause our model's performance to decay (model drift). The best way to do this is to look at all the moving pieces of what we're trying to model and how each one can experience drift.</p> <p> Entity Description Drift \\(X\\) inputs (features) data drift     \\(\\rightarrow P(X) \\neq P_{ref}(X)\\) \\(y\\) outputs (ground-truth) target drift   \\(\\rightarrow P(y) \\neq P_{ref}(y)\\) \\(P(y \\vert X)\\) actual relationship between \\(X\\) and \\(y\\) concept drift  \\(\\rightarrow P(y \\vert X) \\neq P_{ref}(y \\vert X)\\) <p></p>"},{"location":"courses/mlops/monitoring/#data-drift","title":"Data drift","text":"<p>Data drift, also known as feature drift or covariate shift, occurs when the distribution of the production data is different from the training data. The model is not equipped to deal with this drift in the feature space and so, it's predictions may not be reliable. The actual cause of drift can be attributed to natural changes in the real-world but also to systemic issues such as missing data, pipeline errors, schema changes, etc. It's important to inspect the drifted data and trace it back along it's pipeline to identify when and where the drift was introduced.</p> <p>Warning</p> <p>Besides just looking at the distribution of our input data, we also want to ensure that the workflows to retrieve and process our input data is the same during training and serving to avoid training-serving skew. However, we can skip this step if we retrieve our features from the same source location for both training and serving, ie. from a feature store.</p> Data drift can occur in either continuous or categorical features. <p>As data starts to drift, we may not yet notice significant decay in our model's performance, especially if the model is able to interpolate well. However, this is a great opportunity to potentially retrain before the drift starts to impact performance.</p>"},{"location":"courses/mlops/monitoring/#target-drift","title":"Target drift","text":"<p>Besides just the input data changing, as with data drift, we can also experience drift in our outcomes. This can be a shift in the distributions but also the removal or addition of new classes with categorical tasks. Though retraining can mitigate the performance decay caused target drift, it can often be avoided with proper inter-pipeline communication about new classes, schema changes, etc.</p>"},{"location":"courses/mlops/monitoring/#concept-drift","title":"Concept drift","text":"<p>Besides the input and output data drifting, we can have the actual relationship between them drift as well. This concept drift renders our model ineffective because the patterns it learned to map between the original inputs and outputs are no longer relevant. Concept drift can be something that occurs in various patterns:</p> <ul> <li>gradually over a period of time</li> <li>abruptly as a result of an external event</li> <li>periodically as a result of recurring events</li> </ul> <p>All the different types of drift we discussed can can occur simultaneously which can complicated identifying the sources of drift.</p>"},{"location":"courses/mlops/monitoring/#locating-drift","title":"Locating drift","text":"<p>Now that we've identified the different types of drift, we need to learn how to locate and how often to measure it. Here are the constraints we need to consider:</p> <ul> <li>reference window: the set of points to compare production data distributions with to identify drift.</li> <li>test window: the set of points to compare with the reference window to determine if drift has occurred.</li> </ul> <p>Since we're dealing with online drift detection (ie. detecting drift in live production data as opposed to past batch data), we can employ either a fixed or sliding window approach to identify our set of points for comparison. Typically, the reference window is a fixed, recent subset of the training data while the test window slides over time.</p> <p>Scikit-multiflow provides a toolkit for concept drift detection techniques directly on streaming data. The package offers windowed, moving average functionality (including dynamic preprocessing) and even methods around concepts like gradual concept drift.</p> <p>We can also compare across various window sizes simultaneously to ensure smaller cases of drift aren't averaged out by large window sizes.</p>"},{"location":"courses/mlops/monitoring/#measuring-drift","title":"Measuring drift","text":"<p>Once we have the window of points we wish to compare, we need to know how to compare them.</p> <p><pre><code>import great_expectations as ge\nimport json\nimport pandas as pd\nfrom urllib.request import urlopen\n</code></pre> <pre><code># Load labeled projects\nprojects = pd.read_csv(\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\")\ntags = pd.read_csv(\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\")\ndf = ge.dataset.PandasDataset(pd.merge(projects, tags, on=\"id\"))\ndf[\"text\"] = df.title + \" \" + df.description\ndf.drop([\"title\", \"description\"], axis=1, inplace=True)\ndf.head(5)\n</code></pre></p> <pre>\nid\n      created_on\n      tag\n      text\n    0\n      6\n      2020-02-20 06:43:18\n      computer-vision\n      Comparison between YOLO and RCNN on real world...\n    1\n      7\n      2020-02-20 06:47:21\n      computer-vision\n      Show, Infer &amp; Tell: Contextual Inference for C...\n    2\n      9\n      2020-02-24 16:24:45\n      graph-learning\n      Awesome Graph Classification A collection of i...\n    3\n      15\n      2020-02-28 23:55:26\n      reinforcement-learning\n      Awesome Monte Carlo Tree Search A curated list...\n    4\n      19\n      2020-03-03 13:54:31\n      graph-learning\n      Diffusion to Vector Reference implementation o...\n    </pre>"},{"location":"courses/mlops/monitoring/#expectations","title":"Expectations","text":"<p>The first form of measurement can be rule-based such as validating expectations around missing values, data types, value ranges, etc. as we did in our data testing lesson. The difference now is that we'll be validating these expectations on live production data.</p> <p><pre><code># Simulated production data\nprod_df = ge.dataset.PandasDataset([{\"text\": \"hello\"}, {\"text\": 0}, {\"text\": \"world\"}])\n</code></pre> <pre><code># Expectation suite\ndf.expect_column_values_to_not_be_null(column=\"text\")\ndf.expect_column_values_to_be_of_type(column=\"text\", type_=\"str\")\nexpectation_suite = df.get_expectation_suite()\n</code></pre> <pre><code># Validate reference data\ndf.validate(expectation_suite=expectation_suite, only_return_failures=True)[\"statistics\"]\n</code></pre></p> <pre>\n{'evaluated_expectations': 2,\n 'success_percent': 100.0,\n 'successful_expectations': 2,\n 'unsuccessful_expectations': 0}\n</pre> <pre><code># Validate production data\nprod_df.validate(expectation_suite=expectation_suite, only_return_failures=True)[\"statistics\"]\n</code></pre> <pre>\n{'evaluated_expectations': 2,\n 'success_percent': 50.0,\n 'successful_expectations': 1,\n 'unsuccessful_expectations': 1}\n</pre> <p>Once we've validated our rule-based expectations, we need to quantitatively measure drift across the different features in our data.</p>"},{"location":"courses/mlops/monitoring/#univariate","title":"Univariate","text":"<p>Our task may involve univariate (1D) features that we will want to monitor. While there are many types of hypothesis tests we can use, a popular option is the Kolmogorov-Smirnov (KS) test.</p>"},{"location":"courses/mlops/monitoring/#kolmogorov-smirnov-ks-test","title":"Kolmogorov-Smirnov (KS) test","text":"<p>The KS test determines the maximum distance between two distribution's cumulative density functions. Here, we'll measure if there is any drift on the size of our input text feature between two different data subsets.</p> <p>Tip</p> <p>While text is a direct feature in our task, we can also monitor other implicit features such as % of unknown tokens in text (need to maintain a training vocabulary), etc. While they may not be used for our machine learning model, they can be great indicators for detecting drift.</p> <pre><code>from alibi_detect.cd import KSDrift\n</code></pre> <pre><code># Reference\ndf[\"num_tokens\"] = df.text.apply(lambda x: len(x.split(\" \")))\nref = df[\"num_tokens\"][0:200].to_numpy()\nplt.hist(ref, alpha=0.75, label=\"reference\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Initialize drift detector\nlength_drift_detector = KSDrift(ref, p_val=0.01)\n</code></pre> <pre><code># No drift\nno_drift = df[\"num_tokens\"][200:400].to_numpy()\nplt.hist(ref, alpha=0.75, label=\"reference\")\nplt.hist(no_drift, alpha=0.5, label=\"test\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>length_drift_detector.predict(no_drift, return_p_val=True, return_distance=True)\n</code></pre> <pre>\n{'data': {'distance': array([0.09], dtype=float32),\n  'is_drift': 0,\n  'p_val': array([0.3927307], dtype=float32),\n  'threshold': 0.01},\n 'meta': {'data_type': None,\n  'detector_type': 'offline',\n  'name': 'KSDrift',\n  'version': '0.9.1'}}\n</pre> <p>\u2193 p-value = \u2191 confident that the distributions are different.</p> <pre><code># Drift\ndrift = np.random.normal(30, 5, len(ref))\nplt.hist(ref, alpha=0.75, label=\"reference\")\nplt.hist(drift, alpha=0.5, label=\"test\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>length_drift_detector.predict(drift, return_p_val=True, return_distance=True)\n</code></pre> <pre>\n{'data': {'distance': array([0.63], dtype=float32),\n  'is_drift': 1,\n  'p_val': array([6.7101775e-35], dtype=float32),\n  'threshold': 0.01},\n 'meta': {'data_type': None,\n  'detector_type': 'offline',\n  'name': 'KSDrift',\n  'version': '0.9.1'}}\n</pre>"},{"location":"courses/mlops/monitoring/#chi-squared-test","title":"Chi-squared test","text":"<p>Similarly, for categorical data (input features, targets, etc.), we can apply the Pearson's chi-squared test to determine if a frequency of events in production is consistent with a reference distribution.</p> <p>We're creating a categorical variable for the # of tokens in our text feature but we could very very apply it to the tag distribution itself, individual tags (binary), slices of tags, etc.</p> <pre><code>from alibi_detect.cd import ChiSquareDrift\n</code></pre> <pre><code># Reference\ndf.token_count = df.num_tokens.apply(lambda x: \"small\" if x &lt;= 10 else (\"medium\" if x &lt;=25 else \"large\"))\nref = df.token_count[0:200].to_numpy()\nplt.hist(ref, alpha=0.75, label=\"reference\")\nplt.legend()\n</code></pre> <pre><code># Initialize drift detector\ntarget_drift_detector = ChiSquareDrift(ref, p_val=0.01)\n</code></pre> <pre><code># No drift\nno_drift = df.token_count[200:400].to_numpy()\nplt.hist(ref, alpha=0.75, label=\"reference\")\nplt.hist(no_drift, alpha=0.5, label=\"test\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>target_drift_detector.predict(no_drift, return_p_val=True, return_distance=True)\n</code></pre> <pre>\n{'data': {'distance': array([4.135522], dtype=float32),\n  'is_drift': 0,\n  'p_val': array([0.12646863], dtype=float32),\n  'threshold': 0.01},\n 'meta': {'data_type': None,\n  'detector_type': 'offline',\n  'name': 'ChiSquareDrift',\n  'version': '0.9.1'}}\n</pre> <pre><code># Drift\ndrift = np.array([\"small\"]*80 + [\"medium\"]*40 + [\"large\"]*80)\nplt.hist(ref, alpha=0.75, label=\"reference\")\nplt.hist(drift, alpha=0.5, label=\"test\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>target_drift_detector.predict(drift, return_p_val=True, return_distance=True)\n</code></pre> <pre>\n{'data': {'is_drift': 1,\n  'distance': array([118.03355], dtype=float32),\n  'p_val': array([2.3406739e-26], dtype=float32),\n  'threshold': 0.01},\n 'meta': {'name': 'ChiSquareDrift',\n  'detector_type': 'offline',\n  'data_type': None}}\n</pre>"},{"location":"courses/mlops/monitoring/#multivariate","title":"Multivariate","text":"<p>As we can see, measuring drift is fairly straightforward for univariate data but difficult for multivariate data. We'll summarize the reduce and measure approach outlined in the following paper: Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift.</p> <p>We vectorized our text using tf-idf (to keep modeling simple), which has high dimensionality and is not semantically rich in context. However, typically with text, word/char embeddings are used. So to illustrate what drift detection on multivariate data would look like, let's represent our text using pretrained embeddings.</p> <p>Be sure to refer to our embeddings and transformers lessons to learn more about these topics. But note that detecting drift on multivariate text embeddings is still quite difficult so it's typically more common to use these methods applied to tabular features or images.</p> <p>We'll start by loading the tokenizer from a pretrained model.</p> <pre><code>from transformers import AutoTokenizer\n</code></pre> <pre><code>model_name = \"allenai/scibert_scivocab_uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nvocab_size = len(tokenizer)\nprint (vocab_size)\n</code></pre> <pre>\n31090\n</pre> <pre><code># Tokenize inputs\nencoded_input = tokenizer(df.text.tolist(), return_tensors=\"pt\", padding=True)\nids = encoded_input[\"input_ids\"]\nmasks = encoded_input[\"attention_mask\"]\n</code></pre> <pre><code># Decode\nprint (f\"{ids[0]}\\n{tokenizer.decode(ids[0])}\")\n</code></pre> <pre>\ntensor([  102,  2029,   467,  1778,   609,   137,  6446,  4857,   191,  1332,\n         2399, 13572, 19125,  1983,   147,  1954,   165,  6240,   205,   185,\n          300,  3717,  7434,  1262,   121,   537,   201,   137,  1040,   111,\n          545,   121,  4714,   205,   103,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0])\n[CLS] comparison between yolo and rcnn on real world videos bringing theory to experiment is cool. we can easily train models in colab and find the results in minutes. [SEP] [PAD] [PAD] ...\n</pre> <pre><code># Sub-word tokens\nprint (tokenizer.convert_ids_to_tokens(ids=ids[0]))\n</code></pre> <pre>\n['[CLS]', 'comparison', 'between', 'yo', '##lo', 'and', 'rc', '##nn', 'on', 'real', 'world', 'videos', 'bringing', 'theory', 'to', 'experiment', 'is', 'cool', '.', 'we', 'can', 'easily', 'train', 'models', 'in', 'col', '##ab', 'and', 'find', 'the', 'results', 'in', 'minutes', '.', '[SEP]', '[PAD]', '[PAD]', ...]\n</pre> <p>Next, we'll load the pretrained model's weights and use the <code>TransformerEmbedding</code> object to extract the embeddings from the hidden state (averaged across tokens).</p> <pre><code>from alibi_detect.models.pytorch import TransformerEmbedding\n</code></pre> <pre><code># Embedding layer\nemb_type = \"hidden_state\"\nlayers = [-x for x in range(1, 9)]  # last 8 layers\nembedding_layer = TransformerEmbedding(model_name, emb_type, layers)\n</code></pre> <pre><code># Embedding dimension\nembedding_dim = embedding_layer.model.embeddings.word_embeddings.embedding_dim\nembedding_dim\n</code></pre> <pre>\n768\n</pre>"},{"location":"courses/mlops/monitoring/#dimensionality-reduction","title":"Dimensionality reduction","text":"<p>Now we need to use a dimensionality reduction method to reduce our representations dimensions into something more manageable (ex. 32 dim) so we can run our two-sample tests on to detect drift. Popular options include:</p> <ul> <li>Principle component analysis (PCA): orthogonal transformations that preserve the variability of the dataset.</li> <li>Autoencoders (AE): networks that consume the inputs and attempt to reconstruct it from an lower dimensional space while minimizing the error. These can either be trained or untrained (the Failing loudly paper recommends untrained).</li> <li>Black box shift detectors (BBSD): the actual model trained on the training data can be used as a dimensionality reducer. We can either use the softmax outputs (multivariate) or the actual predictions (univariate).</li> </ul> <pre><code>import torch\nimport torch.nn as nn\n</code></pre> <pre><code># Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n</code></pre> <pre>\ncuda\n</pre> <pre><code># Untrained autoencoder (UAE) reducer\nencoder_dim = 32\nreducer = nn.Sequential(\n    embedding_layer,\n    nn.Linear(embedding_dim, 256),\n    nn.ReLU(),\n    nn.Linear(256, encoder_dim)\n).to(device).eval()\n</code></pre> <p>We can wrap all of the operations above into one preprocessing function that will consume input text and produce the reduced representation.</p> <pre><code>from alibi_detect.cd.pytorch import preprocess_drift\nfrom functools import partial\n</code></pre> <pre><code># Preprocessing with the reducer\nmax_len = 100\nbatch_size = 32\npreprocess_fn = partial(preprocess_drift, model=reducer, tokenizer=tokenizer,\n                        max_len=max_len, batch_size=batch_size, device=device)\n</code></pre>"},{"location":"courses/mlops/monitoring/#maximum-mean-discrepancy-mmd","title":"Maximum Mean Discrepancy (MMD)","text":"<p>After applying dimensionality reduction techniques on our multivariate data, we can use different statistical tests to calculate drift. A popular option is Maximum Mean Discrepancy (MMD), a kernel-based approach that determines the distance between two distributions by computing the distance between the mean embeddings of the features from both distributions.</p> <pre><code>from alibi_detect.cd import MMDDrift\n</code></pre> <pre><code># Initialize drift detector\nmmd_drift_detector = MMDDrift(ref, backend=\"pytorch\", p_val=.01, preprocess_fn=preprocess_fn)\n</code></pre> <pre><code># No drift\nno_drift = df.text[200:400].to_list()\nmmd_drift_detector.predict(no_drift)\n</code></pre> <pre>\n{'data': {'distance': 0.0021169185638427734,\n  'distance_threshold': 0.0032651424,\n  'is_drift': 0,\n  'p_val': 0.05999999865889549,\n  'threshold': 0.01},\n 'meta': {'backend': 'pytorch',\n  'data_type': None,\n  'detector_type': 'offline',\n  'name': 'MMDDriftTorch',\n  'version': '0.9.1'}}\n</pre> <pre><code># Drift\ndrift = [\"UNK \" + text for text in no_drift]\nmmd_drift_detector.predict(drift)\n</code></pre> <pre>\n{'data': {'distance': 0.014705955982208252,\n  'distance_threshold': 0.003908038,\n  'is_drift': 1,\n  'p_val': 0.0,\n  'threshold': 0.01},\n 'meta': {'backend': 'pytorch',\n  'data_type': None,\n  'detector_type': 'offline',\n  'name': 'MMDDriftTorch',\n  'version': '0.9.1'}}\n</pre>"},{"location":"courses/mlops/monitoring/#online","title":"Online","text":"<p>So far we've applied our drift detection methods on offline data to try and understand what reference window sizes should be, what p-values are appropriate, etc. However, we'll need to apply these methods in the online production setting so that we can catch drift as easy as possible.</p> <p>Many monitoring libraries and platforms come with online equivalents for their detection methods.</p> <p>Typically, reference windows are large so that we have a proper benchmark to compare our production data points to. As for the test window, the smaller it is, the more quickly we can catch sudden drift. Whereas, a larger test window will allow us to identify more subtle/gradual drift. So it's best to compose windows of different sizes to regularly monitor.</p> <pre><code>from alibi_detect.cd import MMDDriftOnline\n</code></pre> <pre><code># Online MMD drift detector\nref = df.text[0:800].to_list()\nonline_mmd_drift_detector = MMDDriftOnline(\n    ref, ert=400, window_size=200, backend=\"pytorch\", preprocess_fn=preprocess_fn)\n</code></pre> <pre>\nGenerating permutations of kernel matrix..\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 13784.22it/s]\nComputing thresholds: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:32&lt;00:00,  6.11it/s]\n</pre> <p>As data starts to flow in, we can use the detector to predict drift at every point. Our detector should detect drift sooner in our drifter dataset than in our normal data.</p> <pre><code>def simulate_production(test_window):\n    i = 0\n    online_mmd_drift_detector.reset()\n    for text in test_window:\n        result = online_mmd_drift_detector.predict(text)\n        is_drift = result[\"data\"][\"is_drift\"]\n        if is_drift:\n            break\n        else:\n            i += 1\n    print (f\"{i} steps\")\n</code></pre> <pre><code># Normal\ntest_window = df.text[800:]\nsimulate_production(test_window)\n</code></pre> <pre>\n27 steps\n</pre> <pre><code># Drift\ntest_window = \"UNK\" * len(df.text[800:])\nsimulate_production(test_window)\n</code></pre> <pre>\n11 steps\n</pre> <p>There are also several considerations around how often to refresh both the reference and test windows. We could base in on the number of new observations or time without drift, etc. We can also adjust the various thresholds (ERT, window size, etc.) based on what we learn about our system through monitoring.</p>"},{"location":"courses/mlops/monitoring/#outliers","title":"Outliers","text":"<p>With drift, we're comparing a window of production data with reference data as opposed to looking at any one specific data point. While each individual point may not be an anomaly or outlier, the group of points may cause a drift. The easiest way to illustrate this is to imagine feeding our live model the same input data point repeatedly. The actual data point may not have anomalous features but feeding it repeatedly will cause the feature distribution to change and lead to drift.</p> <p>Unfortunately, it's not very easy to detect outliers because it's hard to constitute the criteria for an outlier. Therefore the outlier detection task is typically unsupervised and requires a stochastic streaming algorithm to identify potential outliers. Luckily, there are several powerful libraries such as PyOD, Alibi Detect, WhyLogs (uses Apache DataSketches), etc. that offer a suite of outlier detection functionality (largely for tabular and image data for now).</p> <p>Typically, outlier detection algorithms fit (ex. via reconstruction) to the training set to understand what normal data looks like and then we can use a threshold to predict outliers. If we have a small labeled dataset with outliers, we can empirically choose our threshold but if not, we can choose some reasonable tolerance.</p> <pre><code>from alibi_detect.od import OutlierVAE\nX_train = (n_samples, n_features)\noutlier_detector = OutlierVAE(\n    threshold=0.05,\n    encoder_net=encoder,\n    decoder_net=decoder,\n    latent_dim=512\n)\noutlier_detector.fit(X_train, epochs=50)\noutlier_detector.infer_threshold(X, threshold_perc=95)  # infer from % outliers\npreds = outlier_detector.predict(X, outlier_type=\"instance\", outlier_perc=75)\n</code></pre> <p>When we identify outliers, we may want to let the end user know that the model's response may not be reliable. Additionally, we may want to remove the outliers from the next training set or further inspect them and upsample them in case they're early signs of what future distributions of incoming features will look like.</p>"},{"location":"courses/mlops/monitoring/#solutions","title":"Solutions","text":"<p>It's not enough to just be able to measure drift or identify outliers but to also be able to act on it. We want to be able to alert on drift, inspect it and then act on it.</p>"},{"location":"courses/mlops/monitoring/#alert","title":"Alert","text":"<p>Once we've identified outliers and/or measured statistically significant drift, we need to a devise a workflow to notify stakeholders of the issues. A negative connotation with monitoring is fatigue stemming from false positive alerts. This can be mitigated by choosing the appropriate constraints (ex. alerting thresholds) based on what's important to our specific application. For example, thresholds could be:</p> <ul> <li>fixed values/range for situations where we're concretely aware of expected upper/lower bounds. <pre><code>if percentage_unk_tokens &gt; 5%:\n    trigger_alert()\n</code></pre></li> <li>forecasted thresholds dependent on previous inputs, time, etc. <pre><code>if current_f1 &lt; forecast_f1(current_time):\n    trigger_alert()\n</code></pre></li> <li>appropriate p-values for different drift detectors (\u2193 p-value = \u2191 confident that the distributions are different). <pre><code>from alibi_detect.cd import KSDrift\nlength_drift_detector = KSDrift(reference, p_val=0.01)\n</code></pre></li> </ul> <p>Once we have our carefully crafted alerting workflows in place, we can notify stakeholders as issues arise via email, Slack, PageDuty, etc. The stakeholders can be of various levels (core engineers, managers, etc.) and they can subscribe to the alerts that are relevant for them.</p>"},{"location":"courses/mlops/monitoring/#inspect","title":"Inspect","text":"<p>Once we receive an alert, we need to inspect it before acting on it. An alert needs several components in order for us to completely inspect it:</p> <ul> <li>specific alert that was triggered</li> <li>relevant metadata (time, inputs, outputs, etc.)</li> <li>thresholds / expectations that failed</li> <li>drift detection tests that were conducted</li> <li>data from reference and test windows</li> <li>log records from the relevant window of time</li> </ul> <pre><code># Sample alerting ticket\n{\n\"triggered_alerts\": [\"text_length_drift\"],\n    \"threshold\": 0.05,\n    \"measurement\": \"KSDrift\",\n    \"distance\": 0.86,\n    \"p_val\": 0.03,\n    \"reference\": [],\n    \"target\": [],\n    \"logs\": ...\n}\n</code></pre> <p>With these pieces of information, we can work backwards from the alert towards identifying the root cause of the issue. Root cause analysis (RCA) is an important first step when it comes to monitoring because we want to prevent the same issue from impacting our system again. Often times, many alerts are triggered but they maybe all actually be caused by the same underlying issue. In this case, we'd want to intelligently trigger just one alert that pinpoints the core issue. For example, let's say we receive an alert that our overall user satisfaction ratings are reducing but we also receive another alert that our North American users also have low satisfaction ratings. Here's the system would automatically assess for drift in user satisfaction ratings across many different slices and aggregations to discover that only users in a specific area are experiencing the issue but because it's a popular user base, it ends up triggering all aggregate downstream alerts as well!</p>"},{"location":"courses/mlops/monitoring/#act","title":"Act","text":"<p>There are many different ways we can act to drift based on the situation. An initial impulse may be to retrain our model on the new data but it may not always solve the underlying issue.</p> <ul> <li>ensure all data expectations have passed.</li> <li>confirm no data schema changes.</li> <li>retrain the model on the new shifted dataset.</li> <li>move the reference window to more recent data or give it more weight.</li> <li>determine if outliers are potentially valid data points.</li> </ul>"},{"location":"courses/mlops/monitoring/#production","title":"Production","text":"<p>Since detecting drift and outliers can involve compute intensive operations, we need a solution that can execute serverless workloads on top of our event data streams (ex. Kafka). Typically these solutions will ingest payloads (ex. model's inputs and outputs) and can trigger monitoring workloads. This allows us to segregate the resources for monitoring from our actual ML application and scale them as needed.</p> <p>When it actually comes to implementing a monitoring system, we have several options, ranging from fully managed to from-scratch. Several popular managed solutions are Arize, Arthur, Fiddler, Gantry, Mona, WhyLabs, etc., all of which allow us to create custom monitoring views, trigger alerts, etc. There are even several great open-source solutions such as EvidentlyAI, TorchDrift, WhyLogs, etc.</p> <p>We'll often notice that monitoring solutions are offered as part of the larger deployment option such as Sagemaker, TensorFlow Extended (TFX), TorchServe, etc. And if we're already working with Kubernetes, we could use KNative or Kubeless for serverless workload management. But we could also use a higher level framework such as KFServing or Seldon core that natively use a serverless framework like KNative.</p>"},{"location":"courses/mlops/monitoring/#references","title":"References","text":"<ul> <li>An overview of unsupervised drift detection methods</li> <li>Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift</li> <li>Monitoring and explainability of models in production</li> <li>Detecting and Correcting for Label Shift with Black Box Predictors</li> <li>Outlier and anomaly pattern detection on data streams</li> </ul> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Monitoring - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/orchestration/","title":"Orchestration for Machine Learning","text":""},{"location":"courses/mlops/orchestration/#intuition","title":"Intuition","text":"<p>So far we've implemented our DataOps (ELT, validation, etc.) and MLOps (optimization, training, evaluation, etc.) workflows as Python function calls. This has worked well since our dataset is static and small. But happens when we need to:</p> <ul> <li>schedule these workflows as new data arrives?</li> <li>scale these workflows as our data grows?</li> <li>share these workflows to downstream applications?</li> <li>monitor these workflows?</li> </ul> <p>We'll need to break down our end-to-end ML pipeline into individual workflows that can be orchestrated as needed. There are several tools that can help us so this such as Airflow, Prefect, Dagster, Luigi, Orchest and even some ML focused options such as Metaflow, Flyte, KubeFlow Pipelines, Vertex pipelines, etc. We'll be creating our workflows using AirFlow for its:</p> <ul> <li>wide adoption of the open source platform in industry</li> <li>Python based software development kit (SDK)</li> <li>ability to run locally and scale easily</li> <li>maturity over the years and part of the apache ecosystem</li> </ul> <p>We'll be running Airflow locally but we can easily scale it by running on a managed cluster platform where we can run Python, Hadoop, Spark, etc. on large batch processing jobs (AWS EMR, Google Cloud's Dataproc, on-prem hardware, etc.).</p>"},{"location":"courses/mlops/orchestration/#airflow","title":"Airflow","text":"<p>Before we create our specific pipelines, let's understand and implement Airflow's overarching concepts that will allow us to \"author, schedule, and monitor workflows\".</p> <p>Separate repository</p> <p>Our work in this lesson will live in a separate repository so create a new directory (outside our <code>mlops-course</code> repository) called <code>data-engineering</code>. All the work in this lesson can be found in our  data-engineering repository.</p>"},{"location":"courses/mlops/orchestration/#set-up","title":"Set up","text":"<p>To install and run Airflow, we can either do so locally or with Docker. If using <code>docker-compose</code> to run Airflow inside Docker containers, we'll want to allocate at least 4 GB in memory.</p> <pre><code># Configurations\nexport AIRFLOW_HOME=${PWD}/airflow\nAIRFLOW_VERSION=2.3.3\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\n# Install Airflow (may need to upgrade pip)\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n\n# Initialize DB (SQLite by default)\nairflow db init\n</code></pre> <p>This will create an <code>airflow</code> directory with the following components:</p> <pre><code>airflow/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 airflow.cfg\n\u251c\u2500\u2500 airflow.db\n\u251c\u2500\u2500 unittests.cfg\n\u2514\u2500\u2500 webserver_config.py\n</code></pre> <p>We're going to edit the airflow.cfg file to best fit our needs: <pre><code># Inside airflow.cfg\nenable_xcom_pickling = True  # needed for Great Expectations airflow provider\nload_examples = False  # don't clutter webserver with examples\n</code></pre></p> <p>And we'll perform a reset to implement these configuration changes.</p> <pre><code>airflow db reset -y\n</code></pre> <p>Now we're ready to initialize our database with an admin user, which we'll use to login to access our workflows in the webserver.</p> <pre><code># We'll be prompted to enter a password\nairflow users create \\\n--username admin \\\n--firstname FIRSTNAME \\\n--lastname LASTNAME \\\n--role Admin \\\n--email EMAIL\n</code></pre>"},{"location":"courses/mlops/orchestration/#webserver","title":"Webserver","text":"<p>Once we've created a user, we're ready to launch the webserver and log in using our credentials.</p> <pre><code># Launch webserver\nsource venv/bin/activate\nexport AIRFLOW_HOME=${PWD}/airflow\nairflow webserver --port 8080  # http://localhost:8080\n</code></pre> <p>The webserver allows us to run and inspect workflows, establish connections to external data storage, manager users, etc. through a UI. Similarly, we could also use Airflow's REST API or Command-line interface (CLI) to perform the same operations. However, we'll be using the webserver because it's convenient to visually inspect our workflows.</p> <p>We'll explore the different components of the webserver as we learn about Airflow and implement our workflows.</p>"},{"location":"courses/mlops/orchestration/#scheduler","title":"Scheduler","text":"<p>Next, we need to launch our scheduler, which will execute and monitor the tasks in our workflows. The schedule executes tasks by reading from the metadata database and ensures the task has what it needs to finish running. We'll go ahead and execute the following commands on the separate terminal window:</p> <pre><code># Launch scheduler (in separate terminal)\nsource venv/bin/activate\nexport AIRFLOW_HOME=${PWD}/airflow\nexport OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\nairflow scheduler\n</code></pre>"},{"location":"courses/mlops/orchestration/#executor","title":"Executor","text":"<p>As our scheduler reads from the metadata database, the executor determines what worker processes are necessary for the task to run to completion. Since our default database SQLlite, which can't support multiple connections, our default executor is the Sequential Executor. However, if we choose a more production-grade database option such as PostgresSQL or MySQL, we can choose scalable Executor backends Celery, Kubernetes, etc. For example, running Airflow with Docker uses PostgresSQL as the database and so uses the Celery Executor backend to run tasks in parallel.</p>"},{"location":"courses/mlops/orchestration/#dags","title":"DAGs","text":"<p>Workflows are defined by directed acyclic graphs (DAGs), whose nodes represent tasks and edges represent the data flow relationship between the tasks. Direct and acyclic implies that workflows can only execute in one direction and a previous, upstream task cannot run again once a downstream task has started.</p> <p>DAGs can be defined inside Python workflow scripts inside the <code>airflow/dags</code> directory and they'll automatically appear (and continuously be updated) on the webserver. Before we start creating our DataOps and MLOps workflows, we'll learn about Airflow's concepts via an example DAG outlined in airflow/dags/example.py. Execute the following commands in a new (3rd) terminal window:</p> <pre><code>mkdir airflow/dags\ntouch airflow/dags/example.py\n</code></pre> <p>Inside each workflow script, we can define some default arguments that will apply to all DAGs within that workflow.</p> <pre><code># Default DAG args\ndefault_args = {\n    \"owner\": \"airflow\",\n}\n</code></pre> <p>Typically, our DAGs are not the only ones running in an Airflow cluster. However, it can be messy and sometimes impossible to execute different workflows when they require different resources, package versions, etc. For teams with multiple projects, it\u2019s a good idea to use something like the KubernetesPodOperator to execute each job using an isolated docker image.</p> <p>We can initialize DAGs with many parameters (which will override the same parameters in <code>default_args</code>) and in several different ways:</p> <ul> <li> <p>using a with statement <pre><code>from airflow import DAG\n\nwith DAG(\n    dag_id=\"example\",\n    description=\"Example DAG\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"example\"],\n) as example:\n    # Define tasks\n    pass\n</code></pre></p> </li> <li> <p>using the dag decorator <pre><code>from airflow.decorators import dag\n\n@dag(\n    dag_id=\"example\",\n    description=\"Example DAG\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"example\"],\n)\ndef example():\n    # Define tasks\n    pass\n</code></pre></p> </li> </ul> <p>There are many parameters that we can initialize our DAGs with, including a <code>start_date</code> and a <code>schedule_interval</code>. While we could have our workflows execute on a temporal cadence, many ML workflows are initiated by events, which we can map using sensors and hooks to external databases, file systems, etc.</p>"},{"location":"courses/mlops/orchestration/#tasks","title":"Tasks","text":"<p>Tasks are the operations that are executed in a workflow and are represented by nodes in a DAG. Each task should be a clearly defined single operation and it should be idempotent, which means we can execute it multiple times and expect the same result and system state. This is important in the event we need to retry a failed task and don't have to worry about resetting the state of our system. Like DAGs, there are several different ways to implement tasks:</p> <ul> <li> <p>using the task decorator <pre><code>from airflow.decorators import dag, task\nfrom airflow.utils.dates import days_ago\n\n@dag(\n    dag_id=\"example\",\n    description=\"Example DAG with task decorators\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"example\"],\n)\ndef example():\n    @task\n    def task_1():\n        return 1\n    @task\n    def task_2(x):\n        return x+1\n</code></pre></p> </li> <li> <p>using Operators <pre><code>from airflow.decorators import dag\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.utils.dates import days_ago\n\n@dag(\n    dag_id=\"example\",\n    description=\"Example DAG with Operators\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"example\"],\n)\ndef example():\n    # Define tasks\n    task_1 = BashOperator(task_id=\"task_1\", bash_command=\"echo 1\")\n    task_2 = BashOperator(task_id=\"task_2\", bash_command=\"echo 2\")\n</code></pre></p> </li> </ul> <p>Though the graphs are directed, we can establish certain trigger rules for each task to execute on conditional successes or failures of the parent tasks.</p>"},{"location":"courses/mlops/orchestration/#operators","title":"Operators","text":"<p>The first method of creating tasks involved using Operators, which defines what exactly the task will be doing. Airflow has many built-in Operators such as the BashOperator or PythonOperator, which allow us to execute bash and Python commands respectively.</p> <pre><code># BashOperator\nfrom airflow.operators.bash_operator import BashOperator\ntask_1 = BashOperator(task_id=\"task_1\", bash_command=\"echo 1\")\n\n# PythonOperator\nfrom airflow.operators.python import PythonOperator\ntask_2 = PythonOperator(\n    task_id=\"task_2\",\n    python_callable=foo,\n    op_kwargs={\"arg1\": ...})\n</code></pre> <p>There are also many other Airflow native Operators (email, S3, MySQL, Hive, etc.), as well as community maintained provider packages (Kubernetes, Snowflake, Azure, AWS, Salesforce, Tableau, etc.), to execute tasks specific to certain platforms or tools.</p> <p>We can also create our own custom Operators by extending the BashOperator class.</p>"},{"location":"courses/mlops/orchestration/#relationships","title":"Relationships","text":"<p>Once we've defined our tasks using Operators or as decorated functions, we need to define the relationships between them (edges). The way we define the relationships depends on how our tasks were defined:</p> <ul> <li> <p>using decorated functions <pre><code># Task relationships\nx = task_1()\ny = task_2(x=x)\n</code></pre></p> </li> <li> <p>using Operators <pre><code># Task relationships\ntask_1 &gt;&gt; task_2  # same as task_1.set_downstream(task_2) or\n                  # task_2.set_upstream(task_1)\n</code></pre></p> </li> </ul> <p>In both scenarios, we'll setting <code>task_2</code> as the downstream task to <code>task_1</code>.</p> <p>Note</p> <p>We can even create intricate DAGs by using these notations to define the relationships.</p> <p><pre><code>task_1 &gt;&gt; [task_2_1, task_2_2] &gt;&gt; task_3\ntask_2_2 &gt;&gt; task_4\n[task_3, task_4] &gt;&gt; task_5\n</code></pre> </p>"},{"location":"courses/mlops/orchestration/#xcoms","title":"XComs","text":"<p>When we use task decorators, we can see how values can be passed between tasks. But, how can we pass values when using Operators? Airflow uses XComs (cross communications) objects, defined with a key, value, timestamp and task_id, to push and pull values between tasks. When we use decorated functions, XComs are being used under the hood but it's abstracted away, allowing us to pass values amongst Python functions seamlessly. But when using Operators, we'll need to explicitly push and pull the values as we need it.</p> <pre><code>def _task_1(ti):\n    x = 2\nti.xcom_push(key=\"x\", value=x)\ndef _task_2(ti):\nx = ti.xcom_pull(key=\"x\", task_ids=[\"task_1\"])[0]\ny = x + 3\nti.xcom_push(key=\"y\", value=y)\n@dag(\n    dag_id=\"example\",\n    description=\"Example DAG\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"example\"],\n)\ndef example2():\n    # Tasks\n    task_1 = PythonOperator(task_id=\"task_1\", python_callable=_task_1)\n    task_2 = PythonOperator(task_id=\"task_2\", python_callable=_task_2)\n    task_1 &gt;&gt; task_2\n</code></pre> <p>We can also view our XComs on the webserver by going to Admin &gt;&gt; XComs:</p> <p>Warning</p> <p>The data we pass between tasks should be small (metadata, metrics, etc.) because Airflow's metadata database is not equipped to hold large artifacts. However, if we do need to store and use the large results of our tasks, it's best to use an external data storage (blog storage, model registry, etc.) and perform heavy processing using Spark or inside data systems like a data warehouse.</p>"},{"location":"courses/mlops/orchestration/#dag-runs","title":"DAG runs","text":"<p>Once we've defined the tasks and their relationships, we're ready to run our DAGs. We'll start defining our DAG like so: <pre><code># Run DAGs\nexample1_dag = example_1()\nexample2_dag = example_2()\n</code></pre></p> <p>The new DAG will have appeared when we refresh our Airflow webserver.</p>"},{"location":"courses/mlops/orchestration/#manual","title":"Manual","text":"<p>Our DAG is initially paused since we specified <code>dags_are_paused_at_creation = True</code> inside our airflow.cfg configuration, so we'll have to manually execute this DAG by clicking on it &gt; unpausing it (toggle) &gt; triggering it (button). To view the logs for any of the tasks in our DAG run, we can click on the task &gt; Log.</p> <p>Note</p> <p>We could also use Airflow's REST API (will configured authorization) or Command-line interface (CLI) to inspect and trigger workflows (and a whole lot more). Or we could even use the <code>trigger_dagrun</code> Operator to trigger DAGs from within another workflow.</p> <pre><code># CLI to run dags\nairflow dags trigger &lt;DAG_ID&gt;\n</code></pre>"},{"location":"courses/mlops/orchestration/#interval","title":"Interval","text":"<p>Had we specified a <code>start_date</code> and <code>schedule_interval</code> when defining the DAG, it would have have automatically executed at the appropriate times. For example, the DAG below will have started two days ago and will be triggered at the start of every day.</p> <pre><code>from airflow.decorators import dag\nfrom airflow.utils.dates import days_ago\nfrom datetime import timedelta\n\n@dag(\n    dag_id=\"example\",\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n    start_date=days_ago(2),\n    tags=[\"example\"],\n    catch_up=False,\n)\n</code></pre> <p>Warning</p> <p>Depending on the <code>start_date</code> and <code>schedule_interval</code>, our workflow should have been triggered several times and Airflow will try to catchup to the current time. We can avoid this by setting <code>catchup=False</code> when defining the DAG. We can also set this configuration as part of the default arguments:</p> <pre><code>default_args = {\n    \"owner\": \"airflow\",\n\"catch_up\": False,\n}\n</code></pre> <p>However, if we did want to run particular runs in the past, we can manually backfill what we need.</p> <p>We could also specify a cron expression for our <code>schedule_interval</code> parameter or even use cron presets.</p> <p>Airflow's Scheduler will run our workflows one <code>schedule_interval</code> from the <code>start_date</code>. For example, if we want our workflow to start on <code>01-01-1983</code> and run <code>@daily</code>, then the first run will be immediately after <code>01-01-1983T11:59</code>.</p>"},{"location":"courses/mlops/orchestration/#sensors","title":"Sensors","text":"<p>While it may make sense to execute many data processing workflows on a scheduled interval, machine learning workflows may require more nuanced triggers. We shouldn't be wasting compute by running executing our workflows just in case we have new data. Instead, we can use sensors to trigger workflows when some external condition is met. For example, we can initiate data processing when a new batch of annotated data appears in a database or when a specific file appears in a file system, etc.</p> <p>There's so much more to Airflow (monitoring, Task groups, smart senors, etc.) so be sure to explore them as you need them by using the official documentation.</p>"},{"location":"courses/mlops/orchestration/#dataops","title":"DataOps","text":"<p>Now that we've reviewed Airflow's major concepts, we're ready to create the DataOps workflows. It's the exact same workflow we defined in our data stack lesson -- extract, load and transform -- but this time we'll be doing everything programmatically and orchestrating it with Airflow.</p> <p>We'll start by creating the script where we'll define our workflows:</p> <pre><code>touch airflow/dags/workflows.py\n</code></pre> <pre><code>from pathlib import Path\nfrom airflow.decorators import dag\nfrom airflow.utils.dates import days_ago\n\n# Default DAG args\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"catch_up\": False,\n}\nBASE_DIR = Path(__file__).parent.parent.parent.absolute()\n\n@dag(\n    dag_id=\"dataops\",\n    description=\"DataOps workflows.\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"dataops\"],\n)\ndef dataops():\n\"\"\"DataOps workflows.\"\"\"\n    pass\n\n# Run DAG\ndo = dataops()\n</code></pre> <p>In two separate terminals, activate the virtual environment and spin up the Airflow webserver and scheduler:</p> <pre><code># Airflow webserver\nsource venv/bin/activate\nexport AIRFLOW_HOME=${PWD}/airflow\nexport GOOGLE_APPLICATION_CREDENTIALS=/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE\nairflow webserver --port 8080\n# Go to http://localhost:8080\n</code></pre> <pre><code># Airflow scheduler\nsource venv/bin/activate\nexport AIRFLOW_HOME=${PWD}/airflow\nexport OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\nexport GOOGLE_APPLICATION_CREDENTIALS=~/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE\nairflow scheduler\n</code></pre>"},{"location":"courses/mlops/orchestration/#extract-and-load","title":"Extract and load","text":"<p>We're going to use the Airbyte connections we set up in our data-stack lesson but this time we're going to programmatically trigger the data syncs with Airflow. First, let's ensure that Airbyte is running on a separate terminal in it's repository:</p> <pre><code>git clone https://github.com/airbytehq/airbyte.git  # skip if already create in data-stack lesson\ncd airbyte\ndocker-compose up\n</code></pre> <p>Next, let's install the required packages and establish the connection between Airbyte and Airflow:</p> <pre><code>pip install apache-airflow-providers-airbyte==3.1.0\n</code></pre> <ol> <li>Go to the Airflow webserver and click <code>Admin</code> &gt; <code>Connections</code> &gt; \u2795</li> <li>Add the connection with the following details: <pre><code>Connection ID: airbyte\nConnection Type: HTTP\nHost: localhost\nPort: 8000\n</code></pre></li> </ol> <p>We could also establish connections programmatically but it\u2019s good to use the UI to understand what\u2019s happening under the hood.</p> <p>In order to execute our extract and load data syncs, we can use the <code>AirbyteTriggerSyncOperator</code>:</p> <pre><code>@dag(...)\ndef dataops():\n\"\"\"Production DataOps workflows.\"\"\"\n    # Extract + Load\n    extract_and_load_projects = AirbyteTriggerSyncOperator(\n        task_id=\"extract_and_load_projects\",\n        airbyte_conn_id=\"airbyte\",\n        connection_id=\"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\",  # REPLACE\n        asynchronous=False,\n        timeout=3600,\n        wait_seconds=3,\n    )\n    extract_and_load_tags = AirbyteTriggerSyncOperator(\n        task_id=\"extract_and_load_tags\",\n        airbyte_conn_id=\"airbyte\",\n        connection_id=\"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\",  # REPLACE\n        asynchronous=False,\n        timeout=3600,\n        wait_seconds=3,\n    )\n\n    # Define DAG\n    extract_and_load_projects\n    extract_and_load_tags\n</code></pre> <p>We can find the <code>connection_id</code> for each Airbyte connection by:</p> <ol> <li>Go to our Airbyte webserver and click <code>Connections</code> on the left menu.</li> <li>Click on the specific connection we want to use and the URL should be like this: <pre><code>https://demo.airbyte.io/workspaces/&lt;WORKSPACE_ID&gt;/connections/&lt;CONNECTION_ID&gt;/status\n</code></pre></li> <li>The string in the <code>CONNECTION_ID</code> position is the connection's id.</li> </ol> <p>We can trigger our DAG right now and view the extracted data be loaded into our BigQuery data warehouse but we'll continue developing and execute our DAG once the entire DataOps workflow has been defined.</p>"},{"location":"courses/mlops/orchestration/#validate","title":"Validate","text":"<p>The specific process of where and how we extract our data can be bespoke but what's important is that we have validation at every step of the way. We'll once again use Great Expectations, as we did in our testing lesson, to validate our extracted and loaded data before transforming it.</p> <p>With the Airflow concepts we've learned so far, there are many ways to use our data validation library to validate our data. Regardless of what data validation tool we use (ex. Great Expectations, TFX, AWS Deequ, etc.) we could use the BashOperator, PythonOperator, etc. to run our tests. However, Great Expectations has a Airflow Provider package to make it even easier to validate our data. This package contains a <code>GreatExpectationsOperator</code> which we can use to execute specific checkpoints as tasks.</p> <pre><code>pip install airflow-provider-great-expectations==0.1.1 great-expectations==0.15.19\ngreat_expectations init\n</code></pre> <p>This will create the following directory within our data-engineering repository:</p> <pre><code>tests/great_expectations/\n\u251c\u2500\u2500 checkpoints/\n\u251c\u2500\u2500 expectations/\n\u251c\u2500\u2500 plugins/\n\u251c\u2500\u2500 uncommitted/\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 great_expectations.yml\n</code></pre>"},{"location":"courses/mlops/orchestration/#data-source","title":"Data source","text":"<p>But first, before we can create our tests, we need to define a new <code>datasource</code> within Great Expectations for our Google BigQuery data warehouse. This will require several packages and exports:</p> <pre><code>pip install pybigquery==0.10.2 sqlalchemy_bigquery==1.4.4\nexport GOOGLE_APPLICATION_CREDENTIALS=/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json  # REPLACE\n</code></pre> <p><pre><code>great_expectations datasource new\n</code></pre> <pre><code>What data would you like Great Expectations to connect to?\n    1. Files on a filesystem (for processing with Pandas or Spark)\n2. Relational database (SQL) \ud83d\udc48\n</code></pre> <pre><code>What are you processing your files with?\n1. MySQL\n2. Postgres\n3. Redshift\n4. Snowflake\n5. BigQuery \ud83d\udc48\n6. other - Do you have a working SQLAlchemy connection string?\n</code></pre></p> <p>This will open up an interactive notebook where we can fill in the following details: <pre><code>datasource_name = \u201cdwh\"\nconnection_string = \u201cbigquery://made-with-ml-359923/mlops_course\u201d\n</code></pre></p>"},{"location":"courses/mlops/orchestration/#suite","title":"Suite","text":"<p>Next, we can create a suite of expectations for our data assets:</p> <pre><code>great_expectations suite new\n</code></pre> <p><pre><code>How would you like to create your Expectation Suite?\n    1. Manually, without interacting with a sample batch of data (default)\n2. Interactively, with a sample batch of data \ud83d\udc48\n    3. Automatically, using a profiler\n</code></pre> <pre><code>Select a datasource\n    1. dwh \ud83d\udc48\n</code></pre> <pre><code>Which data asset (accessible by data connector \"default_inferred_data_connector_name\") would you like to use?\n    1. mlops_course.projects \ud83d\udc48\n    2. mlops_course.tags\n</code></pre> <pre><code>Name the new Expectation Suite [mlops.projects.warning]: projects\n</code></pre></p> <p>This will open up an interactive notebook where we can define our expectations. Repeat the same for creating a suite for our tags data asset as well.</p> Expectations for <code>mlops_course.projects</code> <p>Table expectations <pre><code># data leak\nvalidator.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"])\n</code></pre></p> <p>Column expectations: <pre><code># id\nvalidator.expect_column_values_to_be_unique(column=\"id\")\n\n# create_on\nvalidator.expect_column_values_to_not_be_null(column=\"created_on\")\n\n# title\nvalidator.expect_column_values_to_not_be_null(column=\"title\")\nvalidator.expect_column_values_to_be_of_type(column=\"title\", type_=\"STRING\")\n\n# description\nvalidator.expect_column_values_to_not_be_null(column=\"description\")\nvalidator.expect_column_values_to_be_of_type(column=\"description\", type_=\"STRING\")\n</code></pre></p> Expectations for <code>mlops_course.tags</code> <p>Column expectations: <pre><code># id\nvalidator.expect_column_values_to_be_unique(column=\"id\")\n\n# tag\nvalidator.expect_column_values_to_not_be_null(column=\"tag\")\nvalidator.expect_column_values_to_be_of_type(column=\"tag\", type_=\"STRING\")\n</code></pre></p>"},{"location":"courses/mlops/orchestration/#checkpoints","title":"Checkpoints","text":"<p>Once we have our suite of expectations, we're ready to check checkpoints to execute these expectations:</p> <pre><code>great_expectations checkpoint new projects\n</code></pre> <p>This will, of course, open up an interactive notebook. Just ensure that the following information is correct (the default values may not be): <pre><code>datasource_name: dwh\ndata_asset_name: mlops_course.projects\nexpectation_suite_name: projects\n</code></pre></p> <p>And repeat the same for creating a checkpoint for our tags suite.</p>"},{"location":"courses/mlops/orchestration/#tasks_1","title":"Tasks","text":"<p>With our checkpoints defined, we're ready to apply them to our data assets in our warehouse.</p> <pre><code>GE_ROOT_DIR = Path(BASE_DIR, \"great_expectations\")\n\n@dag(...)\ndef dataops():\n    ...\n    validate_projects = GreatExpectationsOperator(\n        task_id=\"validate_projects\",\n        checkpoint_name=\"projects\",\n        data_context_root_dir=GE_ROOT_DIR,\n        fail_task_on_validation_failure=True,\n    )\n    validate_tags = GreatExpectationsOperator(\n        task_id=\"validate_tags\",\n        checkpoint_name=\"tags\",\n        data_context_root_dir=GE_ROOT_DIR,\n        fail_task_on_validation_failure=True,\n    )\n\n    # Define DAG\n    extract_and_load_projects &gt;&gt; validate_projects\n    extract_and_load_tags &gt;&gt; validate_tags\n</code></pre>"},{"location":"courses/mlops/orchestration/#transform","title":"Transform","text":"<p>Once we've validated our extracted and loaded data, we're ready to transform it. Our DataOps workflows are not specific to any particular downstream application so the transformation must be globally relevant (ex. cleaning missing data, aggregation, etc.). Just like in our data stack lesson, we're going to use dbt to transform our data. However, this time, we're going to do everything programmatically using the open-source dbt-core package.</p> <p>In the root of our data-engineering repository, initialize our dbt directory with the following command: <pre><code>dbt init dbf_transforms\n</code></pre> <pre><code>Which database would you like to use?\n[1] bigquery \ud83d\udc48\n</code></pre> <pre><code>Desired authentication method option:\n[1] oauth\n[2] service_account \ud83d\udc48\n</code></pre> <pre><code>keyfile: /Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json  # REPLACE\nproject (GCP project id): made-with-ml-XXXXXX  # REPLACE\ndataset: mlops_course\nthreads: 1\njob_execution_timeout_seconds: 300\n</code></pre> <pre><code>Desired location option:\n[1] US  \ud83d\udc48  # or what you picked when defining your dataset in Airbyte DWH destination setup\n[2] EU\n</code></pre></p>"},{"location":"courses/mlops/orchestration/#models","title":"Models","text":"<p>We'll prepare our dbt models as we did using the dbt Cloud IDE in the previous lesson.</p> <pre><code>cd dbt_transforms\nrm -rf models/example\nmkdir models/labeled_projects\ntouch models/labeled_projects/labeled_projects.sql\ntouch models/labeled_projects/schema.yml\n</code></pre> <p>and add the following code to our model files:</p> <pre><code>-- models/labeled_projects/labeled_projects.sql\nSELECT p.id, created_on, title, description, tag\nFROM `made-with-ml-XXXXXX.mlops_course.projects` p  -- REPLACE\nLEFT JOIN `made-with-ml-XXXXXX.mlops_course.tags` t  -- REPLACE\nON p.id = t.id\n</code></pre> <pre><code># models/labeled_projects/schema.yml\n\nversion: 2\n\nmodels:\n- name: labeled_projects\ndescription: \"Tags for all projects\"\ncolumns:\n- name: id\ndescription: \"Unique ID of the project.\"\ntests:\n- unique\n- not_null\n- name: title\ndescription: \"Title of the project.\"\ntests:\n- not_null\n- name: description\ndescription: \"Description of the project.\"\ntests:\n- not_null\n- name: tag\ndescription: \"Labeled tag for the project.\"\ntests:\n- not_null\n</code></pre> <p>And we can use the BashOperator to execute our dbt commands like so:</p> <pre><code>DBT_ROOT_DIR = Path(BASE_DIR, \"dbt_transforms\")\n\n@dag(...)\ndef dataops():\n    ...\n    # Transform\n    transform = BashOperator(task_id=\"transform\", bash_command=f\"cd {DBT_ROOT_DIR} &amp;&amp; dbt run &amp;&amp; dbt test\")\n\n    # Define DAG\n    extract_and_load_projects &gt;&gt; validate_projects\n    extract_and_load_tags &gt;&gt; validate_tags\n    [validate_projects, validate_tags] &gt;&gt; transform\n</code></pre> <p>Programmatically using dbt Cloud</p> <p>While we developed locally, we could just as easily use Airflow\u2019s dbt cloud provider to connect to our dbt cloud and use the different operators to schedule jobs. This is recommended for production because we can design jobs with proper environment, authentication, schemas, etc.</p> <ul> <li>Connect Airflow with dbt Cloud:</li> </ul> <p>Go to Admin &gt; Connections &gt; + <pre><code>Connection ID: dbt_cloud_default\nConnection Type: dbt Cloud\nAccount ID: View in URL of https://cloud.getdbt.com/\nAPI Token: View in https://cloud.getdbt.com/#/profile/api/\n</code></pre></p> <ul> <li>Transform</li> </ul> <p><pre><code>pip install apache-airflow-providers-dbt-cloud==2.1.0\n</code></pre> <pre><code>from airflow.providers.dbt.cloud.operators.dbt import DbtCloudRunJobOperator\ntransform = DbtCloudRunJobOperator(\n    task_id=\"transform\",\n    job_id=118680,  # Go to dbt UI &gt; click left menu &gt; Jobs &gt; Transform &gt; job_id in URL\n    wait_for_termination=True,  # wait for job to finish running\n    check_interval=10,  # check job status\n    timeout=300,  # max time for job to execute\n)\n</code></pre></p>"},{"location":"courses/mlops/orchestration/#validate_1","title":"Validate","text":"<p>And of course, we'll want to validate our transformations beyond dbt's built-in methods, using great expectations. We'll create a suite and checkpoint as we did above for our projects and tags data assets. <pre><code>great_expectations suite new  # for mlops_course.labeled_projects\n</code></pre></p> Expectations for <code>mlops_course.labeled_projects</code> <p>Table expectations <pre><code># data leak\nvalidator.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"])\n</code></pre></p> <p>Column expectations: <pre><code># id\nvalidator.expect_column_values_to_be_unique(column=\"id\")\n\n# create_on\nvalidator.expect_column_values_to_not_be_null(column=\"created_on\")\n\n# title\nvalidator.expect_column_values_to_not_be_null(column=\"title\")\nvalidator.expect_column_values_to_be_of_type(column=\"title\", type_=\"STRING\")\n\n# description\nvalidator.expect_column_values_to_not_be_null(column=\"description\")\nvalidator.expect_column_values_to_be_of_type(column=\"description\", type_=\"STRING\")\n\n# tag\nvalidator.expect_column_values_to_not_be_null(column=\"tag\")\nvalidator.expect_column_values_to_be_of_type(column=\"tag\", type_=\"STRING\")\n</code></pre></p> <p><pre><code>great_expectations checkpoint new labeled_projects\n</code></pre> <pre><code>datasource_name: dwh\ndata_asset_name: mlops_course.labeled_projects\nexpectation_suite_name: labeled_projects\n</code></pre></p> <p>and just like how we added the validation task for our extracted and loaded data, we can do the same for our transformed data in Airflow:</p> <pre><code>@dag(...)\ndef dataops():\n    ...\n    # Transform\n    transform = BashOperator(task_id=\"transform\", bash_command=f\"cd {DBT_ROOT_DIR} &amp;&amp; dbt run &amp;&amp; dbt test\")\n    validate_transforms = GreatExpectationsOperator(\n        task_id=\"validate_transforms\",\n        checkpoint_name=\"labeled_projects\",\n        data_context_root_dir=GE_ROOT_DIR,\n        fail_task_on_validation_failure=True,\n    )\n\n    # Define DAG\n    extract_and_load_projects &gt;&gt; validate_projects\n    extract_and_load_tags &gt;&gt; validate_tags\n    [validate_projects, validate_tags] &gt;&gt; transform &gt;&gt; validate_transforms\n</code></pre> <p>Now we have our entire DataOps DAG define and executing it will prepare our data from extraction to loading to transformation (and with validation at every step of the way) for downstream applications.</p> <p>Typically we'll use sensors to trigger workflows when a condition is met or trigger them directly from the external source via API calls, etc. For our ML use cases, this could be at regular intervals or when labeling or monitoring workflows trigger retraining, etc.</p>"},{"location":"courses/mlops/orchestration/#mlops","title":"MLOps","text":"<p>Once we have our data prepared, we're ready to create one of the many potential downstream applications that will depend on it. Let's head back to our <code>mlops-course</code> project and follow the same set up instructions for Airflow (you can stop the Airflow webserver and scheduler from our data-engineering project since we'll reuse PORT 8000).</p> <pre><code># Airflow webserver\nsource venv/bin/activate\nexport AIRFLOW_HOME=${PWD}/airflow\nexport GOOGLE_APPLICATION_CREDENTIALS=/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE\nairflow webserver --port 8080\n# Go to http://localhost:8080\n</code></pre> <pre><code># Airflow scheduler\nsource venv/bin/activate\nexport AIRFLOW_HOME=${PWD}/airflow\nexport OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\nexport GOOGLE_APPLICATION_CREDENTIALS=~/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE\nairflow scheduler\n</code></pre> <pre><code>touch airflow/dags/workflows.py\n</code></pre> <pre><code># airflow/dags/workflows.py\nfrom pathlib import Path\nfrom airflow.decorators import dag\nfrom airflow.utils.dates import days_ago\n\n# Default DAG args\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"catch_up\": False,\n}\n\n@dag(\n    dag_id=\"mlops\",\n    description=\"MLOps tasks.\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"mlops\"],\n)\ndef mlops():\n\"\"\"MLOps workflows.\"\"\"\n    pass\n\n# Run DAG\nml = mlops()\n</code></pre>"},{"location":"courses/mlops/orchestration/#dataset","title":"Dataset","text":"<p>We already had an <code>tagifai.elt_data</code> function defined to prepare our data but if we want to leverage the data inside our data warehouse, we'll want to connect to it.</p> <pre><code>pip install google-cloud-bigquery==1.21.0\n</code></pre> <pre><code># airflow/dags/workflows.py\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\nPROJECT_ID = \"made-with-ml-XXXXX\" # REPLACE\nSERVICE_ACCOUNT_KEY_JSON = \"/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json\"  # REPLACE\n\ndef _extract_from_dwh():\n\"\"\"Extract labeled data from\n    our BigQuery data warehouse and\n    save it locally.\"\"\"\n    # Establish connection to DWH\n    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_KEY_JSON)\n    client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n\n    # Query data\n    query_job = client.query(\"\"\"\n        SELECT *\n        FROM mlops_course.labeled_projects\"\"\")\n    results = query_job.result()\n    results.to_dataframe().to_csv(Path(config.DATA_DIR, \"labeled_projects.csv\"), index=False)\n\n@dag(\n    dag_id=\"mlops\",\n    description=\"MLOps tasks.\",\n    default_args=default_args,\n    schedule_interval=None,\n    start_date=days_ago(2),\n    tags=[\"mlops\"],\n)\ndef mlops():\n\"\"\"MLOps workflows.\"\"\"\n    extract_from_dwh = PythonOperator(\n        task_id=\"extract_data\",\n        python_callable=_extract_from_dwh,\n    )\n\n    # Define DAG\n    extract_from_dwh\n</code></pre>"},{"location":"courses/mlops/orchestration/#validate_2","title":"Validate","text":"<p>Next, we'll use Great Expectations to validate our data. Even though we've already validated our data, it's a best practice to test for data quality whenever there is a hand-off of data from one place to another. We've already created a checkpoint for our <code>labeled_projects</code> in our testing lesson so we'll just leverage that inside our MLOps DAG.</p> <pre><code>pip install airflow-provider-great-expectations==0.1.1 great-expectations==0.15.19\n</code></pre> <pre><code>from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator\nfrom config import config\n\nGE_ROOT_DIR = Path(config.BASE_DIR, \"tests\", \"great_expectations\")\n\n@dag(...)\ndef mlops():\n\"\"\"MLOps workflows.\"\"\"\n    extract_from_dwh = PythonOperator(\n        task_id=\"extract_data\",\n        python_callable=_extract_from_dwh,\n    )\n    validate = GreatExpectationsOperator(\n        task_id=\"validate\",\n        checkpoint_name=\"labeled_projects\",\n        data_context_root_dir=GE_ROOT_DIR,\n        fail_task_on_validation_failure=True,\n    )\n\n    # Define DAG\n    extract_from_dwh &gt;&gt; validate\n</code></pre>"},{"location":"courses/mlops/orchestration/#train","title":"Train","text":"<p>Finally, we'll optimize and train a model using our validated data.</p> <pre><code>from airflow.operators.python_operator import PythonOperator\nfrom config import config\nfrom tagifai import main\n\n@dag(...)\ndef mlops():\n\"\"\"MLOps workflows.\"\"\"\n    ...\n    optimize = PythonOperator(\n        task_id=\"optimize\",\n        python_callable=main.optimize,\n        op_kwargs={\n            \"args_fp\": Path(config.CONFIG_DIR, \"args.json\"),\n            \"study_name\": \"optimization\",\n            \"num_trials\": 1,\n        },\n    )\n    train = PythonOperator(\n        task_id=\"train\",\n        python_callable=main.train_model,\n        op_kwargs={\n            \"args_fp\": Path(config.CONFIG_DIR, \"args.json\"),\n            \"experiment_name\": \"baselines\",\n            \"run_name\": \"sgd\",\n        },\n    )\n</code></pre> <p>And with that we have our MLOps workflow defined that uses the prepared data from our DataOps workflow. At this point, we can add additional tasks for offline/online evaluation, deployment, etc. with the same process as above.</p>"},{"location":"courses/mlops/orchestration/#continual-learning","title":"Continual learning","text":"<p>The DataOps and MLOps workflows connect to create an ML system that's capable of continually learning. Such a system will guide us with when to update, what exactly to update and how to update it (easily).</p> <p>We use the word continual (repeat with breaks) instead of continuous (repeat without interruption / intervention) because we're not trying to create a system that will automatically update with new incoming data without human intervention.</p>"},{"location":"courses/mlops/orchestration/#monitoring","title":"Monitoring","text":"<p>Our production system is live and monitored. When an event of interest occurs (ex. drift), one of several events needs to be triggered:</p> <ul> <li><code>continue</code>: with the currently deployed model without any updates. However, an alert was raised so it should analyzed later to reduce false positive alerts.</li> <li><code>improve</code>: by retraining the model to avoid performance degradation causes by meaningful drift (data, target, concept, etc.).</li> <li><code>inspect</code>: to make a decision. Typically expectations are reassessed, schemas are reevaluated for changes, slices are reevaluated, etc.</li> <li><code>rollback</code>: to a previous version of the model because of an issue with the current deployment. Typically these can be avoided using robust deployment strategies (ex. dark canary).</li> </ul>"},{"location":"courses/mlops/orchestration/#retraining","title":"Retraining","text":"<p>If we need to improve on the existing version of the model, it's not just the matter of fact of rerunning the model creation workflow on the new dataset. We need to carefully compose the training data in order to avoid issues such as catastrophic forgetting (forget previously learned patterns when presented with new data).</p> <ul> <li><code>labeling</code>: new incoming data may need to be properly labeled before being used (we cannot just depend on proxy labels).</li> <li><code>active learning</code>: we may not be able to explicitly label every single new data point so we have to leverage active learning workflows to complete the labeling process.</li> <li><code>QA</code>: quality assurance workflows to ensure that labeling is accurate, especially for known false positives/negatives and historically poorly performing slices of data.</li> <li><code>augmentation</code>: increasing our training set with augmented data that's representative of the original dataset.</li> <li><code>sampling</code>: upsampling and downsampling to address imbalanced data slices.</li> <li><code>evaluation</code>:  creation of an evaluation dataset that's representative of what the model will encounter once deployed.</li> </ul> <p>Once we have the proper dataset for retraining, we can kickoff the workflows to update our system!</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Orchestration for Machine Learning - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/pre-commit/","title":"Pre-commit","text":""},{"location":"courses/mlops/pre-commit/#intuition","title":"Intuition","text":"<p>Before performing a commit to our local repository, there are a lot of items on our mental todo list, ranging from styling, formatting, testing, etc. And it's very easy to forget some of these steps, especially when we want to \"push to quick fix\". To help us manage all these important steps, we can use pre-commit hooks, which will automatically be triggered when we try to perform a commit. These hooks can ensure that certain rules are followed or specific actions are executed successfully and if any of them fail, the commit will be aborted.</p>"},{"location":"courses/mlops/pre-commit/#installation","title":"Installation","text":"<p>We'll be using the Pre-commit framework to help us automatically perform important checks via hooks when we make a commit.</p> <p>We'll start by installing and autoupdating pre-commit (we only have to do this once). <pre><code>pre-commit install\npre-commit autoupdate\n</code></pre></p>"},{"location":"courses/mlops/pre-commit/#config","title":"Config","text":"<p>We define our pre-commit hooks via a <code>.pre-commit-config.yaml</code> configuration file. We can either create our yaml configuration from scratch or use the pre-commit CLI to create a sample configuration which we can add to.</p> <pre><code># Simple config\npre-commit sample-config &gt; .pre-commit-config.yaml\ncat .pre-commit-config.yaml\n</code></pre>"},{"location":"courses/mlops/pre-commit/#hooks","title":"Hooks","text":"<p>When it comes to creating and using hooks, we have several options to choose from.</p>"},{"location":"courses/mlops/pre-commit/#built-in","title":"Built-in","text":"<p>Inside the sample configuration, we can see that pre-commit has added some default hooks from it's repository. It specifies the location of the repository, version as well as the specific hook ids to use. We can read about the function of these hooks and add even more by exploring pre-commit's built-in hooks. Many of them also have additional arguments that we can configure to customize the hook.</p> <pre><code># Inside .pre-commit-config.yaml\n...\n-   id: check-added-large-files\nargs: ['--maxkb=1000']\nexclude: \"notebooks\"\n...\n</code></pre> <p>Be sure to explore the many other built-in hooks because there are some really useful ones that we use in our project. For example, <code>check-merge-conflict</code> to see if there are any lingering merge conflict strings or <code>detect-aws-credentials</code> if we accidentally left our credentials exposed in a file, and so much more.</p> <p>And we can also exclude certain files from being processed by the hooks by using the optional exclude key. There are many other optional keys we can configure for each hook ID.</p> <pre><code># Inside .pre-commit-config.yaml\n...\n-   id: check-yaml\nexclude: \"mkdocs.yml\"\n...\n</code></pre>"},{"location":"courses/mlops/pre-commit/#custom","title":"Custom","text":"<p>Besides pre-commit's built-in hooks, there are also many custom, 3rd party popular hooks that we can choose from. For example, if we want to apply formatting checks with Black as a hook, we can leverage Black's pre-commit hook.</p> <pre><code># Inside .pre-commit-config.yaml\n...\n-   repo: https://github.com/psf/black\nrev: 20.8b1\nhooks:\n-   id: black\nargs: []\nfiles: .\n...\n</code></pre> <p>This specific hook is defined under a .pre-commit-hooks.yaml inside Black's repository, as are other custom hooks under their respective package repositories.</p>"},{"location":"courses/mlops/pre-commit/#local","title":"Local","text":"<p>We can also create our own local hooks without configuring a separate .pre-commit-hooks.yaml. Here we're defining two pre-commit hooks, <code>test-non-training</code> and <code>clean</code>, to run some commands that we've defined in our Makefile. Similarly, we can run any entry command with arguments to create hooks very quickly.</p> <pre><code># Inside .pre-commit-config.yaml\n...\n-   repo: local\nhooks:\n-   id: clean\nname: clean\nentry: make\nargs: [\"clean\"]\nlanguage: system\npass_filenames: false\n</code></pre>"},{"location":"courses/mlops/pre-commit/#commit","title":"Commit","text":"<p>Our pre-commit hooks will automatically execute when we try to make a commit. We'll be able to see if each hook passed or failed and make any changes. If any of the hooks fail, we have to fix the errors ourselves or, in many instances, reformatting will occur automatically.</p> <pre>\ncheck yaml..............................................PASSED\nclean...................................................FAILED\n</pre> <p>In the event that any of the hooks failed, we need to <code>add</code> and <code>commit</code> again to ensure that all hooks are passed.</p> <pre><code>git add .\ngit commit -m &lt;MESSAGE&gt;\n</code></pre>"},{"location":"courses/mlops/pre-commit/#run","title":"Run","text":"<p>Though pre-commit hooks are meant to run before (pre) a commit, we can manually trigger all or individual hooks on all or a set of files.</p> <pre><code># Run\npre-commit run --all-files  # run all hooks on all files\npre-commit run &lt;HOOK_ID&gt; --all-files # run one hook on all files\npre-commit run --files &lt;PATH_TO_FILE&gt;  # run all hooks on a file\npre-commit run &lt;HOOK_ID&gt; --files &lt;PATH_TO_FILE&gt; # run one hook on a file\n</code></pre>"},{"location":"courses/mlops/pre-commit/#skip","title":"Skip","text":"<p>It is highly not recommended to skip running any of the pre-commit hooks because they are there for a reason. But for some highly urgent, world saving commits, we can use the no-verify flag.</p> <pre><code># Commit without hooks\ngit commit -m &lt;MESSAGE&gt; --no-verify\n</code></pre> <p>Highly recommend not doing this because no commit deserves to be force pushed no matter how \"small\" your change was. If you accidentally did this and want to clear the cache, run <code>pre-commit run --all-files</code> and execute the commit message operation again.</p>"},{"location":"courses/mlops/pre-commit/#update","title":"Update","text":"<p>In our <code>.pre-commit-config.yaml</code> configuration files, we've had to specify the versions for each of the repositories so we can use their latest hooks. Pre-commit has an autoupdate CLI command which will update these versions as they become available.</p> <pre><code># Autoupdate\npre-commit autoupdate\n</code></pre> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Pre-commit - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/preparation/","title":"Data Preparation","text":""},{"location":"courses/mlops/preparation/#intuition","title":"Intuition","text":"<p>We'll start by first preparing our data by ingesting it from source and splitting it into training, validation and test data splits.</p>"},{"location":"courses/mlops/preparation/#ingestion","title":"Ingestion","text":"<p>Our data could reside in many different places (databases, files, etc.) and exist in different formats (CSV, JSON, Parquet, etc.). For our application, we'll load the data from a CSV file to a Pandas DataFrame using the <code>read_csv</code> function.</p> <p>Here is a quick refresher on the Pandas library.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code># Data ingestion\nDATASET_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\ndf = pd.read_csv(DATASET_LOC)\ndf.head()\n</code></pre> id created_on title description tag 0 6 2020-02-20 06:43:18 Comparison between YOLO and RCNN on real world... Bringing theory to experiment is cool. We can ... computer-vision 1 7 2020-02-20 06:47:21 Show, Infer &amp; Tell: Contextual Inference for C... The beauty of the work lies in the way it arch... computer-vision 2 9 2020-02-24 16:24:45 Awesome Graph Classification A collection of important graph embedding, cla... other 3 15 2020-02-28 23:55:26 Awesome Monte Carlo Tree Search A curated list of Monte Carlo tree search pape... other 4 25 2020-03-07 23:04:31 AttentionWalk A PyTorch Implementation of \"Watch Your Step: ... other <p>In our data engineering lesson we'll look at how to continually ingest data from more complex sources (ex. data warehouses)</p>"},{"location":"courses/mlops/preparation/#splitting","title":"Splitting","text":"<p>Next, we need to split our training dataset into <code>train</code> and <code>val</code> data splits.</p> <ol> <li>Use the <code>train</code> split to train the model. <p>Here the model will have access to both inputs (features) and outputs (labels) to optimize its internal weights.</p> </li> <li>After each iteration (epoch) through the training split, we will use the <code>val</code> split to determine the model's performance. <p>Here the model will not use the labels to optimize its weights but instead, we will use the validation performance to optimize training hyperparameters such as the learning rate, etc.</p> </li> <li>Finally, we will use a separate holdout <code>test</code> dataset to determine the model's performance after training. <p>This is our best measure of how the model may behave on new, unseen data that is from a similar distribution to our training dataset.</p> </li> </ol> <p>Tip</p> <p>For our application, we will have a training dataset to split into <code>train</code> and <code>val</code> splits and a separate testing dataset for the <code>test</code> set. While we could have one large dataset and split that into the three splits, it's a good idea to have a separate test dataset. Over time, our training data may grow and our test splits will look different every time. This will make it difficult to compare models against other models and against each other.</p> <p>We can view the class counts in our dataset by using the <code>pandas.DataFrame.value_counts</code> function:</p> <pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Value counts\ndf.tag.value_counts()\n</code></pre> <pre>\ntag\nnatural-language-processing    310\ncomputer-vision                285\nother                          106\nmlops                           63\nName: count, dtype: int64\n</pre> <p>For our multi-class task (where each project has exactly one tag), we want to ensure that the data splits have similar class distributions. We can achieve this by specifying how to stratify the split by using the <code>stratify</code> keyword argument with sklearn's <code>train_test_split()</code> function.</p> <p>Creating proper data splits</p> <p>What are the criteria we should focus on to ensure proper data splits?</p> Show answer <ul> <li>the dataset (and each data split) should be representative of data we will encounter</li> <li>equal distributions of output values across all splits</li> <li>shuffle your data if it's organized in a way that prevents input variance</li> <li>avoid random shuffles if your task can suffer from data leaks (ex. <code>time-series</code>)</li> </ul> <pre><code># Split dataset\ntest_size = 0.2\ntrain_df, val_df = train_test_split(df, stratify=df.tag, test_size=test_size, random_state=1234)\n</code></pre> <p>How can we validate that our data splits have similar class distributions? We can view the frequency of each class in each split:</p> <pre><code># Train value counts\ntrain_df.tag.value_counts()\n</code></pre> <pre>\ntag\nnatural-language-processing    248\ncomputer-vision                228\nother                           85\nmlops                           50\nName: count, dtype: int64\n</pre> <p>Before we view our validation split's class counts, recall that our validation split is only <code>test_size</code> of the entire dataset. So we need to adjust the value counts so that we can compare it to the training split's class counts.</p> \\[ \\alpha * N_{test} = N_{train} \\] \\[ N_{train} = 1 - N_{test} \\] \\[ \\alpha = \\frac{N_{train}}{N_{test}} = \\frac{1 - N_{test}}{N_{test}} \\] <pre><code># Validation (adjusted) value counts\nval_df.tag.value_counts() * int((1-test_size) / test_size)\n</code></pre> <pre>\ntag\nnatural-language-processing    248\ncomputer-vision                228\nother                           84\nmlops                           52\nName: count, dtype: int64\n</pre> <p>These adjusted counts looks very similar to our train split's counts. Now we're ready to explore our dataset!</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Preparation - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/preprocessing/","title":"Data Preprocessing","text":""},{"location":"courses/mlops/preprocessing/#intuition","title":"Intuition","text":"<p>Data preprocessing can be categorized into two types of processes: preparation and transformation. We'll explore common preprocessing techniques and then we'll preprocess our dataset.</p> <p>Warning</p> <p>Certain preprocessing steps are <code>global</code> (don't depend on our dataset, ex. lower casing text, removing stop words, etc.) and others are <code>local</code> (constructs are learned only from the training split, ex. vocabulary, standardization, etc.). For the local, dataset-dependent preprocessing steps, we want to ensure that we split the data first before preprocessing to avoid data leaks.</p>"},{"location":"courses/mlops/preprocessing/#preparing","title":"Preparing","text":"<p>Preparing the data involves organizing and cleaning the data.</p>"},{"location":"courses/mlops/preprocessing/#joins","title":"Joins","text":"<p>Performing SQL joins with existing data tables to organize all the relevant data you need into one view. This makes working with our dataset a whole lot easier.</p> <pre><code>SELECT * FROM A\nINNER JOIN B on A.id == B.id\n</code></pre> <p>Warning</p> <p>We need to be careful to perform point-in-time valid joins to avoid data leaks. For example, if Table B may have features for objects in Table A that were not available at the time inference would have been needed.</p>"},{"location":"courses/mlops/preprocessing/#missing-values","title":"Missing values","text":"<p>First, we'll have to identify the rows with missing values and once we do, there are several approaches to dealing with them.</p> <ul> <li> <p>omit samples with missing values (if only a small subset are missing it) <pre><code># Drop a row (sample) by index\ndf.drop([4, 10, ...])\n# Conditionally drop rows (samples)\ndf = df[df.value &gt; 0]\n# Drop samples with any missing feature\ndf = df[df.isnull().any(axis=1)]\n</code></pre></p> </li> <li> <p>omit the entire feature (if too many samples are missing the value) <pre><code># Drop a column (feature)\ndf.drop([\"A\"], axis=1)\n</code></pre></p> </li> <li> <p>fill in missing values for features (using domain knowledge, heuristics, etc.) <pre><code># Fill in missing values with mean\ndf.A = df.A.fillna(df.A.mean())\n</code></pre></p> </li> <li> <p>may not always seem \"missing\" (ex. 0, null, NA, etc.) <pre><code># Replace zeros to NaNs\nimport numpy as np\ndf.A = df.A.replace({\"0\": np.nan, 0: np.nan})\n</code></pre></p> </li> </ul>"},{"location":"courses/mlops/preprocessing/#outliers-anomalies","title":"Outliers (anomalies)","text":"<ul> <li>craft assumptions about what is a \"normal\" expected value <pre><code># Ex. Feature value must be within 2 standard deviations\ndf[np.abs(df.A - df.A.mean()) &lt;= (2 * df.A.std())]\n</code></pre></li> <li>be careful not to remove important outliers (ex. fraud)</li> <li>values may not be outliers when we apply a transformation (ex. power law)</li> <li>anomalies can be global (point), contextual (conditional) or collective (individual points are not anomalous and the collective group is an outlier)</li> </ul>"},{"location":"courses/mlops/preprocessing/#feature-engineering","title":"Feature engineering","text":"<p>Feature engineering involves combining features in unique ways to draw out signal.</p> <pre><code># Input\ndf.C = df.A + df.B\n</code></pre> <p>Tip</p> <p>Feature engineering can be done in collaboration with domain experts that can guide us on what features to engineer and use.</p>"},{"location":"courses/mlops/preprocessing/#cleaning","title":"Cleaning","text":"<p>Cleaning our data involves apply constraints to make it easier for our models to extract signal from the data.</p> <ul> <li>use domain expertise and EDA</li> <li>apply constraints via filters</li> <li>ensure data type consistency</li> <li>removing data points with certain or null column values</li> <li>images (crop, resize, clip, etc.) <pre><code># Resize\nimport cv2\ndims = (height, width)\nresized_img = cv2.resize(src=img, dsize=dims, interpolation=cv2.INTER_LINEAR)\n</code></pre></li> <li>text (lower, stem, lemmatize, regex, etc.) <pre><code># Lower case the text\ntext = text.lower()\n</code></pre></li> </ul>"},{"location":"courses/mlops/preprocessing/#transformations","title":"Transformations","text":"<p>Transforming the data involves feature encoding and engineering.</p>"},{"location":"courses/mlops/preprocessing/#scaling","title":"Scaling","text":"<ul> <li>required for models where the scale of the input affects the processes</li> <li>learn constructs from train split and apply to other splits (local)</li> <li> <p>don't blindly scale features (ex. categorical features)</p> </li> <li> <p>standardization: rescale values to mean 0, std 1</p> <p><pre><code># Standardization\nimport numpy as np\nx = np.random.random(4) # values between 0 and 1\nprint (\"x:\\n\", x)\nprint (f\"mean: {np.mean(x):.2f}, std: {np.std(x):.2f}\")\nx_standardized = (x - np.mean(x)) / np.std(x)\nprint (\"x_standardized:\\n\", x_standardized)\nprint (f\"mean: {np.mean(x_standardized):.2f}, std: {np.std(x_standardized):.2f}\")\n</code></pre> <pre>\nx: [0.36769939 0.82302265 0.9891467  0.56200803]\nmean: 0.69, std: 0.24\nx_standardized: [-1.33285946  0.57695671  1.27375049 -0.51784775]\nmean: 0.00, std: 1.00\n</pre></p> </li> <li> <p>min-max: rescale values between a min and max</p> <p><pre><code># Min-max\nimport numpy as np\nx = np.random.random(4) # values between 0 and 1\nprint (\"x:\", x)\nprint (f\"min: {x.min():.2f}, max: {x.max():.2f}\")\nx_scaled = (x - x.min()) / (x.max() - x.min())\nprint (\"x_scaled:\", x_scaled)\nprint (f\"min: {x_scaled.min():.2f}, max: {x_scaled.max():.2f}\")\n</code></pre> <pre>\nx: [0.20195674 0.99108855 0.73005081 0.02540603]\nmin: 0.03, max: 0.99\nx_scaled: [0.18282479 1.         0.72968575 0.        ]\nmin: 0.00, max: 1.00\n</pre></p> </li> <li> <p>binning: convert a continuous feature into categorical using bins</p> <p><pre><code># Binning\nimport numpy as np\nx = np.random.random(4) # values between 0 and 1\nprint (\"x:\", x)\nbins = np.linspace(0, 1, 5) # bins between 0 and 1\nprint (\"bins:\", bins)\nbinned = np.digitize(x, bins)\nprint (\"binned:\", binned)\n</code></pre> <pre>\nx: [0.54906364 0.1051404  0.2737904  0.2926313 ]\nbins: [0.   0.25 0.5  0.75 1.  ]\nbinned: [3 1 2 2]\n</pre></p> </li> <li> <p>and many more!</p> </li> </ul>"},{"location":"courses/mlops/preprocessing/#encoding","title":"Encoding","text":"<ul> <li> <p>allows for representing data efficiently (maintains signal) and effectively (learns patterns, ex. one-hot vs embeddings)</p> </li> <li> <p>label: unique index for categorical value</p> <p><pre><code># Label encoding\nlabel_encoder.class_to_index = {\n\"attention\": 0,\n\"autoencoders\": 1,\n\"convolutional-neural-networks\": 2,\n\"data-augmentation\": 3,\n... }\nlabel_encoder.transform([\"attention\", \"data-augmentation\"])\n</code></pre> <pre>\narray([2, 2, 1])\n</pre></p> </li> <li> <p>one-hot: representation as binary vector</p> <p><pre><code># One-hot encoding\none_hot_encoder.transform([\"attention\", \"data-augmentation\"])\n</code></pre> <pre>\narray([1, 0, 0, 1, 0, ..., 0])\n</pre></p> </li> <li> <p>embeddings: dense representations capable of representing context</p> <p><pre><code># Embeddings\nself.embeddings = nn.Embedding(\n    embedding_dim=embedding_dim, num_embeddings=vocab_size)\nx_in = self.embeddings(x_in)\nprint (x_in.shape)\n</code></pre> <pre>\n(len(X), embedding_dim)\n</pre></p> </li> <li> <p>and many more!</p> </li> </ul>"},{"location":"courses/mlops/preprocessing/#extraction","title":"Extraction","text":"<ul> <li>signal extraction from existing features</li> <li>combine existing features</li> <li>transfer learning: using a pretrained model as a feature extractor and finetuning on it's results</li> <li> <p>autoencoders: learn to encode inputs for compressed knowledge representation</p> </li> <li> <p>principle component analysis (PCA): linear dimensionality reduction to project data in a lower dimensional space.</p> <p><pre><code># PCA\nimport numpy as np\nfrom sklearn.decomposition import PCA\nX = np.array([[-1, -1, 3], [-2, -1, 2], [-3, -2, 1]])\npca = PCA(n_components=2)\npca.fit(X)\nprint (pca.transform(X))\nprint (pca.explained_variance_ratio_)\nprint (pca.singular_values_)\n</code></pre> <pre>\n[[-1.44245791 -0.1744313 ]\n [-0.1148688   0.31291575]\n [ 1.55732672 -0.13848446]]\n[0.96838847 0.03161153]\n[2.12582835 0.38408396]\n</pre></p> </li> <li> <p>counts (ngram): sparse representation of text as matrix of token counts \u2014 useful if feature values have lot's of meaningful, separable signal.</p> <p><pre><code># Counts (ngram)\nfrom sklearn.feature_extraction.text import CountVectorizer\ny = [\n    \"acetyl acetone\",\n    \"acetyl chloride\",\n    \"chloride hydroxide\",\n]\nvectorizer = CountVectorizer()\ny = vectorizer.fit_transform(y)\nprint (vectorizer.get_feature_names())\nprint (y.toarray())\n# \ud83d\udca1 Repeat above with char-level ngram vectorizer\n# vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3)) # uni, bi and trigrams\n</code></pre> <pre>\n['acetone', 'acetyl', 'chloride', 'hydroxide']\n[[1 1 0 0]\n [0 1 1 0]\n [0 0 1 1]]\n</pre></p> </li> <li> <p>similarity: similar to count vectorization but based on similarities in tokens</p> </li> <li>and many more!</li> </ul> <p>We'll often was to retrieve feature values for an entity (user, item, etc.) over time and reuse the same features across different projects. To ensure that we're retrieving the proper feature values and to avoid duplication of efforts, we can use a feature store.</p> <p>Curse of dimensionality</p> <p>What can we do if a feature has lots of unique values but enough data points for each unique value (ex. URL as a feature)?</p> Show answer <p>We can encode our data with hashing or using it's attributes instead of the exact entity itself. For example, representing a user by their location and favorites as opposed to using their user ID or representing a webpage with it's domain as opposed to the exact url. This methods effectively decrease the total number of unique feature values and increase the number of data points for each.</p>"},{"location":"courses/mlops/preprocessing/#implementation","title":"Implementation","text":"<p>For our application, we'll be implementing a few of these preprocessing steps that are relevant for our dataset.</p> <pre><code>import json\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\n</code></pre>"},{"location":"courses/mlops/preprocessing/#feature-engineering_1","title":"Feature engineering","text":"<p>We can combine existing input features to create new meaningful signal for helping the model learn. However, there's usually no simple way to know if certain feature combinations will help or not without empirically experimenting with the different combinations. Here, we could use a project's title and description separately as features but we'll combine them to create one input feature.</p> <pre><code># Input\ndf[\"text\"] = df.title + \" \" + df.description\n</code></pre>"},{"location":"courses/mlops/preprocessing/#cleaning_1","title":"Cleaning","text":"<p>Since we're dealing with text data, we can apply some common text preprocessing operations. Here, we'll be using Python's built-in regular expressions library <code>re</code> and the Natural Language Toolkit <code>nltk</code>.</p> <pre><code>nltk.download(\"stopwords\")\nSTOPWORDS = stopwords.words(\"english\")\n</code></pre> <pre><code>def clean_text(text, stopwords=STOPWORDS):\n\"\"\"Clean raw text string.\"\"\"\n    # Lower\n    text = text.lower()\n\n    # Remove stopwords\n    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n    text = pattern.sub('', text)\n\n    # Spacing and filters\n    text = re.sub(r\"([!\\\"'#$%&amp;()*\\+,-./:;&lt;=&gt;?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)  # add spacing\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n    text = text.strip()  # strip white space at the ends\n    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n\n    return text\n</code></pre> <p>Note</p> <p>We could definitely try and include emojis, punctuations, etc. because they do have a lot of signal for the task but it's best to simplify the initial feature set we use to just what we think are the most influential and then we can slowly introduce other features and assess utility.</p> <p>Once we're defined our function, we can apply it to each row in our dataframe via <code>pandas.DataFrame.apply</code>.</p> <pre><code># Apply to dataframe\noriginal_df = df.copy()\ndf.text = df.text.apply(clean_text)\nprint (f\"{original_df.text.values[0]}\\n{df.text.values[0]}\")\n</code></pre> <pre>\nComparison between YOLO and RCNN on real world videos Bringing theory to experiment is cool. We can easily train models in colab and find the results in minutes.\ncomparison yolo rcnn real world videos bringing theory experiment cool easily train models colab find results minutes\n</pre> <p>Warning</p> <p>We'll want to introduce less frequent features as they become more frequent or encode them in a clever way (ex. binning, extract general attributes, common n-grams, mean encoding using other feature values, etc.) so that we can mitigate the feature value dimensionality issue until we're able to collect more data.</p> <p>We'll wrap up our cleaning operation by removing columns (<code>pandas.DataFrame.drop</code>) and rows with null tag values (<code>pandas.DataFrame.dropna</code>).</p> <pre><code># DataFrame cleanup\ndf = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\")  # drop cols\ndf = df.dropna(subset=[\"tag\"])  # drop nulls\ndf = df[[\"text\", \"tag\"]]  # rearrange cols\ndf.head()\n</code></pre> text tag 0 comparison yolo rcnn real world videos bringin... computer-vision 1 show infer tell contextual inference creative ... computer-vision 2 awesome graph classification collection import... other 3 awesome monte carlo tree search curated list m... other 4 attentionwalk pytorch implementation watch ste... other"},{"location":"courses/mlops/preprocessing/#encoding_1","title":"Encoding","text":"<p>We need to encode our data into numerical values so that our models can process them. We'll start by encoding our text labels into unique indices.</p> <pre><code># Label to index\ntags = train_df.tag.unique().tolist()\nnum_classes = len(tags)\nclass_to_index = {tag: i for i, tag in enumerate(tags)}\nclass_to_index\n</code></pre> <pre>\n{'mlops': 0,\n 'natural-language-processing': 1,\n 'computer-vision': 2,\n 'other': 3}\n</pre> <p>Next, we can use the <code>pandas.Series.map</code> function to map our <code>class_to_index</code> dictionary on our tag column to encode our labels.</p> <pre><code># Encode labels\ndf[\"tag\"] = df[\"tag\"].map(class_to_index)\ndf.head()\n</code></pre> text tag 0 comparison yolo rcnn real world videos bringin... 2 1 show infer tell contextual inference creative ... 2 2 awesome graph classification collection import... 3 3 awesome monte carlo tree search curated list m... 3 4 attentionwalk pytorch implementation watch ste... 3 <p>We'll also want to be able to decode our predictions back into text labels. We can do this by creating an <code>index_to_class</code> dictionary and using that to convert encoded labels back into text labels.</p> <pre><code>def decode(indices, index_to_class):\n    return [index_to_class[index] for index in indices]\n</code></pre> <pre><code>index_to_class = {v:k for k, v in class_to_index.items()}\ndecode(df.head()[\"tag\"].values, index_to_class=index_to_class)\n</code></pre> <pre>\n['computer-vision', 'computer-vision', 'other', 'other', 'other']\n</pre>"},{"location":"courses/mlops/preprocessing/#tokenizer","title":"Tokenizer","text":"<p>Next we'll encode our text as well. Instead of using a random dictionary, we'll use a tokenizer that was used for a pretrained LLM (scibert) to tokenize our text. We'll be fine-tuning this exact model later when we train our model.</p> <p>Here is a quick refresher on attention and Transformers.</p> <pre><code>import numpy as np\nfrom transformers import BertTokenizer\n</code></pre> <p>The tokenizer will convert our input text into a list of token ids and a list of attention masks. The token ids are the indices of the tokens in the vocabulary. The attention mask is a binary mask indicating the position of the token indices so that the model can attend to them (and ignore the pad tokens).</p> <pre><code># Bert tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\ntext = \"Transfer learning with transformers for text classification.\"\nencoded_inputs = tokenizer([text], return_tensors=\"np\", padding=\"longest\")  # pad to longest item in batch\nprint (\"input_ids:\", encoded_inputs[\"input_ids\"])\nprint (\"attention_mask:\", encoded_inputs[\"attention_mask\"])\nprint (tokenizer.decode(encoded_inputs[\"input_ids\"][0]))\n</code></pre> <pre>\ninput_ids: [[  102  2268  1904   190 29155   168  3267  2998   205   103]]\nattention_mask: [[1 1 1 1 1 1 1 1 1 1]]\n[CLS] transfer learning with transformers for text classification. [SEP]\n</pre> <p>Note that we use <code>padding=\"longest\"</code> in our tokenizer function to pad our inputs to the longest item in the batch. This becomes important when we use batches of inputs later and want to create a uniform input size, where shorted text sequences will be padded with zeros to meet the length of the longest input in the batch.</p> <p>We'll wrap our tokenization into a <code>tokenize</code> function that we can use to tokenize batches of our data.</p> <pre><code>def tokenize(batch):\n    tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n    encoded_inputs = tokenizer(batch[\"text\"].tolist(), return_tensors=\"np\", padding=\"longest\")\n    return dict(ids=encoded_inputs[\"input_ids\"], masks=encoded_inputs[\"attention_mask\"], targets=np.array(batch[\"tag\"]))\n</code></pre> <pre><code># Tokenization\ntokenize(df.head(1))\n</code></pre> <pre>\n{'ids': array([[  102,  2029,  1778,   609,  6446,  4857,  1332,  2399, 13572,\n         19125,  1983,  1954,  6240,  3717,  7434,  1262,   537,   201,\n          1040,   545,  4714,   103]]),\n 'masks': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n 'targets': array([2])}\n</pre>"},{"location":"courses/mlops/preprocessing/#best-practices","title":"Best practices","text":"<p>We'll wrap up by combining all of our preprocessing operations into function. This way we can easily apply it to different datasets (training, inference, etc.)</p> <pre><code>def preprocess(df, class_to_index):\n\"\"\"Preprocess the data.\"\"\"\n    df[\"text\"] = df.title + \" \" + df.description  # feature engineering\n    df[\"text\"] = df.text.apply(clean_text)  # clean text\n    df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\")  # clean dataframe\n    df = df[[\"text\", \"tag\"]]  # rearrange columns\n    df[\"tag\"] = df[\"tag\"].map(class_to_index)  # label encoding\n    outputs = tokenize(df)\n    return outputs\n</code></pre> <pre><code># Apply\npreprocess(df=train_df, class_to_index=class_to_index)\n</code></pre> <pre>\n{'ids': array([[  102,   856,   532, ...,     0,     0,     0],\n        [  102,  2177, 29155, ...,     0,     0,     0],\n        [  102,  2180,  3241, ...,     0,     0,     0],\n        ...,\n        [  102,   453,  2068, ...,  5730,   432,   103],\n        [  102, 11268,  1782, ...,     0,     0,     0],\n        [  102,  1596,   122, ...,     0,     0,     0]]),\n 'masks': array([[1, 1, 1, ..., 0, 0, 0],\n        [1, 1, 1, ..., 0, 0, 0],\n        [1, 1, 1, ..., 0, 0, 0],\n        ...,\n        [1, 1, 1, ..., 1, 1, 1],\n        [1, 1, 1, ..., 0, 0, 0],\n        [1, 1, 1, ..., 0, 0, 0]]),\n 'targets': array([0, 1, 1, ... 0, 2, 3])}\n</pre> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Preprocessing - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/product-design/","title":"Machine Learning Product Design","text":""},{"location":"courses/mlops/product-design/#overview","title":"Overview","text":"<p>Before we start developing any machine learning models, we need to first motivate and design our application. While this is a technical course, this initial product design process is extremely crucial for creating great products. We'll focus on the product design aspects of our application in this lesson and the systems design aspects in the next lesson.</p>"},{"location":"courses/mlops/product-design/#template","title":"Template","text":"<p>The template below is designed to guide machine learning product development. It involves both the product and systems design (next lesson) aspects of our application:</p> <p>Product design (What &amp; Why) \u2192 Systems design (How)</p> <p>\ud83d\udc49 \u00a0 Download a PDF of the ML canvas to use for your own products \u2192 ml-canvas.pdf (right click the link and hit \"Save Link As...\")</p>"},{"location":"courses/mlops/product-design/#product-design","title":"Product design","text":"<p>Motivate the need for the product and outline the objectives and impact.</p> <p>Note</p> <p>Each section below has a part called \"Our task\", which will discuss how the specific topic relates to the application that we will be building.</p>"},{"location":"courses/mlops/product-design/#background","title":"Background","text":"<p>Set the scene for what we're trying to do through a user-centric approach:</p> <ul> <li><code>users</code>: profile/persona of our users</li> <li><code>goals</code>: our users' main goals</li> <li><code>pains</code>: obstacles preventing our users from achieving their goals</li> </ul> <p>Our task</p> <ul> <li><code>users</code>: machine learning developers and researchers.</li> <li><code>goals</code>: stay up-to-date on ML content for work, knowledge, etc.</li> <li><code>pains</code>: too much unlabeled content scattered around the internet.</li> </ul>"},{"location":"courses/mlops/product-design/#value-proposition","title":"Value proposition","text":"<p>Propose the value we can create through a product-centric approach:</p> <ul> <li><code>product</code>: what needs to be built to help our users reach their goals?</li> <li><code>alleviates</code>: how will the product reduce pains?</li> <li><code>advantages</code>: how will the product create gains?</li> </ul> <p>Our task</p> <p>We will build a platform that helps machine learning developers and researchers stay up-to-date on ML content. We'll do this by discovering and categorizing content from popular sources (Reddit, Twitter, etc.) and displaying it on our platform. For simplicity, assume that we already have a pipeline that delivers ML content from popular sources to our platform. We will just focus on developing the ML service that can correctly categorize the content.</p> <ul> <li><code>product</code>: a service that discovers and categorizes ML content from popular sources.</li> <li><code>alleviates</code>: display categorized content for users to discover.</li> <li><code>advantages</code>: when users visit our platform to stay up-to-date on ML content, they don't waste time searching for that content themselves in the noisy internet.</li> </ul> <p> </p>"},{"location":"courses/mlops/product-design/#objectives","title":"Objectives","text":"<p>Breakdown the product into key objectives that we want to focus on.</p> <p>Our task</p> <ul> <li>Discover ML content from trusted sources to bring into our platform.</li> <li>Classify incoming content for our users to easily discover. [OUR FOCUS]</li> <li>Display categorized content on our platform (recent, popular, recommended, etc.)</li> </ul>"},{"location":"courses/mlops/product-design/#solution","title":"Solution","text":"<p>Describe the solution required to meet our objectives, including its:</p> <ul> <li><code>core features</code>: key features that will be developed.</li> <li><code>integration</code>: how the product will integrate with other services.</li> <li><code>alternatives</code>: alternative solutions that we should considered.</li> <li><code>constraints</code>: limitations that we need to be aware of.</li> <li><code>out-of-scope.</code>: features that we will not be developing for now.</li> </ul> <p>Our task</p> <p>Develop a model that can classify the content so that it can be organized by category (tag) on our platform.</p> <p><code>Core features</code>:</p> <ul> <li>predict the correct tag for a given content. [OUR FOCUS]</li> <li>user feedback process for incorrectly classified content.</li> <li>workflows to categorize ML content that our model is incorrect / unsure about.</li> </ul> <p><code>Integrations</code>:</p> <ul> <li>ML content from reliable sources will be sent to our service for classification.</li> </ul> <p><code>Alternatives</code>:</p> <ul> <li>allow users to add content manually and classify them (noisy, cold start, etc.)</li> </ul> <p><code>Constraints</code>:</p> <ul> <li>maintain low latency (&gt;100ms) when classifying incoming content. [Latency]</li> <li>only recommend tags from our list of approved tags. [Security]</li> <li>avoid duplicate content from being added to the platform. [UI/UX]</li> </ul> <p><code>Out-of-scope</code>:</p> <ul> <li>identify relevant tags beyond our approved list of tags (<code>natural-language-processing</code>, <code>computer-vision</code>, <code>mlops</code> and <code>other</code>).</li> <li>using full-text HTML from content links to aid in classification.</li> </ul>"},{"location":"courses/mlops/product-design/#feasibility","title":"Feasibility","text":"<p>How feasible is our solution and do we have the required resources to deliver it (data, $, team, etc.)?</p> <p>Our task</p> <p>We have a dataset with ML content that has been labeled. We'll need to assess if it has the necessary signals to meet our objectives.</p> Sample data point<pre><code>{\n\"id\": 443,\n\"created_on\": \"2020-04-10 17:51:39\",\n\"title\": \"AllenNLP Interpret\",\n\"description\": \"A Framework for Explaining Predictions of NLP Models\",\n\"tag\": \"natural-language-processing\"\n}\n</code></pre> <p>Now that we've set up the product design requirements for our ML service, let's move on to the systems design requirements in the next lesson.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Product - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/scripting/","title":"Moving from Notebooks to Scripts","text":""},{"location":"courses/mlops/scripting/#intuition","title":"Intuition","text":"<p>In this lesson, we'll discuss how to migrate and organize code from our notebook to Python scripts. We'll be using VSCode in this course, but feel free to use any editor you feel comfortable with.</p> <p>Notebooks have been great so far for development. They're interactive, stateful (don't have to rerun code), and allow us to visualize outputs. However, when we want to a develop quality codebase, we need to move to scripts. Here are some reasons why:</p> <ul> <li> <p>stateless: when we run code in a notebook, it's automatically saved to the global state (memory). This is great for experimentation because code and variables will be readily available across different cells. However, this can be very problematic as well because there can be hidden state that we're not aware of. Scripts, on the other hand, are stateless and we have to explicitly pass variables to functions and classes.</p> </li> <li> <p>linear: in notebooks, the order in which we execute cells matter. This can be problematic because we can easily execute cells out of order. Scripts, on the other hand, are linear and we have to explicitly execute code for each workload.</p> </li> <li> <p>testing: As we'll see in our testing lesson, it's significantly easier to compose and run tests on scripts, as opposed to Jupyter notebooks. This is crucial for ensuring that we have quality code that works as expected.</p> </li> </ul>"},{"location":"courses/mlops/scripting/#setup","title":"Setup","text":"<p>We already have all the scripts provided in our repository so let's discuss how this was all organized.</p>"},{"location":"courses/mlops/scripting/#readme","title":"README","text":"<p>It's always a good idea to start organizing our scripts with a <code>README.md</code> file. This is where we can organize all of the instructions necessary to walkthrough our codebase. Our README has information on how to set up our environment, how to run our scripts, etc.</p> <p>The contents of the <code>README.md</code> file is what everyone will see when they visit your repository on GitHub. So, it's a good idea to keep it updated with the latest information.</p>"},{"location":"courses/mlops/scripting/#scripts","title":"Scripts","text":"<p>Let's start by moving our code from notebooks to scripts. We're going to start by creating the different files and directories that we'll need for our project. The exact number and name of these scripts is entirely up to us, however, it's best to organize and choose names that relate to a specific workload. For example, <code>data.py</code> will have all of our data related functions and classes. And we can also have scripts for configurations (<code>config.py</code>), shared utilities (<code>utils.py</code>), etc.</p> <pre><code>madewithml/\n\u251c\u2500\u2500 config.py\n\u251c\u2500\u2500 data.py\n\u251c\u2500\u2500 evaluate.py\n\u251c\u2500\u2500 models.py\n\u251c\u2500\u2500 predict.py\n\u251c\u2500\u2500 serve.py\n\u251c\u2500\u2500 train.py\n\u251c\u2500\u2500 tune.py\n\u2514\u2500\u2500 utils.py\n</code></pre> <p>Don't worry about the contents in these files that aren't from our notebooks just yet or if our code looks significantly more documented. We'll be taking a closer look at those in the respective lessons.</p>"},{"location":"courses/mlops/scripting/#functions-and-classes","title":"Functions and classes","text":"<p>Once we have these ready, we can start moving code from our notebooks to the appropriate scripts. It should intuitive in which script a particular function or class belongs to. If not, we need to rethink how the names of our scripts. For example, <code>train.py</code> has functions from our notebook such as <code>train_step</code>, <code>val_step</code>, <code>train_loop_per_worker</code>, etc.</p> <pre><code># madewithml/train.py\ndef train_step(...):\n    pass\n\ndef val_step(...):\n    pass\n\ndef train_loop_per_worker(...):\n    pass\n\n...\n</code></pre>"},{"location":"courses/mlops/scripting/#workloads","title":"Workloads","text":"<p>Recall that for training a model, we wrote code in our notebook for setting configurations, training, etc. that was freeform in a code cell:</p> <pre><code># Scaling config\nscaling_config = ScalingConfig(\n    num_workers=num_workers,\n    use_gpu=bool(resources_per_worker[\"GPU\"]),\n    resources_per_worker=resources_per_worker,\n    _max_cpu_fraction_per_node=0.8,\n)\n\n# Checkpoint config\ncheckpoint_config = CheckpointConfig(\n    num_to_keep=1,\n    checkpoint_score_attribute=\"val_loss\",\n    checkpoint_score_order=\"min\",\n)\n\n...\n</code></pre> <p>These code cells are not part of a function or class, so we need to wrap them around a function so that we can easily execute that workload. For example, all of this training logic is wrapped inside a <code>train_model</code> function in <code>train.py</code> that has all the required inputs to execute the workload:</p> <pre><code># madewithml/train.py\ndef train_model(experiment_name, dataset_loc, ...):\n    ...\n\n    # Scaling config\n    scaling_config = ScalingConfig(\n        num_workers=num_workers,\n        use_gpu=bool(gpu_per_worker),\n        resources_per_worker={\"CPU\": cpu_per_worker, \"GPU\": gpu_per_worker},\n        _max_cpu_fraction_per_node=0.8,\n    )\n\n    # Checkpoint config\n    checkpoint_config = CheckpointConfig(\n        num_to_keep=1,\n        checkpoint_score_attribute=\"val_loss\",\n        checkpoint_score_order=\"min\",\n    )\n\n    ...\n</code></pre> <p>In the next lesson on command-line interfaces (CLI), we'll learn how to execute these main workloads in our scripts from the command line.</p>"},{"location":"courses/mlops/scripting/#config","title":"Config","text":"<p>In addition to our core workload scripts, recall that we also have a <code>config.py</code> script. This file will include all of the setup and configuration that all/most of our workloads depend on. For example, setting up our model registry:</p> <pre><code># Config MLflow\nMODEL_REGISTRY = Path(\"/tmp/mlflow\")\nPath(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\nMLFLOW_TRACKING_URI = \"file://\" + str(MODEL_REGISTRY.absolute())\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n</code></pre> <p>We wouldn't have configurations like our <code>ScalingConfig</code> here because that's specific to our training workload. The <code>config.py</code> script is for configurations that are shared across different workloads.</p>"},{"location":"courses/mlops/scripting/#utilities","title":"Utilities","text":"<p>Similarly, we also have a <code>utils.py</code> script to include components that will be reused across different scripts. It's a good idea to organize these shared components here as opposed to the core scripts to avoid circular dependency conflicts (two scripts call on functions from each other). Here is an example of one of our utility functions, <code>set_seeds</code>, that's used in both our <code>train.py</code> and <code>tune.py</code> scripts.</p> <pre><code>def set_seeds(seed: int = 42):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    eval(\"setattr(torch.backends.cudnn, 'deterministic', True)\")\n    eval(\"setattr(torch.backends.cudnn, 'benchmark', False)\")\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n</code></pre>"},{"location":"courses/mlops/scripting/#ray","title":"Ray","text":"<p>Recall in our setup lesson that we initialized Ray inside our notebooks. We still need to initialize Ray before executing our ML workloads via scripts but we can decide to do this only for the scripts with Ray dependent workloads. For example, at the bottom of our <code>train.py</code> script, we have:</p> <pre><code># madewithml/train.py\nif __name__ == \"__main__\":\n    if ray.is_initialized():\n        ray.shutdown()\n    ray.init()\n    app()  # initialize Typer app\n</code></pre> <p>Now that we've set up our scripts, we can start executing them from the command line. In the next lesson, we'll learn how to do this with command-line interfaces (CLI).</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Scripting - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/serving/","title":"Model Serving","text":""},{"location":"courses/mlops/serving/#intuition","title":"Intuition","text":"<p>In this lesson, we're going to serve the machine learning models that we have developed so that we can use them to make predictions on unseen data. And we want to be able to serve our models in a scalable and robust manner so it can deliver high throughput (handle many requests) and low latency (quickly respond to each request). In an effort to be comprehensive, we will implement both batch inference (offline) and online inference (real-time), though we will focus on the latter in the remaining lessons as it's more appropriate for our application.</p>"},{"location":"courses/mlops/serving/#frameworks","title":"Frameworks","text":"<p>There are many frameworks to choose from when it comes to model serving, such as Ray Serve, Nvidia Triton, HuggingFace, Bento ML, etc. When choosing between these frameworks, we want to choose the option that will allow us to:</p> <ul> <li>Pythonic: we don't want to learn a new framework to be able to serve our models.</li> <li>framework agnostic: we want to be able to serve models from all frameworks (PyTorch, TensorFlow, etc.)</li> <li>scale: (auto)scaling our service should be as easy as changing a configuration.</li> <li>composition: combine multiple models and business logic into our service.</li> <li>integrations: integrate with popular API frameworks like FastAPI.</li> </ul> <p>To address all of these requirements (and more), we will be using Ray Serve to create our service. While we'll be specifically using it's integration with FastAPI, there are many other integrations you might want to explore based on your stack (LangChain, Kubernetes, etc.).</p>"},{"location":"courses/mlops/serving/#batch-inference","title":"Batch inference","text":"<p>We will first implement batch inference (or offline inference), which is when we make predictions on a large batch of data. This is useful when we don't need to serve a model's prediction on input data as soon as the input data is received. For example, our service can be used to make predictions once at the end of every day on the batches of content collected throughout the day. This can be more efficient than making predictions on each content individually if we don't need that kind of low latency.</p> <p>Let's take a look at our how we can easily implement batch inference with Ray Serve. We'll start with some setup and load the best checkpoint from our training run.</p> <p><pre><code>import ray.data\nfrom ray.train.torch import TorchPredictor\nfrom ray.data import ActorPoolStrategy\n</code></pre> <pre><code># Load predictor\nrun_id = sorted_runs.iloc[0].run_id\nbest_checkpoint = get_best_checkpoint(run_id=run_id)\n</code></pre></p> <p>Next, we'll define a <code>Predictor</code> class that will load the model from our checkpoint and then define the <code>__call__</code> method that will be used to make predictions on our input data.</p> <pre><code>class Predictor:\n    def __init__(self, checkpoint):\n        self.predictor = TorchPredictor.from_checkpoint(checkpoint)\n    def __call__(self, batch):\n        z = self.predictor.predict(batch)[\"predictions\"]\n        y_pred = np.stack(z).argmax(1)\n        prediction = decode(y_pred, preprocessor.index_to_class)\n        return {\"prediction\": prediction}\n</code></pre> <p>The <code>__call__</code> function in Python defines the logic that will be executed when our object is called like a function. <pre><code>predictor = Predictor()\nprediction = predictor(batch)\n</code></pre></p> <p>To do batch inference, we'll be using the <code>map_batches</code> functionality. We previously used <code>map_batches</code> to <code>map</code> (or apply) a preprocessing function across <code>batches</code> (chunks) of our data. We're now using the same concept to apply our predictor across batches of our inference data.</p> <pre><code># Batch predict\npredictions = test_ds.map_batches(\n    Predictor,\n    batch_size=128,\n    compute=ActorPoolStrategy(min_size=1, max_size=2),  # scaling\n    batch_format=\"pandas\",\n    fn_constructor_kwargs={\"checkpoint\": best_checkpoint})\n</code></pre> <p>Note that <code>best_checkpoint</code> as a keyword argument to our <code>Predictor</code> class so that we can load the model from that checkpoint. We can pass this in via the <code>fn_constructor_kwargs</code> argument in our <code>map_batches</code> function.</p> <pre><code># Sample predictions\npredictions.take(3)\n</code></pre> <pre>\n[{'prediction': 'computer-vision'},\n {'prediction': 'other'},\n {'prediction': 'other'}]\n</pre>"},{"location":"courses/mlops/serving/#online-inference","title":"Online inference","text":"<p>While we can achieve batch inference at scale, many models will need to be served in an real-time manner where we may need to deliver predictions for many incoming requests (high throughput) with low latency. We want to use online inference for our application over batch inference because we want to quickly categorize content as they are received/submitted to our platform so that the community can discover them quickly.</p> <pre><code>from fastapi import FastAPI\nfrom ray import serve\nimport requests\nfrom starlette.requests import Request\n</code></pre> <p>We'll start by defining our FastAPI application which involves initializing a predictor (and preprocessor) from the best checkpoint for a particular run (specified by <code>run_id</code>). We'll also define a <code>predict</code> function that will be used to make predictions on our input data.</p> <pre><code># Define application\napp = FastAPI(\n    title=\"Made With ML\",\n    description=\"Classify machine learning projects.\",\n    version=\"0.1\")\n</code></pre> <pre><code>class ModelDeployment:\n\n    def __init__(self, run_id):\n\"\"\"Initialize the model.\"\"\"\n        self.run_id = run_id\n        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)  # so workers have access to model registry\n        best_checkpoint = get_best_checkpoint(run_id=run_id)\n        self.predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n        self.preprocessor = self.predictor.get_preprocessor()\n\n    @app.post(\"/predict/\")\n    async def _predict(self, request: Request):\n        data = await request.json()\n        df = pd.DataFrame([{\"title\": data.get(\"title\", \"\"), \"description\": data.get(\"description\", \"\"), \"tag\": \"\"}])\n        results = predict_with_proba(df=df, predictor=self.predictor)\n        return {\"results\": results}\n</code></pre> <p><code>async def</code> refers to an asynchronous function (when we call the function we don't have to wait for the function to complete executing). The <code>await</code> keyword is used inside an asynchronous function to wait for the completion of the <code>request.json()</code> operation.</p> <p>We can now combine our FastAPI application with Ray Serve by simply wrapping our application with the <code>serve.ingress</code> decorator. We can further wrap all of this with the <code>serve.deployment</code> decorator to define our deployment configuration (ex. number of replicas, compute resources, etc.). These configurations allow us to easily scale our service as needed.</p> <pre><code>@serve.deployment(route_prefix=\"/\", num_replicas=\"1\", ray_actor_options={\"num_cpus\": 8, \"num_gpus\": 0})\n@serve.ingress(app)\nclass ModelDeployment:\n    pass\n</code></pre> <p>Now let's run our service and perform some real-time inference.</p> <pre><code># Run service\nsorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\nrun_id = sorted_runs.iloc[0].run_id\nserve.run(ModelDeployment.bind(run_id=run_id))\n</code></pre> <pre>\nStarted detached Serve instance in namespace \"serve\".\nDeployment 'default_ModelDeployment:IcuFap' is ready at `http://127.0.0.1:8000/`. component=serve deployment=default_ModelDeployment\nRayServeSyncHandle(deployment='default_ModelDeployment')\n</pre> <pre><code># Query\ntitle = \"Transfer learning with transformers\"\ndescription = \"Using transformers for transfer learning on text classification tasks.\"\njson_data = json.dumps({\"title\": title, \"description\": description})\nrequests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()\n</code></pre> <pre>\n{'results': [{'prediction': 'natural-language-processing',\n   'probabilities': {'computer-vision': 0.00038025027606636286,\n    'mlops': 0.0003820903366431594,\n    'natural-language-processing': 0.9987919926643372,\n    'other': 0.00044562897528521717}}]}\n</pre> <p>The issue with neural networks (and especially LLMs) is that they are notoriously overconfident. For every input, they will always make some prediction. And to account for this, we have an <code>other</code> class but that class only has projects that are not in our accepted tags but are still machine learning related nonetheless. Here's what happens when we input complete noise as our input:</p> <pre><code># Query (noise)\ntitle = \" 65n7r5675\"  # random noise\njson_data = json.dumps({\"title\": title, \"description\": \"\"})\nrequests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()\n</code></pre> <pre>\n{'results': [{'prediction': 'natural-language-processing',\n   'probabilities': {'computer-vision': 0.11885979026556015,\n    'mlops': 0.09778415411710739,\n    'natural-language-processing': 0.6735526323318481,\n    'other': 0.1098034456372261}}]}\n</pre> <p>Let's shutdown our service before we fixed this issue.</p> <pre><code># Shutdown\nserve.shutdown()\n</code></pre>"},{"location":"courses/mlops/serving/#custom-logic","title":"Custom logic","text":"<p>To make our service a bit more robust, let's add some custom logic to predict the <code>other</code> class if the probability of the predicted class is below a certain <code>threshold</code> probability.</p> <pre><code>@serve.deployment(route_prefix=\"/\", num_replicas=\"1\", ray_actor_options={\"num_cpus\": 8, \"num_gpus\": 0})\n@serve.ingress(app)\nclass ModelDeploymentRobust:\n\n    def __init__(self, run_id, threshold=0.9):\n\"\"\"Initialize the model.\"\"\"\n        self.run_id = run_id\n        self.threshold = threshold\n        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)  # so workers have access to model registry\n        best_checkpoint = get_best_checkpoint(run_id=run_id)\n        self.predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n        self.preprocessor = self.predictor.get_preprocessor()\n\n    @app.post(\"/predict/\")\n    async def _predict(self, request: Request):\n        data = await request.json()\n        df = pd.DataFrame([{\"title\": data.get(\"title\", \"\"), \"description\": data.get(\"description\", \"\"), \"tag\": \"\"}])\n        results = predict_with_proba(df=df, predictor=self.predictor)\n\n        # Apply custom logic\n        for i, result in enumerate(results):\n            pred = result[\"prediction\"]\n            prob = result[\"probabilities\"]\n            if prob[pred] &lt; self.threshold:\n                results[i][\"prediction\"] = \"other\"\n\n        return {\"results\": results}\n</code></pre> <p>Tip</p> <p>It's easier to incorporate custom logic instead of altering the model itself. This way, we won't have to collect new data. change the model's architecture or retrain it. This also makes it really easy to change the custom logic as our product specifications may change (clean separation of product and machine learning).</p> <pre><code># Run service\nserve.run(ModelDeploymentRobust.bind(run_id=run_id, threshold=0.9))\n</code></pre> <pre>\nStarted detached Serve instance in namespace \"serve\".\nDeployment 'default_ModelDeploymentRobust:RTbrNg' is ready at `http://127.0.0.1:8000/`. component=serve deployment=default_ModelDeploymentRobust\nRayServeSyncHandle(deployment='default_ModelDeploymentRobust')\n</pre> <p>Now let's see how we perform on the same random noise with our custom logic incorporate into the service.</p> <pre><code># Query (noise)\ntitle = \" 65n7r5675\"  # random noise\njson_data = json.dumps({\"title\": title, \"description\": \"\"})\nrequests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()\n</code></pre> <pre>\n{'results': [{'prediction': 'other',\n   'probabilities': {'computer-vision': 0.11885979026556015,\n    'mlops': 0.09778415411710739,\n    'natural-language-processing': 0.6735526323318481,\n    'other': 0.1098034456372261}}]}\n</pre> <pre><code># Shutdown\nserve.shutdown()\n</code></pre> <p>We'll learn how to deploy our service to production in our Jobs and Services lesson a bit later.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Serving - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/setup/","title":"Setup","text":"<p>In this lesson, we'll setup the development environment that we'll be using in all of our lessons. We'll have instructions for both local laptop and remote scalable clusters (Anyscale). While everything will work locally on your laptop, you can sign up to join one of our upcoming live cohorts where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day \u2192 sign up here.</p>"},{"location":"courses/mlops/setup/#cluster","title":"Cluster","text":"<p>We'll start with defining our cluster, which refers to a group of servers that come together to form one system. Our clusters will have a head node that manages the cluster and it will be connected to a set of worker nodes that will execute workloads for us. These clusters can be fixed in size or autoscale based on our application's compute needs, which makes them highly scalable and performant. We'll create our cluster by defining a compute configuration and an environment.</p>"},{"location":"courses/mlops/setup/#environment","title":"Environment","text":"<p>We'll start by defining our cluster environment which will specify the software dependencies that we'll need for our workloads.</p> <p>\ud83d\udcbb Local</p> <p>Your personal laptop will need to have Python installed and we highly recommend using Python <code>3.10</code>. You can use a tool like pyenv (mac) or pyenv-win (windows) to easily download and switch between Python versions.</p> <pre><code>pyenv install 3.10.11  # install\npyenv global 3.10.11  # set default\n</code></pre> <p>Once we have our Python version, we can create a virtual environment to install our dependencies. We'll download our Python dependencies after we clone our repository from git shortly.</p> <pre><code>mkdir madewithml\ncd madewithml\npython3 -m venv venv  # create virtual environment\nsource venv/bin/activate  # on Windows: venv\\Scripts\\activate\npython3 -m pip install --upgrade pip setuptools wheel\n</code></pre> <p>\ud83d\ude80 Anyscale</p> <p>Our cluster environment will be defined inside a <code>cluster_env.yaml</code> file. Here we specify some details around our base image (anyscale/ray:2.6.0-py310-cu118) that has our Python version, GPU dependencies, etc.</p> <pre><code>base_image: anyscale/ray:2.6.0-py310-cu118\nenv_vars: {}\ndebian_packages:\n- curl\n\npython:\npip_packages: []\nconda_packages: []\n\npost_build_cmds:\n- python3 -m pip install --upgrade pip setuptools wheel\n- python3 -m pip install -r https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/requirements.txt\n</code></pre> <p>We could specify any python packages inside <code>pip_packages</code> or <code>conda_packages</code> but we're going to use a <code>requirements.txt</code> file to load our dependencies under <code>post_build_cmds</code>.</p>"},{"location":"courses/mlops/setup/#compute","title":"Compute","text":"<p>Next, we'll define our compute configuration, which will specify our hardware dependencies (head and worker nodes) that we'll need for our workloads.</p> <p>\ud83d\udcbb Local</p> <p>Your personal laptop (single machine) will act as the cluster, where one CPU will be the head node and some of the remaining CPU will be the worker nodes (no GPUs required). All of the code in this course will work in any personal laptop though it will be slower than executing the same workloads on a larger cluster.</p> <p>\ud83d\ude80 Anyscale</p> <p>Our cluster compute will be defined inside a <code>cluster_compute.yaml</code> file. Here we specify some details around where our compute resources will come from (cloud computing platform like AWS), types of nodes and their counts, etc.</p> <pre><code>cloud: madewithml-us-east-2\nregion: us-east2\nhead_node_type:\nname: head_node_type\ninstance_type: m5.2xlarge  # 8 CPU, 0 GPU, 32 GB RAM\nworker_node_types:\n- name: gpu_worker\ninstance_type: g4dn.xlarge  # 4 CPU, 1 GPU, 16 GB RAM\nmin_workers: 0\nmax_workers: 1\n...\n</code></pre> <p>Our worker nodes will be GPU-enabled so we can train our models faster and we set <code>min_workers</code> to 0 so that we can autoscale these workers only when they're needed (up to a maximum of <code>max_workers</code>). This will help us significantly reduce our compute costs without having to manage the infrastructure ourselves.</p>"},{"location":"courses/mlops/setup/#workspaces","title":"Workspaces","text":"<p>With our compute and environment defined, we're ready to create our cluster workspace. This is where we'll be developing our ML application on top of our compute, environment and storage.</p> <p>\ud83d\udcbb Local</p> <p>Your personal laptop will need to have an interactive development environment (IDE) installed, such as VS Code. For bash commands in this course, you're welcome to use the terminal on VSCode or a separate one.</p> <p>\ud83d\ude80 Anyscale</p> <p>We're going to launch an Anyscale Workspace to do all of our development in. Workspaces allow us to use development tools such as VSCode, Jupyter notebooks, web terminal, etc. on top of our cluster compute, environment and storage. This create an \"infinite laptop\" experience that feels like a local laptop experience but on a powerful, scalable cluster.</p> <p> </p> <p>We have the option to create our Workspace using a CLI but we're going to create it using the web UI (you will receive the required credentials during the cohort). On the UI, we can fill in the following information:</p> <pre><code>- Workspace name: `madewithml`\n- Project: `madewithml`\n- Cluster environment name: `madewithml-cluster-env`\n# Toggle `Select from saved configurations`\n- Compute config: `madewithml-cluster-compute`\n- Click on the **Start** button to launch the Workspace\n</code></pre> <p> </p> <p>We have already saved created our Project, cluster environment and compute config so we can select them from the dropdowns but we could just as easily create new ones / update these using the CLI.</p> CLI method <pre><code># Set credentials\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from Anyscale credentials page\n\n# Create project\nexport PROJECT_NAME=\"madewithml\"\nanyscale project create --name $PROJECT_NAME\n\n# Cluster environment\nexport CLUSTER_ENV_NAME=\"madewithml-cluster-env\"\nanyscale cluster-env build deploy/cluster_env.yaml --name $CLUSTER_ENV_NAME\n\n# Compute config\nexport CLUSTER_COMPUTE_NAME=\"madewithml-cluster-compute\"\nanyscale cluster-compute create deploy/cluster_compute.yaml --name $CLUSTER_COMPUTE_NAME\n</code></pre>"},{"location":"courses/mlops/setup/#git","title":"Git","text":"<p>With our development workspace all set up, we're ready to start developing. We'll start by following these instructions to create a repository:</p> <ol> <li>Create a new repository</li> <li>name it <code>Made-With-ML</code></li> <li>Toggle <code>Add a README file</code> (very important as this creates a <code>main</code> branch)</li> <li>Scroll down and click <code>Create repository</code></li> </ol> <p>Now we're ready to clone the Made With ML repository's contents from GitHub inside our <code>madewithml</code> directory.</p> <pre><code>export GITHUB_USERNAME=\"YOUR_GITHUB_UESRNAME\"  # &lt;-- CHANGE THIS to your username\ngit clone https://github.com/GokuMohandas/Made-With-ML.git .\ngit remote set-url origin https://github.com/$GITHUB_USERNAME/Made-With-ML.git\ngit checkout -b dev\nexport PYTHONPATH=$PYTHONPATH:$PWD  # so we can import modules from our scripts\n</code></pre> <p>\ud83d\udcbb Local</p> <p>Recall that we created our virtual environment earlier but didn't actually load any Python dependencies yet. We'll clone our repository and then install the packages using the <code>requirements.txt</code> file.</p> <pre><code>python3 -m pip install -r requirements.txt\n</code></pre> <p>Caution: make sure that we're installing our Python packages inside our virtual environment.</p> <p>\ud83d\ude80 Anyscale</p> <p>Our environment with the appropriate Python version and libraries is already all set for us through the cluster environment we used when setting up our Anyscale Workspace. But if we want to install additional Python packages as we develop, we need to do pip install with the <code>--user</code> flag inside our Workspaces (via terminal) to ensure that our head and all worker nodes receive the package. And then we should also add it to our requirements file so it becomes part of the cluster environment build process next time.</p> <pre><code>pip install --user &lt;package_name&gt;:&lt;version&gt;\n</code></pre>"},{"location":"courses/mlops/setup/#notebook","title":"Notebook","text":"<p>Now we're ready to launch our Jupyter notebook to interactively develop our ML application.</p> <p>\ud83d\udcbb Local</p> <p>We already installed jupyter through our <code>requirements.txt</code> file in the previous step, so we can just launch it.</p> <pre><code>jupyter lab notebooks/madewithml.ipynb\n</code></pre> <p>\ud83d\ude80 Anyscale</p> <p>Click on the Jupyter icon \u00a0\u00a0 at the top right corner of our Anyscale Workspace page and this will open up our JupyterLab instance in a new tab. Then navigate to the <code>notebooks</code> directory and open up the <code>madewithml.ipynb</code> notebook.</p> <p> </p>"},{"location":"courses/mlops/setup/#ray","title":"Ray","text":"<p>We'll be using Ray to scale and productionize our ML application. Ray consists of a core distributed runtime along with libraries for scaling ML workloads and has companies like OpenAI, Spotify, Netflix, Instacart, Doordash + many more using it to develop their ML applications. We're going to start by initializing Ray inside our notebooks:</p> <pre><code>import ray\n</code></pre> <pre><code># Initialize Ray\nif ray.is_initialized():\n    ray.shutdown()\nray.init()\n</code></pre> <p>We can also view our cluster resources to view the available compute resources:</p> <pre><code>ray.cluster_resources()\n</code></pre> <p>\ud83d\udcbb Local</p> <p>If you are running this on a local laptop (no GPU), use the CPU count from <code>ray.cluster_resources()</code> to set your resources. For example if your machine has 10 CPUs:</p> <p><pre>\n{'CPU': 10.0,\n 'object_store_memory': 2147483648.0,\n 'node:127.0.0.1': 1.0}\n </pre></p> <pre><code>num_workers = 6  # prefer to do a few less than total available CPU (1 for head node + 1 for background tasks)\nresources_per_worker={\"CPU\": 1, \"GPU\": 0}\n</code></pre> <p>\ud83d\ude80 Anyscale</p> <p>On our Anyscale Workspace, the <code>ray.cluster_resources()</code> command will produce:</p> <p><pre>\n{'CPU': 8.0,\n'node:internal_head': 1.0,\n'node:10.0.56.150': 1.0,\n'memory': 34359738368.0,\n'object_store_memory': 9492578304.0}\n</pre></p> <p>These cluster resources only reflect our head node (1 m5.2xlarge). But recall earlier in our compute configuration that we also added g4dn.xlarge worker nodes (each has 1 GPU and 4 CPU) to our cluster. But because we set <code>min_workers=0</code>, our worker nodes will autoscale ( up to <code>max_workers</code>) as they're needed for specific workloads (ex. training). So we can set the # of workers and resources by worker based on this insight:</p> <pre><code># Workers (1 g4dn.xlarge)\nnum_workers = 1\nresources_per_worker={\"CPU\": 3, \"GPU\": 1}\n</code></pre> <p>Head on over to the next lesson, where we'll motivate the specific application that we're trying to build from a product and systems design perspective. And after that, we're ready to start developing!</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Setup - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/splitting/","title":"Splitting a Dataset for Machine Learning","text":""},{"location":"courses/mlops/splitting/#intuition","title":"Intuition","text":"<p>To determine the efficacy of our models, we need to have an unbiased measuring approach. To do this, we split our dataset into <code>training</code>, <code>validation</code>, and <code>testing</code> data splits.</p> <ol> <li>Use the training split to train the model. <p>Here the model will have access to both inputs and outputs to optimize its internal weights.</p> </li> <li>After each loop (epoch) of the training split, we will use the validation split to determine model performance. <p>Here the model will not use the outputs to optimize its weights but instead, we will use the performance to optimize training hyperparameters such as the learning rate, etc.</p> </li> <li>After training stops (epoch(s)), we will use the testing split to perform a one-time assessment of the model. <p>This is our best measure of how the model may behave on new, unseen data. Note that training stops when the performance improvement is not significant or any other stopping criteria that we may have specified.</p> </li> </ol> <p>Creating proper data splits</p> <p>What are the criteria we should focus on to ensure proper data splits?</p> Show answer <ul> <li>the dataset (and each data split) should be representative of data we will encounter</li> <li>equal distributions of output values across all splits</li> <li>shuffle your data if it's organized in a way that prevents input variance</li> <li>avoid random shuffles if your task can suffer from data leaks (ex. <code>time-series</code>)</li> </ul> <p>We need to clean our data first before splitting, at least for the features that splitting depends on. So the process is more like: preprocessing (global, cleaning) \u2192 splitting \u2192 preprocessing (local, transformations).</p>"},{"location":"courses/mlops/splitting/#naive-split","title":"Naive split","text":"<p>We'll start by splitting our dataset into three data splits for training, validation and testing.</p> <p><pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <pre><code># Split sizes\ntrain_size = 0.7\nval_size = 0.15\ntest_size = 0.15\n</code></pre></p> <p>For our multi-class task (each input has one label), we want to ensure that each data split has similar class distributions. We can achieve this by specifying how to stratify the split by adding the <code>stratify</code> keyword argument.</p> <p><pre><code># Split (train)\nX_train, X_, y_train, y_ = train_test_split(\n    X, y, train_size=train_size, stratify=y)\n</code></pre> <pre><code>print (f\"train: {len(X_train)} ({(len(X_train) / len(X)):.2f})\\n\"\n       f\"remaining: {len(X_)} ({(len(X_) / len(X)):.2f})\")\n</code></pre></p> <pre>\ntrain: 668 (0.70)\nremaining: 287 (0.30)\n</pre> <p><pre><code># Split (test)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_, y_, train_size=0.5, stratify=y_)\n</code></pre> <pre><code>print(f\"train: {len(X_train)} ({len(X_train)/len(X):.2f})\\n\"\n      f\"val: {len(X_val)} ({len(X_val)/len(X):.2f})\\n\"\n      f\"test: {len(X_test)} ({len(X_test)/len(X):.2f})\")\n</code></pre></p> <pre>\ntrain: 668 (0.70)\nval: 143 (0.15)\ntest: 144 (0.15)\n</pre> <p><pre><code># Get counts for each class\ncounts = {}\ncounts[\"train_counts\"] = {tag: label_encoder.decode(y_train).count(tag) for tag in label_encoder.classes}\ncounts[\"val_counts\"] = {tag: label_encoder.decode(y_val).count(tag) for tag in label_encoder.classes}\ncounts[\"test_counts\"] = {tag: label_encoder.decode(y_test).count(tag) for tag in label_encoder.classes}\n</code></pre> <pre><code># View distributions\npd.DataFrame({\n    \"train\": counts[\"train_counts\"],\n    \"val\": counts[\"val_counts\"],\n    \"test\": counts[\"test_counts\"]\n}).T.fillna(0)\n</code></pre></p> computer-vision mlops natural-language-processing other train 249 55 272 92 val 53 12 58 20 test 54 12 58 20 <p>It's hard to compare these because our train and test proportions are different. Let's see what the distribution looks like once we balance it out. What do we need to multiply our test ratio by so that we have the same amount as our train ratio?</p> \\[ \\alpha * N_{test} = N_{train} \\] \\[ \\alpha = \\frac{N_{train}}{N_{test}} \\] <p><pre><code># Adjust counts across splits\nfor k in counts[\"val_counts\"].keys():\n    counts[\"val_counts\"][k] = int(counts[\"val_counts\"][k] * \\\n        (train_size/val_size))\nfor k in counts[\"test_counts\"].keys():\n    counts[\"test_counts\"][k] = int(counts[\"test_counts\"][k] * \\\n        (train_size/test_size))\n</code></pre> <pre><code>dist_df = pd.DataFrame({\n    \"train\": counts[\"train_counts\"],\n    \"val\": counts[\"val_counts\"],\n    \"test\": counts[\"test_counts\"]\n}).T.fillna(0)\ndist_df\n</code></pre></p> computer-vision mlops natural-language-processing other train 249 55 272 92 val 247 56 270 93 test 252 56 270 93 <p>We can see how much deviance there is in our naive data splits by computing the standard deviation of each split's class counts from the mean (ideal split).</p> \\[ \\sigma = \\sqrt{\\frac{(x - \\bar{x})^2}{N}} \\] <pre><code># Standard deviation\nnp.mean(np.std(dist_df.to_numpy(), axis=0))\n</code></pre> <pre>\n0.9851056877051131\n</pre> <pre><code># Split DataFrames\ntrain_df = pd.DataFrame({\"text\": X_train, \"tag\": label_encoder.decode(y_train)})\nval_df = pd.DataFrame({\"text\": X_val, \"tag\": label_encoder.decode(y_val)})\ntest_df = pd.DataFrame({\"text\": X_test, \"tag\": label_encoder.decode(y_test)})\ntrain_df.head()\n</code></pre> text tags 0 laplacian pyramid reconstruction refinement se... computer-vision 1 extract stock sentiment news headlines project... natural-language-processing 2 big bad nlp database collection 400 nlp datasets... natural-language-processing 3 job classification job classification done usi... natural-language-processing 4 optimizing mobiledet mobile deployments learn ... computer-vision <p>Multi-label classification</p> <p>If we had a multi-label classification task, then we would've applied iterative stratification via the skmultilearn library, which essentially splits each input into subsets (where each label is considered individually) and then it distributes the samples starting with fewest \"positive\" samples and working up to the inputs that have the most labels.</p> <pre><code>from skmultilearn.model_selection import IterativeStratification\ndef iterative_train_test_split(X, y, train_size):\n\"\"\"Custom iterative train test split which\n    'maintains balanced representation with respect\n    to order-th label combinations.'\n    \"\"\"\n    stratifier = IterativeStratification(\n        n_splits=2, order=1, sample_distribution_per_fold=[1.0-train_size, train_size, ])\n    train_indices, test_indices = next(stratifier.split(X, y))\n    X_train, y_train = X[train_indices], y[train_indices]\n    X_test, y_test = X[test_indices], y[test_indices]\n    return X_train, X_test, y_train, y_test\n</code></pre> <p>Iterative stratification essentially creates splits while \"trying to maintain balanced representation with respect to order-th label combinations\". We used to an <code>order=1</code> for our iterative split which means we cared about providing representative distribution of each tag across the splits. But we can account for higher-order label relationships as well where we may care about the distribution of label combinations.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Splitting a Dataset for Machine Learning - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/styling/","title":"Styling and Formatting Code","text":""},{"location":"courses/mlops/styling/#intuition","title":"Intuition","text":"<p>Code is read more often than it is written. -- Guido Van Rossum (author of Python)</p> <p>When we write a piece of code, it's almost never the last time we see it or the last time it's edited. So we need to explain what's going on (via documentation) and make it easy to read. One of the easiest ways to make code more readable is to follow consistent style and formatting conventions. There are many options when it comes to Python style conventions to adhere to, but most are based on PEP8 conventions. Different teams follow different conventions and that's perfectly alright. The most important aspects are:</p> <ul> <li><code>consistency</code>: everyone follows the same standards.</li> <li><code>automation</code>: formatting should be largely effortless after initial configuration.</li> </ul>"},{"location":"courses/mlops/styling/#tools","title":"Tools","text":"<p>We will be using a very popular blend of style and formatting conventions that makes some very opinionated decisions on our behalf (with configurable options).</p> <ul> <li><code>Black</code>: an in-place reformatter that (mostly) adheres to PEP8.</li> <li><code>isort</code>: sorts and formats import statements inside Python scripts.</li> <li><code>flake8</code>: a code linter with stylistic conventions that adhere to PEP8.</li> </ul>"},{"location":"courses/mlops/styling/#configuration","title":"Configuration","text":"<p>Before we can properly use these tools, we'll have to configure them because they may have some discrepancies amongst them since they follow slightly different conventions that extend from PEP8.</p>"},{"location":"courses/mlops/styling/#black","title":"Black","text":"<p>To configure Black, we could just pass in options using the CLI method, but it's much cleaner to do this through our <code>pyproject.toml</code> file.</p> <pre><code># Black formatting\n[tool.black]\nline-length = 150\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n      .eggs         # exclude a few common directories in the\n    | .git          # root of the project\n    | .hg\n    | .mypy_cache\n    | .tox\n    | venv\n    | _build\n    | buck-out\n    | build\n    | dist\n  )/\n'''\n</code></pre> <p>Here we're telling Black what our maximum line length should and to include and exclude certain file extensions.</p> <p>The pyproject.toml was created to establish a more human-readable configuration file that is meant to replace a <code>setup.py</code> or <code>setup.cfg</code> file and is increasingly adopted by many open-source libraries.</p>"},{"location":"courses/mlops/styling/#isort","title":"isort","text":"<p>Next, we're going to configure isort in our <code>pyproject.toml</code> file (just below Black's configurations):</p> <pre><code># iSort\n[tool.isort]\nprofile = \"black\"\nline_length = 79\nmulti_line_output = 3\ninclude_trailing_comma = true\nvirtual_env = \"venv\"\n</code></pre> <p>Though there is a complete list of configuration options for isort, we've decided to set these explicitly so there are no conflicts with Black.</p>"},{"location":"courses/mlops/styling/#flake8","title":"flake8","text":"<p>Lastly, we'll set up flake8 by also adding it's configuration details to out <code>pyproject.toml</code> file.</p> <pre><code>[tool.flake8]\nexclude = \"venv\"\nignore = [\"E501\", \"W503\", \"E226\"]\n# E501: Line too long\n# W503: Line break occurred before binary operator\n# E226: Missing white space around arithmetic operator\n</code></pre> <p>Here we're including an <code>ignore</code> option to ignore certain flake8 rules so everything works with our Black and isort configurations. And besides defining configuration options here, which are applied globally, we can also choose to specifically ignore certain conventions on a line-by-line basis. Here is an example of how we utilize this:</p> <pre><code># madewithml/config.py\nimport pretty_errors  # NOQA: F401 (imported but unused)\n</code></pre> <p>By placing the <code># NOQA: &lt;error-code&gt;</code> on a line, we're telling flake8 to do NO Quality Assurance for that particular error on this line.</p>"},{"location":"courses/mlops/styling/#usage","title":"Usage","text":"<p>To use these tools that we've configured, we have to execute them from the project directory: <pre><code>black .\nflake8\nisort .\n</code></pre></p> <pre>\nblack .\nAll done! \u2728 \ud83c\udf70 \u2728\n9 files left unchanged.\nflake8\npython3 -m isort . isort .\nFixing ...\n</pre> <p>Take a look at your files to see all the changes that have been made!</p> <p>the <code>.</code> signifies that the configuration file for that package is in the current directory</p>"},{"location":"courses/mlops/styling/#makefile","title":"Makefile","text":"<p>Remembering these three lines to style our code is a bit cumbersome so it's a good idea to create a Makefile. This file can be used to define a set of commands that can be executed with a single command. Here's what our Makefile looks like:</p> <pre><code># Makefile\nSHELL = /bin/bash\n\n# Styling\n.PHONY: style\nstyle:\nblack .\n    flake8\n    python3 -m isort .\n    pyupgrade\n\n# Cleaning\n.PHONY: clean\nclean: style\nfind . -type f -name \"*.DS_Store\" -ls -delete\n    find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf\n    find . | grep -E \".pytest_cache\" | xargs rm -rf\n    find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf\n    rm -rf .coverage*\n</code></pre> <p>Notice that the <code>clean</code> command depends on the <code>style</code> command (<code>clean: style</code>), which means that <code>style</code> will be executed first before <code>clean</code> is executed.</p> <p>.PHONY</p> <p>As the name suggests, a Makefile is typically used to make a file, where if a file with the name already exists, then the commands below won't be executed. But we're using it in a way where we want to execute some commands with a single alias. Therefore, the <code>.PHONY: $FILENAME</code> lines indicate that even if there is a file called <code>$FILENAME</code>, go ahead and execute the commands below anyway.</p> <p>In the next lesson on pre-commit we'll learn how to automatically execute this formatting whenever we make changes to our code.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Styling - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/systems-design/","title":"Machine Learning Systems Design","text":""},{"location":"courses/mlops/systems-design/#overview","title":"Overview","text":"<p>In the previous lesson, we covered the product design process for our ML application. In this lesson, we'll cover the systems design process where we'll learn how to design the ML system that will address our product objectives.</p>"},{"location":"courses/mlops/systems-design/#template","title":"Template","text":"<p>The template below is designed to guide machine learning product development. It involves both the product and systems design aspects of our application:</p> <p>Product design (What &amp; Why) \u2192 Systems design (How)</p> <p>\ud83d\udc49 \u00a0 Download a PDF of the ML canvas to use for your own products \u2192 ml-canvas.pdf (right click the link and hit \"Save Link As...\")</p>"},{"location":"courses/mlops/systems-design/#systems-design","title":"Systems design","text":"<p>How can we engineer our approach for building the product? We need to account for everything from data ingestion to model serving.</p>"},{"location":"courses/mlops/systems-design/#data","title":"Data","text":"<p>Describe the training and production (batches/streams) sources of data.</p> id created_on title description tag 0 6 2020-02-20 06:43:18 Comparison between YOLO and RCNN on real world ... Bringing theory to experiment is cool. We can ... computer-vision 1 89 2020-03-20 18:17:31 Rethinking Batch Normalization in Transformers We found that NLP batch statistics exhibit large ... natural-language-processing 2 1274 2020-06-10 05:21:00 Getting Machine Learning to Production Machine learning is hard and there are a lot, a lot of ... mlops 4 19 2020-03-03 13:54:31 Diffusion to Vector Reference implementation of Diffusion2Vec ... other <p>Our task</p> <ul> <li>training:<ul> <li>access to training data and testing (holdout) data.</li> <li>was there sampling of any kind applied to create this dataset?</li> <li>are we introducing any data leaks?</li> </ul> </li> <li>production:<ul> <li>access to batches or real-time streams of ML content from various sources</li> <li>how can we trust that this stream only has data that is consistent with what we have historically seen?</li> </ul> </li> </ul> <p> Assumption Reality Reason All of our incoming data is only machine learning related (no spam). We would need a filter to remove spam content that's not ML related. To simplify our ML task, we will assume all the data is ML content. </p>"},{"location":"courses/mlops/systems-design/#labeling","title":"Labeling","text":"<p>Describe the labeling process (ingestions, QA, etc.) and how we decided on the features and labels.</p> <p>Our task</p> <p>Labels: categories of machine learning (for simplification, we've restricted the label space to the following tags: <code>natural-language-processing</code>, <code>computer-vision</code>, <code>mlops</code> and <code>other</code>).</p> <p>Features: text features (title and description) that describe the content.</p> <p> Assumption Reality Reason Content can only belong to one category (multiclass). Content can belong to more than one category (multilabel). For simplicity and many libraries don't support or complicate multilabel scenarios. </p>"},{"location":"courses/mlops/systems-design/#metrics","title":"Metrics","text":"<p>One of the hardest challenges with ML systems is tying our core objectives, many of which may be qualitative, with quantitative metrics that our model can optimize towards.</p> <p>Our task</p> <p>For our task, we want to have both high precision and recall, so we'll optimize for f1 score (weighted combination of precision and recall). We'll determine these metrics for the overall dataset, as well as specific classes or slices of data.</p> <ul> <li>True positives (TP): we correctly predicted class X.</li> <li>False positives (FP): we incorrectly predicted class X but it was another class.</li> <li>True negatives (TN): we correctly predicted that it's wasn't the class X.</li> <li>False negatives (FN): we incorrectly predicted that it wasn't the class X but it was.</li> </ul> \\[ \\text{precision} = \\frac{TP}{TP + FP} \\] \\[ \\text{recall} = \\frac{TP}{TP + FN} \\] \\[ \\text{f1} = \\frac{2 * precision * recall}{precision + recall} \\] <p> </p> <p>What are our priorities</p> <p>How do we decide which metrics to prioritize?</p> Show answer <p>It entirely depends on the specific task. For example, in an email spam detector, precision is very important because it's better than we some spam then completely miss an important email. Overtime, we need to iterate on our solution so all evaluation metrics improve but it's important to know which one's we can't comprise on from the get-go.</p>"},{"location":"courses/mlops/systems-design/#evaluation","title":"Evaluation","text":"<p>Once we have our metrics defined, we need to think about when and how we'll evaluate our model.</p>"},{"location":"courses/mlops/systems-design/#offline-evaluation","title":"Offline evaluation","text":"<p>Offline evaluation requires a gold standard holdout dataset that we can use to benchmark all of our models.</p> <p>Our task</p> <p>We'll be using this holdout dataset for offline evaluation. We'll also be creating slices of data that we want to evaluate in isolation.</p>"},{"location":"courses/mlops/systems-design/#online-evaluation","title":"Online evaluation","text":"<p>Online evaluation ensures that our model continues to perform well in production and can be performed using labels or, in the event we don't readily have labels, proxy signals.</p> <p>Our task</p> <ul> <li>manually label a subset of incoming data to evaluate periodically.</li> <li>asking the initial set of users viewing a newly categorized content if it's correctly classified.</li> <li>allow users to report misclassified content by our model.</li> </ul> <p>It's important that we measure real-time performance before committing to replace our existing version of the system.</p> <ul> <li>Internal canary rollout, monitoring for proxy/actual performance, etc.</li> <li>Rollout to the larger internal team for more feedback.</li> <li>A/B rollout to a subset of the population to better understand UX, utility, etc.</li> </ul> <p>Not all releases have to be high stakes and external facing. We can always include internal releases, gather feedback and iterate until we\u2019re ready to increase the scope.</p>"},{"location":"courses/mlops/systems-design/#modeling","title":"Modeling","text":"<p>While the specific methodology we employ can differ based on the problem, there are core principles we always want to follow:</p> <ul> <li>End-to-end utility: the end result from every iteration should deliver minimum end-to-end utility so that we can benchmark iterations against each other and plug-and-play with the system.</li> <li>Manual before ML: try to see how well a simple rule-based system performs before moving onto more complex ones.</li> <li>Augment vs. automate: allow the system to supplement the decision making process as opposed to making the actual decision.</li> <li>Internal vs. external: not all early releases have to be end-user facing. We can use early versions for internal validation, feedback, data collection, etc.</li> <li>Thorough: every approach needs to be well tested (code, data + models) and evaluated, so we can objectively benchmark different approaches.</li> </ul> <p>Our task</p> <ol> <li>creating a gold-standard labeled dataset that is representative of the problem space.</li> <li>rule-based text matching approaches to categorize content.</li> <li>predict labels (probabilistic) from content title and description.</li> </ol> <p> Assumption Reality Reason Solution needs to involve ML due to unstructured data and ineffectiveness of rule-based systems for this task. An iterative approach where we start with simple rule-based solutions and slowly add complexity. This course is about responsibly delivering value with ML, so we'll jump to it right away. </p> <p>Utility in starting simple</p> <p>Some of the earlier, simpler, approaches may not deliver on a certain performance objective. What are some advantages of still starting simple?</p> Show answer <ul> <li>get internal feedback on end-to-end utility.</li> <li>perform A/B testing to understand UI/UX design.</li> <li>deployed locally to start generating more data required for more complex approaches.</li> </ul>"},{"location":"courses/mlops/systems-design/#inference","title":"Inference","text":"<p>Once we have a model we're satisfied with, we need to think about whether we want to perform batch (offline) or real-time (online) inference.</p>"},{"location":"courses/mlops/systems-design/#batch-inference","title":"Batch inference","text":"<p>We can use our models to make batch predictions on a finite set of inputs which are then written to a database for low latency inference. When a user or downstream service makes an inference request, cached results from the database are returned. In this scenario, our trained model can directly be loaded and used for inference in the code. It doesn't have to be served as a separate service.</p> <ul> <li>\u2705\u00a0 generate and cache predictions for very fast inference for users.</li> <li>\u2705\u00a0 the model doesn't need to be spun up as it's own service since it's never used in real-time.</li> <li>\u274c\u00a0 predictions can become stale if user develops new interests that aren\u2019t captured by the old data that the current predictions are based on.</li> </ul> <p>Batch serving tasks</p> <p>What are some tasks where batch serving is ideal?</p> Show answer <p>Recommend content that existing users will like based on their viewing history. However, new users may just receive some generic recommendations based on their explicit interests until we process their history the next day. And even if we're not doing batch serving, it might still be useful to cache very popular sets of input features (ex. combination of explicit interests leads to certain recommended content) so that we can serve those predictions faster.</p>"},{"location":"courses/mlops/systems-design/#online-inference","title":"Online inference","text":"<p>We can also serve real-time predictions where input features are fed to the model to retrieve predictions. In this scenario, our model will need to be served as a separate service (ex. api endpoint) that can handle incoming requests.</p> <ul> <li>\u2705\u00a0 can yield more up-to-date predictions which may yield a more meaningful user experience, etc.</li> <li>\u274c\u00a0 requires managed microservices to handle request traffic.</li> <li>\u274c\u00a0 requires real-time monitoring since input space in unbounded, which could yield erroneous predictions.</li> </ul> <p>Online inference tasks</p> <p>In our example task for batch inference above, how can online inference significantly improve content recommendations?</p> Show answer <p>With batch processing, we generate content recommendations for users offline using their history. These recommendations won't change until we process the batch the next day using the updated user features. But what is the user's taste significantly changes during the day (ex. user is searching for horror movies to watch). With real-time serving, we can use these recent features to recommend highly relevant content based on the immediate searches.</p> <p>Our task</p> <p>For our task, we'll be serving our model as a separate service to handle real-time requests. We want to be able to perform online inference so that we can quickly categorize ML content as they become available. However, we will also demonstrate how to do batch inference for the sake of completeness.</p>"},{"location":"courses/mlops/systems-design/#feedback","title":"Feedback","text":"<p>How do we receive feedback on our system and incorporate it into the next iteration? This can involve both human-in-the-loop feedback as well as automatic feedback via monitoring, etc.</p> <p>Our task</p> <ul> <li>enforce human-in-loop checks when there is low confidence in classifications.</li> <li>allow users to report issues related to misclassification.</li> </ul> <p>Always return to the value proposition</p> <p>While it's important to iterate and optimize on our models, it's even more important to ensure that our ML systems are actually making an impact. We need to constantly engage with our users to iterate on why our ML system exists and how it can be made better.</p> <p> </p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Systems - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/testing/","title":"Testing Machine Learning Systems: Code, Data and Models","text":""},{"location":"courses/mlops/testing/#intuition","title":"Intuition","text":"<p>In this lesson, we'll learn how to test code, data and machine learning models to construct a machine learning system that we can reliably iterate on. Tests are a way for us to ensure that something works as intended. We're incentivized to implement tests and discover sources of error as early in the development cycle as possible so that we can decrease downstream costs and wasted time. Once we've designed our tests, we can automatically execute them every time we change or add to our codebase.</p> <p>Tip</p> <p>We highly recommend that you explore this lesson after completing the previous lessons since the topics (and code) are iteratively developed. We did, however, create the  testing-ml repository for a quick overview with an interactive notebook.</p>"},{"location":"courses/mlops/testing/#types-of-tests","title":"Types of tests","text":"<p>There are four majors types of tests which are utilized at different points in the development cycle:</p> <ol> <li><code>Unit tests</code>: tests on individual components that each have a single responsibility (ex. function that filters a list).</li> <li><code>Integration tests</code>: tests on the combined functionality of individual components (ex. data processing).</li> <li><code>System tests</code>: tests on the design of a system for expected outputs given inputs (ex. training, inference, etc.).</li> <li><code>Acceptance tests</code>: tests to verify that requirements have been met, usually referred to as User Acceptance Testing (UAT).</li> <li><code>Regression tests</code>: tests based on errors we've seen before to ensure new changes don't reintroduce them.</li> </ol> <p>While ML systems are probabilistic in nature, they are composed of many deterministic components that can be tested in a similar manner as traditional software systems. The distinction between testing ML systems begins when we move from testing code to testing the data and models.</p> <p>There are many other types of functional and non-functional tests as well, such as smoke tests (quick health checks), performance tests (load, stress), security tests, etc. but we can generalize all of these under the system tests above.</p>"},{"location":"courses/mlops/testing/#how-should-we-test","title":"How should we test?","text":"<p>The framework to use when composing tests is the Arrange Act Assert methodology.</p> <ul> <li><code>Arrange</code>: set up the different inputs to test on.</li> <li><code>Act</code>: apply the inputs on the component we want to test.</li> <li><code>Assert</code>: confirm that we received the expected output.</li> </ul> <p><code>Cleaning</code> is an unofficial fourth step to this methodology because it's important to not leave remnants of a previous test which may affect subsequent tests. We can use packages such as pytest-randomly to test against state dependency by executing tests randomly.</p> <p>In Python, there are many tools, such as unittest, pytest, etc. that allow us to easily implement our tests while adhering to the Arrange Act Assert framework. These tools come with powerful built-in functionality such as parametrization, filters, and more, to test many conditions at scale.</p>"},{"location":"courses/mlops/testing/#what-should-we-test","title":"What should we test?","text":"<p>When arranging our inputs and asserting our expected outputs, what are some aspects of our inputs and outputs that we should be testing for?</p> <ul> <li>inputs: data types, format, length, edge cases (min/max, small/large, etc.)</li> <li>outputs: data types, formats, exceptions, intermediary and final outputs</li> </ul> <p>\ud83d\udc49 \u00a0We'll cover specific details pertaining to what to test for regarding our data and models below.</p>"},{"location":"courses/mlops/testing/#best-practices","title":"Best practices","text":"<p>Regardless of the framework we use, it's important to strongly tie testing into the development process.</p> <ul> <li><code>atomic</code>: when creating functions and classes, we need to ensure that they have a single responsibility so that we can easily test them. If not, we'll need to split them into more granular components.</li> <li><code>compose</code>: when we create new components, we want to compose tests to validate their functionality. It's a great way to ensure reliability and catch errors early on.</li> <li><code>reuse</code>: we should maintain central repositories where core functionality is tested at the source and reused across many projects. This significantly reduces testing efforts for each new project's code base.</li> <li><code>regression</code>: we want to account for new errors we come across with a regression test so we can ensure we don't reintroduce the same errors in the future.</li> <li><code>coverage</code>: we want to ensure 100% coverage for our codebase. This doesn't mean writing a test for every single line of code but rather accounting for every single line.</li> <li><code>automate</code>: in the event we forget to run our tests before committing to a repository, we want to auto run tests when we make changes to our codebase. We'll learn how to do this locally using pre-commit hooks and remotely via GitHub actions in subsequent lessons.</li> </ul>"},{"location":"courses/mlops/testing/#implementation","title":"Implementation","text":"<p>In our codebase, we'll be testing the code, data and models.</p> <pre><code>tests/\n\u251c\u2500\u2500 code/\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u251c\u2500\u2500 test_data.py\n\u2502   \u251c\u2500\u2500 test_predict.py\n\u2502   \u251c\u2500\u2500 test_train.py\n\u2502   \u251c\u2500\u2500 test_tune.py\n\u2502   \u251c\u2500\u2500 test_utils.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2514\u2500\u2500 test_dataset.py\n\u2514\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2514\u2500\u2500 test_behavioral.py\n</code></pre> <p>Note that we aren't testing <code>evaluate.py</code> and <code>serve.py</code> because it involves complicated testing that's based on the data and models. We'll be testing these components as part of our integration tests when we test our system end-to-end.</p>"},{"location":"courses/mlops/testing/#code","title":"\ud83d\udcbb\u00a0 Code","text":"<p>We'll start by testing our code and we'll use pytest as our testing framework for it's powerful builtin features such as parametrization, fixtures, markers and more.</p>"},{"location":"courses/mlops/testing/#configuration","title":"Configuration","text":"<p>Pytest expects tests to be organized under a <code>tests</code> directory by default. However, we can also add to our existing <code>pyproject.toml</code> file to configure any other test directories as well. Once in the directory, pytest looks for python scripts starting with <code>tests_*.py</code> but we can configure it to read any other file patterns as well.</p> <pre><code># Pytest\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = \"test_*.py\"\n</code></pre>"},{"location":"courses/mlops/testing/#assertions","title":"Assertions","text":"<p>Let's see what a sample test and it's results look like. Assume we have a simple function that decodes a list of indices into their respective classes using a dictionary mapping.</p> <pre><code># madewithml/predict.py\ndef decode(indices: Iterable[Any], index_to_class: Dict) -&gt; List:\n    return [index_to_class[index] for index in indices]\n</code></pre> <p>To test this function, we can use assert statements to map inputs with expected outputs. The statement following the word <code>assert</code> must return <code>True</code>.</p> <pre><code># tests/code/test_predict.py\ndef test_decode():\ndecoded = predict.decode(\n    indices=[0, 1, 1],\n    index_to_class={0: \"x\", 1: \"y\"})\nassert decoded == [\"x\", \"y\", \"y\"]\n</code></pre> <p>We can also have assertions about exceptions like we do in lines 6-8 where all the operations under the with statement are expected to raise the specified exception.</p>"},{"location":"courses/mlops/testing/#execution","title":"Execution","text":"<p>We can execute our tests above using several different levels of granularity:</p> <pre><code>python3 -m pytest                                          # all tests\npython3 -m pytest tests/code                               # tests under a directory\npython3 -m pytest tests/code/test_predict.py               # tests for a single file\npython3 -m pytest tests/code/test_predict.py::test_decode  # tests for a single function\n</code></pre> <p>Running our specific test above would produce the following output: <pre><code>python3 -m pytest tests/code/test_predict.py::test_decode --verbose --disable-warnings\n</code></pre></p> <pre>\ntests/code/test_predict.py::test_decode PASSED           [100%]\n</pre> <p>Had any of our assertions in this test failed, we would see the failed assertions, along with the expected and actual output from our function.</p> <pre>\ntests/code/test_predict.py::test_decode FAILED                          [100%]\n\n    def test_decode():\n        decoded = predict.decode(\n            indices=[0, 1, 1],\n            index_to_class={0: \"x\", 1: \"y\"})\n&gt;       assert decoded == [\"x\", \"x\", \"y\"]\nE       AssertionError: assert ['x', 'y', 'y'] == ['x', 'x', 'y']\nE        At index 1 diff: 'y' != 'x'\n</pre> <p>Tip</p> <p>It's important to test for the variety of inputs and expected outputs that we outlined above and to never assume that a test is trivial. In our example above, it's important that we test for both \"apple\" and \"Apple\" in the event that our function didn't account for casing!</p>"},{"location":"courses/mlops/testing/#classes","title":"Classes","text":"<p>We can also test classes and their respective functions.</p> <pre><code>def test_fit_transform():\n    preprocessor = data.CustomPreprocessor()\n    ds = data.load_data(dataset_loc=\"...\")\n    preprocessor.fit_transform(ds)\n    assert len(preprocessor.class_to_index) == 4\n</code></pre> <p>There are also more xunit-style testing options available as well for more involved testing with classes.</p>"},{"location":"courses/mlops/testing/#parametrize","title":"Parametrize","text":"<p>So far, in our tests, we've had to create individual assert statements to validate different combinations of inputs and expected outputs. However, there's a bit of redundancy here because the inputs always feed into our functions as arguments and the outputs are compared with our expected outputs. To remove this redundancy, pytest has the <code>@pytest.mark.parametrize</code> decorator which allows us to represent our inputs and outputs as parameters.</p> <pre><code>@pytest.mark.parametrize(\n    \"text, sw, clean_text\",\n    [\n        (\"hi\", [], \"hi\"),\n        (\"hi you\", [\"you\"], \"hi\"),\n        (\"hi yous\", [\"you\"], \"hi yous\"),\n    ],\n)\ndef test_clean_text(text, sw, clean_text):\n    assert data.clean_text(text=text, stopwords=sw) == clean_text\n</code></pre> <ol> <li><code>[Line 2]</code>: define the names of the parameters under the decorator, ex. \"fruit, crisp\" (note that this is one string).</li> <li><code>[Lines 3-7]</code>: provide a list of combinations of values for the parameters from Step 1.</li> <li><code>[Line 9]</code>: pass in parameter names to the test function.</li> <li><code>[Line 10]</code>: include necessary assert statements which will be executed for each of the combinations in the list from Step 2.</li> </ol> <p>Similarly, we could pass in an exception as the expected result as well:</p> <pre><code>@pytest.mark.parametrize(\n    \"x, exception\",\n    [\n        (1, ValueError),\n    ],\n)\ndef test_foo(x, exception):\n    with pytest.raises(exception):\n        foo(x=x)\n</code></pre>"},{"location":"courses/mlops/testing/#fixtures","title":"Fixtures","text":"<p>Parametrization allows us to reduce redundancy inside test functions but what about reducing redundancy across different test functions? For example, suppose that different test functions all have a common component (ex. preprocessor). Here, we can use pytest's builtin fixture, which is a function that is executed before the test function. Let's rewrite our <code>test_fit_transform</code> function from above using a fixture:</p> <pre><code>def test_fit_transform(dataset_loc, preprocessor):\n    ds = data.load_data(dataset_loc=dataset_loc)\n    preprocessor.fit_transform(ds)\n    assert len(preprocessor.class_to_index) == 4\n</code></pre> <p>where <code>dataset_loc</code> and <code>preprocessor</code> are fixtures defined in our <code>tests/code/conftest.py</code> script:</p> <pre><code># tests/code/conftest.py\nimport pytest\nfrom madewithml.data import CustomPreprocessor\n\n@pytest.fixture\ndef dataset_loc():\n    return \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n\n@pytest.fixture\ndef preprocessor():\n    return CustomPreprocessor()\n</code></pre> <p>All of our test scripts know to look inside a <code>conftest.py</code> script in the same directory for any fixtures. Note that the name of the fixture and the input argument to our function have to be the same.</p> <p>Fixture scopes</p> <p>Fixtures can have different scopes depending on how we want to use them. For example our <code>df</code> fixture has the module scope because we don't want to keep recreating it after every test but, instead, we want to create it just once for all the tests in our module (<code>tests/test_data.py</code>).</p> <ul> <li><code>function</code>: fixture is destroyed after every test. <code>[default]</code></li> <li><code>class</code>: fixture is destroyed after the last test in the class.</li> <li><code>module</code>: fixture is destroyed after the last test in the module (script).</li> <li><code>package</code>: fixture is destroyed after the last test in the package.</li> <li><code>session</code>: fixture is destroyed after the last test of the session.</li> </ul>"},{"location":"courses/mlops/testing/#markers","title":"Markers","text":"<p>We've been able to execute our tests at various levels of granularity (all tests, script, function, etc.) but we can create custom granularity by using markers. We've already used one type of marker (parametrize) but there are several other builtin markers as well. For example, the <code>skipif</code> marker allows us to skip execution of a test if a condition is met. For example, supposed we only wanted to test training our model if a GPU is available:</p> <pre><code>@pytest.mark.skipif(\n    not torch.cuda.is_available(),\n    reason=\"Full training tests require a GPU.\"\n)\ndef test_training():\n    pass\n</code></pre> <p>We can also create our own custom markers with the exception of a few reserved marker names.</p> <pre><code>@pytest.mark.training\ndef test_train_model(dataset_loc):\n    pass\n</code></pre> <p>We can execute them by using the <code>-m</code> flag which requires a (case-sensitive) marker expression like below:</p> <pre><code>pytest -m \"training\"      #  runs all tests marked with `training`\npytest -m \"not training\"  #  runs all tests besides those marked with `training`\n</code></pre> <p>Tip</p> <p>The proper way to use markers is to explicitly list the ones we've created in our pyproject.toml file. Here we can specify that all markers must be defined in this file with the <code>--strict-markers</code> flag and then declare our markers (with some info about them) in our <code>markers</code> list:</p> <pre><code>@pytest.mark.training\ndef test_train_model():\n    assert ...\n</code></pre> <p><pre><code># Pytest\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = \"test_*.py\"\naddopts = \"--strict-markers --disable-pytest-warnings\"\nmarkers = [\n\"training: tests that involve training\",\n]\n</code></pre> Once we do this, we can view all of our existing list of markers by executing <code>pytest --markers</code> and we'll receive an error when we're trying to use a new marker that's not defined here.</p>"},{"location":"courses/mlops/testing/#coverage","title":"Coverage","text":"<p>As we're developing tests for our application's components, it's important to know how well we're covering our code base and to know if we've missed anything. We can use the Coverage library to track and visualize how much of our codebase our tests account for. With pytest, it's even easier to use this package thanks to the pytest-cov plugin.</p> <pre><code>python3 -m pytest tests/code --cov madewithml --cov-report html --disable-warnings\n</code></pre> <p>Here we're asking to run all tests under <code>tests/code</code> and to check for coverage for all the code in our <code>madewithml</code> directory. When we run this, we'll see the tests from our tests directory executing while the coverage plugin is keeping tracking of which lines in our application are being executed. Once our tests are done, we can view the generated report either through the terminal:</p> <pre><code>coverage report -m\n</code></pre> <pre>\nName                    Stmts   Miss  Cover   Missing\n-----------------------------------------------------\nmadewithml/config.py       16      0   100%\nmadewithml/data.py         51      0   100%\nmadewithml/models.py        2      0   100%\nmadewithml/predict.py      23      0   100%\nmadewithml/train.py        45      0   100%\nmadewithml/tune.py         51      0   100%\nmadewithml/utils.py        39      0   100%\n-----------------------------------------------------\nTOTAL                     227      0   100%\n</pre> <p>but a more interactive way is to view it through the <code>htmlcov/index.html</code> on a browser. Here we can click on individual files to see which parts were not covered by any tests.</p> <p>Warning</p> <p>Though we have 100% coverage, this does not mean that our application is perfect. Coverage only indicates that a piece of code executed in a test, not necessarily that every part of it was tested, let alone thoroughly tested. Therefore, coverage should never be used as a representation of correctness. However, it is very useful to maintain coverage at 100% so we can know when new functionality has yet to be tested. In our CI/CD lesson, we'll see how to use GitHub actions to make 100% coverage a requirement when pushing to specific branches.</p>"},{"location":"courses/mlops/testing/#exclusions","title":"Exclusions","text":"<p>Sometimes it doesn't make sense to write tests to cover every single line in our application yet we still want to account for these lines so we can maintain 100% coverage. We have two levels of purview when applying exclusions:</p> <ol> <li> <p>Excusing lines by adding this comment <code># pragma: no cover, &lt;MESSAGE&gt;</code> <pre><code>if results_fp:  # pragma: no cover, saving results\n    utils.save_dict(d, results_fp)\n</code></pre></p> </li> <li> <p>Excluding files by specifying them in our <code>pyproject.toml</code> configuration:</p> </li> </ol> <pre><code># Pytest cov\n[tool.coverage.run]\nomit=[\"madewithml/evaluate.py\", \"madewithml/serve.py\"]\n</code></pre> <p>The main point is that we were able to add justification to these exclusions through comments so our team can follow our reasoning.</p> <p>Now that we have a foundation for testing traditional software, let's dive into testing our data and models in the context of machine learning systems.</p>"},{"location":"courses/mlops/testing/#data","title":"\ud83d\udd22\u00a0 Data","text":"<p>So far, we've used unit and integration tests to test the functions that interact with our data but we haven't tested the validity of the data itself. We're going to use the great expectations library to test what our data is expected to look like. It's a library that allows us to create expectations as to what our data should look like in a standardized way. It also provides modules to seamlessly connect with backend data sources such as local file systems, S3, databases, etc. Let's explore the library by implementing the expectations we'll need for our application.</p> <p>\ud83d\udc49 \u00a0 Follow along interactive notebook in the  testing-ml repository as we implement the concepts below.</p> <p>First we'll load the data we'd like to apply our expectations on. We can load our data from a variety of sources (filesystem, database, cloud etc.) which we can then wrap around a Dataset module (Pandas / Spark DataFrame, SQLAlchemy). Since multiple data tests may want access to this data, we'll create a fixture for it.</p> <pre><code># tests/data/conftest.py\nimport great_expectations as ge\nimport pandas as pd\nimport pytest\n\n@pytest.fixture(scope=\"module\")\ndef df(request):\n    dataset_loc = request.config.getoption(\"--dataset-loc\")\n    df = ge.dataset.PandasDataset(pd.read_csv(dataset_loc))\n    return df\n</code></pre>"},{"location":"courses/mlops/testing/#expectations","title":"Expectations","text":"<p>When it comes to creating expectations as to what our data should look like, we want to think about our entire dataset and all the features (columns) within it.</p> <pre><code>column_list = [\"id\", \"created_on\", \"title\", \"description\", \"tag\"]\ndf.expect_table_columns_to_match_ordered_list(column_list=column_list)  # schema adherence\ntags = [\"computer-vision\", \"natural-language-processing\", \"mlops\", \"other\"]\ndf.expect_column_values_to_be_in_set(column=\"tag\", value_set=tags)  # expected labels\ndf.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"])  # data leaks\ndf.expect_column_values_to_not_be_null(column=\"tag\")  # missing values\ndf.expect_column_values_to_be_unique(column=\"id\")  # unique values\ndf.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\")  # type adherence\n</code></pre> <p>Each of these expectations will create an output with details about success or failure, expected and observed values, expectations raised, etc. For example, the expectation <code>df.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\")</code> would produce the following if successful:</p> <pre><code>{\n\"exception_info\": {\n\"raised_exception\": false,\n\"exception_traceback\": null,\n\"exception_message\": null\n},\n\"success\": true,\n\"meta\": {},\n\"expectation_config\": {\n\"kwargs\": {\n\"column\": \"title\",\n\"type_\": \"str\",\n\"result_format\": \"BASIC\"\n},\n\"meta\": {},\n\"expectation_type\": \"_expect_column_values_to_be_of_type__map\"\n},\n\"result\": {\n\"element_count\": 955,\n\"missing_count\": 0,\n\"missing_percent\": 0.0,\n\"unexpected_count\": 0,\n\"unexpected_percent\": 0.0,\n\"unexpected_percent_nonmissing\": 0.0,\n\"partial_unexpected_list\": []\n}\n}\n</code></pre> <p>and if we have a failed expectation (ex. <code>df.expect_column_values_to_be_of_type(column=\"title\", type_=\"int\")</code>), we'd receive this output(notice the counts and examples for what caused the failure): <pre><code>{\n\"success\": false,\n\"exception_info\": {\n\"raised_exception\": false,\n\"exception_traceback\": null,\n\"exception_message\": null\n},\n\"expectation_config\": {\n\"meta\": {},\n\"kwargs\": {\n\"column\": \"title\",\n\"type_\": \"int\",\n\"result_format\": \"BASIC\"\n},\n\"expectation_type\": \"_expect_column_values_to_be_of_type__map\"\n},\n\"result\": {\n\"element_count\": 955,\n\"missing_count\": 0,\n\"missing_percent\": 0.0,\n\"unexpected_count\": 955,\n\"unexpected_percent\": 100.0,\n\"unexpected_percent_nonmissing\": 100.0,\n\"partial_unexpected_list\": [\n\"How to Deal with Files in Google Colab: What You Need to Know\",\n\"Machine Learning Methods Explained (+ Examples)\",\n\"OpenMMLab Computer Vision\",\n\"...\",\n]\n},\n\"meta\": {}\n}\n</code></pre></p> <p>There are just a few of the different expectations that we can create. Be sure to explore all the expectations, including custom expectations. Here are some other popular expectations that don't pertain to our specific dataset but are widely applicable:</p> <ul> <li>feature value relationships with other feature values \u2192 <code>expect_column_pair_values_a_to_be_greater_than_b</code></li> <li>value statistics (mean, std, median, max, min, sum, etc.) \u2192 <code>expect_column_mean_to_be_between</code></li> </ul>"},{"location":"courses/mlops/testing/#suite","title":"Suite","text":"<p>Instead of running each of these individually, we can combine them all into an expectation suite.</p> <pre><code># tests/data/test_dataset.py\ndef test_dataset(df):\n\"\"\"Test dataset quality and integrity.\"\"\"\n    column_list = [\"id\", \"created_on\", \"title\", \"description\", \"tag\"]\n    df.expect_table_columns_to_match_ordered_list(column_list=column_list)  # schema adherence\n    tags = [\"computer-vision\", \"natural-language-processing\", \"mlops\", \"other\"]\n    df.expect_column_values_to_be_in_set(column=\"tag\", value_set=tags)  # expected labels\n    df.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"])  # data leaks\n    df.expect_column_values_to_not_be_null(column=\"tag\")  # missing values\n    df.expect_column_values_to_be_unique(column=\"id\")  # unique values\n    df.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\")  # type adherence\n\n    # Expectation suite\nexpectation_suite = df.get_expectation_suite(discard_failed_expectations=False)\nresults = df.validate(expectation_suite=expectation_suite, only_return_failures=True).to_json_dict()\nassert results[\"success\"]\n</code></pre> <p>We can now execute these data tests just like a code test.</p> <pre><code>export DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings\n</code></pre> <p>Note</p> <p>We've added a <code>--dataset-loc</code> flag to pytest by specifying in our <code>tests/data/conftest.py</code> script. This allows us to pass in the dataset location as an argument to our tests.</p> <pre><code># tests/data/conftest.py\ndef pytest_addoption(parser):\n    parser.addoption(\"--dataset-loc\", action=\"store\", default=None, help=\"Dataset location.\")\n</code></pre> <p>We're keeping things simple by using our expectations with pytest but Great expectations also has a lot more functionality around connecting to data sources, Checkpoints to execute suites across various parts of the pipeline, data docs to generate reports, etc.</p>"},{"location":"courses/mlops/testing/#production","title":"Production","text":"<p>While we're validating our datasets inside our machine learning applications, in most production scenarios, the data validation happens much further upstream. Our dataset may not be used just for our specific application and may actually be feeding into many other downstream application (ML and otherwise). Therefore, it's a great idea to execute these data validation tests as up stream as possible so that downstream applications can reliably use the data.</p> <p>Learn more about different data systems in our data engineering lesson if you're not familiar with them.</p>"},{"location":"courses/mlops/testing/#models","title":"\ud83e\udd16\u00a0 Models","text":"<p>The final aspect of testing ML systems involves how to test machine learning models during training, evaluation, inference and deployment.</p>"},{"location":"courses/mlops/testing/#training","title":"Training","text":"<p>We want to write tests iteratively while we're developing our training pipelines so we can catch errors quickly. This is especially important because, unlike traditional software, ML systems can run to completion without throwing any exceptions / errors but can produce incorrect systems. We also want to catch errors quickly to save on time and compute.</p> <ul> <li>Check shapes and values of model output <pre><code>assert model(inputs).shape == torch.Size([len(inputs), num_classes])\n</code></pre></li> <li>Check for decreasing loss after one batch of training <pre><code>assert epoch_loss &lt; prev_epoch_loss\n</code></pre></li> <li>Overfit on a batch <pre><code>accuracy = train(model, inputs=batches[0])\nassert accuracy == pytest.approx(0.95, abs=0.05) # 0.95 \u00b1 0.05\n</code></pre></li> <li>Train to completion (tests early stopping, saving, etc.) <pre><code>train(model)\nassert learning_rate &gt;= min_learning_rate\nassert artifacts\n</code></pre></li> <li>On different devices <pre><code>assert train(model, device=torch.device(\"cpu\"))\nassert train(model, device=torch.device(\"cuda\"))\n</code></pre></li> </ul> <p>Note</p> <p>You can mark the compute intensive tests with a pytest marker and only execute them when there is a change being made to system affecting the model. <pre><code>@pytest.mark.training\ndef test_train_model():\n    ...\n</code></pre></p>"},{"location":"courses/mlops/testing/#behavioral-testing","title":"Behavioral testing","text":"<p>Behavioral testing is the process of testing input data and expected outputs while treating the model as a black box (model agnostic evaluation). They don't necessarily have to be adversarial in nature but more along the types of perturbations we may expect to see in the real world once our model is deployed. A landmark paper on this topic is Beyond Accuracy: Behavioral Testing of NLP Models with CheckList which breaks down behavioral testing into three types of tests:</p> <ul> <li><code>invariance</code>: Changes should not affect outputs. <pre><code># INVariance via verb injection (changes should not affect outputs)\nget_label(text=\"Transformers applied to NLP have revolutionized machine learning.\", predictor=predictor)\nget_label(text=\"Transformers applied to NLP have disrupted machine learning.\", predictor=predictor)\n</code></pre></li> </ul> <pre>\n'natural-language-processing'\n'natural-language-processing'\n</pre> <ul> <li><code>directional</code>: Change should affect outputs. <pre><code># DIRectional expectations (changes with known outputs)\nget_label(text=\"ML applied to text classification.\", predictor=predictor)\nget_label(text=\"ML applied to image classification.\", predictor=predictor)\nget_label(text=\"CNNs for text classification.\", predictor=predictor)\n</code></pre></li> </ul> <pre>\n'natural-language-processing'\n'computer-vision'\n'natural-language-processing'\n</pre> <ul> <li><code>minimum functionality</code>: Simple combination of inputs and expected outputs. <pre><code># Minimum Functionality Tests (simple input/output pairs)\nget_label(text=\"Natural language processing is the next big wave in machine learning.\", predictor=predictor)\nget_label(text=\"MLOps is the next big wave in machine learning.\", predictor=predictor)\nget_label(text=\"This is about graph neural networks.\", predictor=predictor)\n</code></pre></li> </ul> <pre>\n'natural-language-processing'\n'mlops'\n'other'\n</pre> <p>And we can convert these tests into proper parameterized tests by first defining from fixtures in our <code>tests/model/conftest.py</code> and our <code>tests/model/utils.py</code> scripts:</p> <pre><code># tests/model/conftest.py\nimport pytest\nfrom ray.train.torch.torch_predictor import TorchPredictor\nfrom madewithml import predict\n\ndef pytest_addoption(parser):\n    parser.addoption(\"--run-id\", action=\"store\", default=None, help=\"Run ID of model to use.\")\n\n\n@pytest.fixture(scope=\"module\")\ndef run_id(request):\n    return request.config.getoption(\"--run-id\")\n\n\n@pytest.fixture(scope=\"module\")\ndef predictor(run_id):\n    best_checkpoint = predict.get_best_checkpoint(run_id=run_id)\n    predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n    return predictor\n</code></pre> <pre><code># tests/model/utils.py\nimport numpy as np\nimport pandas as pd\nfrom madewithml import predict\n\ndef get_label(text, predictor):\n    df = pd.DataFrame({\"title\": [text], \"description\": \"\", \"tag\": \"other\"})\n    z = predictor.predict(data=df)[\"predictions\"]\n    preprocessor = predictor.get_preprocessor()\n    label = predict.decode(np.stack(z).argmax(1), preprocessor.index_to_class)[0]\n    return label\n</code></pre> <p>And now, we can use these components to create our behavioral tests:</p> <pre><code># tests/model/test_behavioral.py\n@pytest.mark.parametrize(\n    \"input_a, input_b, label\",\n    [\n        (\n            \"Transformers applied to NLP have revolutionized machine learning.\",\n            \"Transformers applied to NLP have disrupted machine learning.\",\n            \"natural-language-processing\",\n        ),\n    ],\n)\ndef test_invariance(input_a, input_b, label, predictor):\n\"\"\"INVariance via verb injection (changes should not affect outputs).\"\"\"\n    label_a = utils.get_label(text=input_a, predictor=predictor)\n    label_b = utils.get_label(text=input_b, predictor=predictor)\n    assert label_a == label_b == label\n</code></pre> <pre><code># tests/model/test_behavioral.py\n@pytest.mark.parametrize(\n    \"input, label\",\n    [\n        (\n            \"ML applied to text classification.\",\n            \"natural-language-processing\",\n        ),\n        (\n            \"ML applied to image classification.\",\n            \"computer-vision\",\n        ),\n        (\n            \"CNNs for text classification.\",\n            \"natural-language-processing\",\n        ),\n    ],\n)\ndef test_directional(input, label, predictor):\n\"\"\"DIRectional expectations (changes with known outputs).\"\"\"\n    prediction = utils.get_label(text=input, predictor=predictor)\n    assert label == prediction\n</code></pre> <pre><code># tests/model/test_behavioral.py\n@pytest.mark.parametrize(\n    \"input, label\",\n    [\n        (\n            \"Natural language processing is the next big wave in machine learning.\",\n            \"natural-language-processing\",\n        ),\n        (\n            \"MLOps is the next big wave in machine learning.\",\n            \"mlops\",\n        ),\n        (\n            \"This is about graph neural networks.\",\n            \"other\",\n        ),\n    ],\n)\ndef test_mft(input, label, predictor):\n\"\"\"Minimum Functionality Tests (simple input/output pairs).\"\"\"\n    prediction = utils.get_label(text=input, predictor=predictor)\n    assert label == prediction\n</code></pre> <p>And we can execute them just like any other test:</p> <pre><code># Model tests\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npytest --run-id=$RUN_ID tests/model --verbose --disable-warnings\n</code></pre>"},{"location":"courses/mlops/testing/#testing-vs-monitoring","title":"Testing vs. monitoring","text":"<p>We'll conclude by talking about the similarities and distinctions between testing and monitoring. They're both integral parts of the ML development pipeline and depend on each other for iteration. Testing is assuring that our system (code, data and models) passes the expectations that we've established offline. Whereas, monitoring involves that these expectations continue to pass online on live production data while also ensuring that their data distributions are comparable to the reference window (typically subset of training data) through \\(t_n\\). When these conditions no longer hold true, we need to inspect more closely (retraining may not always fix our root problem).</p> <p>With monitoring, there are quite a few distinct concerns that we didn't have to consider during testing since it involves (live) data we have yet to see.</p> <ul> <li>features and prediction distributions (drift), typing, schema mismatches, etc.</li> <li>determining model performance (rolling and window metrics on overall and slices of data) using indirect signals (since labels may not be readily available).</li> <li>in situations with large data, we need to know which data points to label and upsample for training.</li> <li>identifying anomalies and outliers.</li> </ul> <p>We'll cover all of these concepts in much more depth (and code) in our monitoring lesson.</p>"},{"location":"courses/mlops/testing/#resources","title":"Resources","text":"<ul> <li>The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction</li> <li>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</li> <li>Robustness Gym: Unifying the NLP Evaluation Landscape</li> </ul> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Code - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/training/","title":"Distributed training","text":""},{"location":"courses/mlops/training/#intuition","title":"Intuition","text":"<p>Now that we have our data prepared, we can start training our models to optimize on our objective. Ideally, we would start with the simplest possible baseline and slowly add complexity to our models:</p> <ol> <li>Start with a random (chance) model. <p>Since we have four classes, we may expect a random model to be correct around 25% of the time but recall that not all of our classes have equal counts.</p> </li> <li>Develop a rule-based approach using if-else statements, regular expressions, etc. <p>We could build a list of common words for each class and if a word in the input matches a word in the list, we can predict that class.</p> </li> <li>Slowly add complexity by addressing limitations and motivating representations and model architectures. <p>We could start with a simple term frequency (TF-IDF) mode and then move onto embeddings with CNNs, RNNs, Transformers, etc.</p> </li> <li>Weigh tradeoffs (performance, latency, size, etc.) between performant baselines.</li> <li>Revisit and iterate on baselines as your dataset grows and new model architectures are developed.</li> </ol> <p>We're going to skip straight to step 3 of developing a complex model because this task involves unstructured data and rule-based systems are not well suited for this. And with the increase adoption of large language models (LLMs) as a proven model architecture for NLP tasks, we'll fine-tune a pretrained LLM on our dataset.</p> <p>Iterate on the data</p> <p>Instead of using a fixed dataset and iterating on the models, we could keep the model constant and iterate on the dataset. This is useful to improve the quality of our datasets.</p> <ul> <li>remove or fix data samples (false positives &amp; negatives)</li> <li>prepare and transform features</li> <li>expand or consolidate classes</li> <li>incorporate auxiliary datasets</li> <li>identify unique slices to boost</li> </ul>"},{"location":"courses/mlops/training/#distributed-training","title":"Distributed training","text":"<p>With the rapid increase in data (unstructured) and model sizes (ex. LLMs), it's becoming increasingly difficult to train models on a single machine. We need to be able to distribute our training across multiple machines in order to train our models in a reasonable amount of time. And we want to be able to do this without having to:</p> <ul> <li>set up a cluster by individually (and painstakingly) provisioning compute resources (CPU, GPU, etc.)</li> <li>writing complex code to distribute our training across multiple machines</li> <li>worry about communication and resource utilization between our different distributed compute resources</li> <li>worry about fault tolerance and recovery from our large training workloads</li> </ul> <p>To address all of these concerns, we'll be using Ray Train here in order to create a training workflow that can scale across multiple machines. While there are many options to choose from for distributed training, such as Pytorch Distributed Data Parallel (DDP), Horovod, etc., none of them allow us to scale across different machines with ease and do so with minimal changes to our single-machine training code as Ray does.</p> <p>Primer on distributed training</p> <p>With distributed training, there will be a head node that's responsible for orchestrating the training process. While the worker nodes that will be responsible for training the model and communicating results back to the head node. From a user's perspective, Ray abstracts away all of this complexity and we can simply define our training functionality with minimal changes to our code (as if we were training on a single machine).</p>"},{"location":"courses/mlops/training/#generative-ai","title":"Generative AI","text":"<p>In this lesson, we're going to be fine-tuning a pretrained large language model (LLM) using our labeled dataset. The specific class of LLMs we'll be using is called BERT. Bert models are encoder-only models and are the gold-standard for supervised NLP tasks. However, you may be wondering how do all the (much larger) LLM, created for generative applications, fare (GPT 4, Falcon 40B, Llama 2, etc.)?</p> <p>We chose the smaller BERT model for our course because it's easier to train and fine-tune. However, the workflow for fine-tuning the larger LLMs are quite similar as well. They do require much more compute but Ray abstracts away the scaling complexities involved with that.</p> <p>Note</p> <p>All the code for this section can be found in our separate benchmarks.ipynb notebook.</p>"},{"location":"courses/mlops/training/#set-up","title":"Set up","text":"<p>You'll need to first sign up for an OpenAI account and then grab your API key from here.</p> <pre><code>import openai\nopenai.api_key = \"YOUR_API_KEY\"\n</code></pre>"},{"location":"courses/mlops/training/#load-data","title":"Load data","text":"<p>We'll first load the our training and inference data into dataframes.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code># Load training data\nDATASET_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\ntrain_df = pd.read_csv(DATASET_LOC)\ntrain_df.head()\n</code></pre> id created_on title description tag 0 6 2020-02-20 06:43:18 Comparison between YOLO and RCNN on real world... Bringing theory to experiment is cool. We can ... computer-vision 1 7 2020-02-20 06:47:21 Show, Infer &amp; Tell: Contextual Inference for C... The beauty of the work lies in the way it arch... computer-vision 2 9 2020-02-24 16:24:45 Awesome Graph Classification A collection of important graph embedding, cla... other 3 15 2020-02-28 23:55:26 Awesome Monte Carlo Tree Search A curated list of Monte Carlo tree search pape... other 4 25 2020-03-07 23:04:31 AttentionWalk A PyTorch Implementation of \"Watch Your Step: ... other <pre><code># Unique labels\ntags = train_df.tag.unique().tolist()\ntags\n</code></pre> <pre>\n['computer-vision', 'other', 'natural-language-processing', 'mlops']\n</pre> <pre><code># Load inference dataset\nHOLDOUT_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\ntest_df = pd.read_csv(HOLDOUT_LOC)\n</code></pre>"},{"location":"courses/mlops/training/#utilities","title":"Utilities","text":"<p>We'll define a few utility functions to make the OpenAI request and to store our predictions. While we could perform batch prediction by loading samples until the context length is reached, we'll just perform one at a time since it's not too many data points and we can have fully deterministic behavior (if you insert new data, etc.). We'll also added some reliability in case we overload the endpoints with too many request at once.</p> <pre><code>import json\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set_theme()\nfrom sklearn.metrics import precision_recall_fscore_support\nimport time\nfrom tqdm import tqdm\n</code></pre> <p>We'll first define what a sample call to the OpenAI endpoint looks like. We'll pass in: - <code>system_content</code> that has information about how the LLM should behave. - <code>assistant_content</code> for any additional context it should have for answering our questions. - <code>user_content</code> that has our message or query to the LLM. - <code>model</code> should specify which specific model we want to send our request to.</p> <p>We can pass all of this information in through the <code>openai.ChatCompletion.create</code> function to receive our response.</p> <pre><code># Query OpenAI endpoint\nsystem_content = \"you only answer in rhymes\"  # system content (behavior)\nassistant_content = \"\"  # assistant content (context)\nuser_content = \"how are you\"  # user content (message)\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo-0613\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_content},\n        {\"role\": \"assistant\", \"content\": assistant_content},\n        {\"role\": \"user\", \"content\": user_content},\n    ],\n)\nprint (response.to_dict()[\"choices\"][0].to_dict()[\"message\"][\"content\"])\n</code></pre> <pre>\nI'm doing just fine, so glad you ask,\nRhyming away, up to the task.\nHow about you, my dear friend?\nTell me how your day did ascend.\n</pre> <p>Now, let's create a function that can predict tags for a given sample.</p> <pre><code>def get_tag(model, system_content=\"\", assistant_content=\"\", user_content=\"\"):\n    try:\n        # Get response from OpenAI\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": system_content},\n                {\"role\": \"assistant\", \"content\": assistant_content},\n                {\"role\": \"user\", \"content\": user_content},\n            ],\n        )\n        predicted_tag = response.to_dict()[\"choices\"][0].to_dict()[\"message\"][\"content\"]\n        return predicted_tag\n\n    except (openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n        return None\n</code></pre> <pre><code># Get tag\nmodel = \"gpt-3.5-turbo-0613\"\nsystem_context = f\"\"\"\n    You are a NLP prediction service that predicts the label given an input's title and description.\n    You must choose between one of the following labels for each input: {tags}.\n    Only respond with the label name and nothing else.\n    \"\"\"\nassistant_content = \"\"\nuser_context = \"Transfer learning with transformers: Using transformers for transfer learning on text classification tasks.\"\ntag = get_tag(model=model, system_content=system_context, assistant_content=assistant_content, user_content=user_context)\nprint (tag)\n</code></pre> <pre>\nnatural-language-processing\n</pre> <p>Next, let's create a function that can predict tags for a list of inputs.</p> <pre><code># List of dicts w/ {title, description} (just the first 3 samples for now)\nsamples = test_df[[\"title\", \"description\"]].to_dict(orient=\"records\")[:3]\nsamples\n</code></pre> <pre>\n[{'title': 'Diffusion to Vector',\n  'description': 'Reference implementation of Diffusion2Vec (Complenet 2018) built on Gensim and NetworkX. '},\n {'title': 'Graph Wavelet Neural Network',\n  'description': 'A PyTorch implementation of \"Graph Wavelet Neural Network\" (ICLR 2019) '},\n {'title': 'Capsule Graph Neural Network',\n  'description': 'A PyTorch implementation of \"Capsule Graph Neural Network\" (ICLR 2019).'}]\n</pre> <pre><code>def get_predictions(inputs, model, system_content, assistant_content=\"\"):\n    y_pred = []\n    for item in tqdm(inputs):\n        # Convert item dict to string\n        user_content = str(item)\n\n        # Get prediction\n        predicted_tag = get_tag(\n            model=model, system_content=system_content,\n            assistant_content=assistant_content, user_content=user_content)\n\n        # If error, try again after pause (repeatedly until success)\n        while predicted_tag is None:\n            time.sleep(30)  # could also do exponential backoff\n            predicted_tag = get_tag(\n                model=model, system_content=system_content,\n                assistant_content=assistant_content, user_content=user_content)\n\n        # Add to list of predictions\n        y_pred.append(predicted_tag)\n\n    return y_pred\n</code></pre> <pre><code># Get predictions for a list of inputs\nget_predictions(inputs=samples, model=model, system_content=system_context)\n</code></pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  2.96its]\n['computer-vision', 'computer-vision', 'computer-vision']\n</pre> <p>Next we'll define a function that can clean our predictions in the event that it's not the proper format or has hallucinated a tag outside of our expected tags.</p> <pre><code>def clean_predictions(y_pred, tags, default=\"other\"):\n    for i, item in enumerate(y_pred):\n        if item not in tags:  # hallucinations\n            y_pred[i] = default\n        if item.startswith(\"'\") and item.endswith(\"'\"):  # GPT 4 likes to places quotes\n            y_pred[i] = item[1:-1]\n    return y_pred\n</code></pre> <p>Tip</p> <p>Open AI has now released function calling and custom instructions which is worth exploring to avoid this manual cleaning.</p> <p>Next, we'll define a function that will plot our ground truth labels and predictions.</p> <pre><code>def plot_tag_dist(y_true, y_pred):\n    # Distribution of tags\n    true_tag_freq = dict(Counter(y_true))\n    pred_tag_freq = dict(Counter(y_pred))\n    df_true = pd.DataFrame({\"tag\": list(true_tag_freq.keys()), \"freq\": list(true_tag_freq.values()), \"source\": \"true\"})\n    df_pred = pd.DataFrame({\"tag\": list(pred_tag_freq.keys()), \"freq\": list(pred_tag_freq.values()), \"source\": \"pred\"})\n    df = pd.concat([df_true, df_pred], ignore_index=True)\n\n    # Plot\n    plt.figure(figsize=(10, 3))\n    plt.title(\"Tag distribution\", fontsize=14)\n    ax = sns.barplot(x=\"tag\", y=\"freq\", hue=\"source\", data=df)\n    ax.set_xticklabels(list(true_tag_freq.keys()), rotation=0, fontsize=8)\n    plt.legend()\n    plt.show()\n</code></pre> <p>And finally, we'll define a function that will combine all the utilities above to predict, clean and plot our results.</p> <pre><code>def evaluate(test_df, model, system_content, assistant_content=\"\", tags):\n    # Predictions\n    y_test = test_df.tag.to_list()\n    test_samples = test_df[[\"title\", \"description\"]].to_dict(orient=\"records\")\n    y_pred = get_predictions(\n        inputs=test_samples, model=model,\n        system_content=system_content, assistant_content=assistant_content)\n    y_pred = clean_predictions(y_pred=y_pred, tags=tags)\n\n    # Performance\n    metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n    performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n    print(json.dumps(performance, indent=2))\n    plot_tag_dist(y_true=y_test, y_pred=y_pred)\n    return y_pred, performance\n</code></pre>"},{"location":"courses/mlops/training/#zero-shot-learning","title":"Zero-shot learning","text":"<p>Now we're ready to start benchmarking our different LLMs with different context.</p> <pre><code>y_pred = {\"zero_shot\": {}, \"few_shot\": {}}\nperformance = {\"zero_shot\": {}, \"few_shot\": {}}\n</code></pre> <p>We'll start with zero-shot learning which involves providing the model with the <code>system_content</code> that tells it how to behave but no examples of the behavior (no <code>assistant_content</code>).</p> <pre><code>system_content = f\"\"\"\n    You are a NLP prediction service that predicts the label given an input's title and description.\n    You must choose between one of the following labels for each input: {tags}.\n    Only respond with the label name and nothing else.\n    \"\"\"\n</code></pre> <pre><code># Zero-shot with GPT 3.5\nmethod = \"zero_shot\"\nmodel = \"gpt-3.5-turbo-0613\"\ny_pred[method][model], performance[method][model] = evaluate(\n    test_df=test_df, model=model, system_content=system_content, tags=tags)\n</code></pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 191/191 [11:01&lt;00:00,  3.46s/it]\n{\n  \"precision\": 0.7919133278407181,\n  \"recall\": 0.806282722513089,\n  \"f1\": 0.7807530967691199\n}\n</pre> <pre><code># Zero-shot with GPT 4\nmethod = \"zero_shot\"\nmodel = \"gpt-4-0613\"\ny_pred[method][model], performance[method][model] = evaluate(\n    test_df=test_df, model=model, system_content=system_content, tags=tags)\n</code></pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 191/191 [02:28&lt;00:00,  1.29it/s]\n{\n  \"precision\": 0.9314722577069027,\n  \"recall\": 0.9267015706806283,\n  \"f1\": 0.9271956481845013\n}\n</pre>"},{"location":"courses/mlops/training/#few-shot-learning","title":"Few-shot learning","text":"<p>Now, we'll be adding a <code>assistant_context</code> with a few samples from our training data for each class. The intuition here is that we're giving the model a few examples (few-shot learning) of what each class looks like so that it can learn to generalize better.</p> <pre><code># Create additional context with few samples from each class\nnum_samples = 2\nadditional_context = []\ncols_to_keep = [\"title\", \"description\", \"tag\"]\nfor tag in tags:\n    samples = train_df[cols_to_keep][train_df.tag == tag][:num_samples].to_dict(orient=\"records\")\n    additional_context.extend(samples)\nadditional_context\n</code></pre> <pre>\n[{'title': 'Comparison between YOLO and RCNN on real world videos',\n  'description': 'Bringing theory to experiment is cool. We can easily train models in colab and find the results in minutes.',\n  'tag': 'computer-vision'},\n {'title': 'Show, Infer &amp; Tell: Contextual Inference for Creative Captioning',\n  'description': 'The beauty of the work lies in the way it architects the fundamental idea that humans look at the overall image and then individual pieces of it.\\r\\n',\n  'tag': 'computer-vision'},\n {'title': 'Awesome Graph Classification',\n  'description': 'A collection of important graph embedding, classification and representation learning papers with implementations.',\n  'tag': 'other'},\n {'title': 'Awesome Monte Carlo Tree Search',\n  'description': 'A curated list of Monte Carlo tree search papers with implementations. ',\n  'tag': 'other'},\n {'title': 'Rethinking Batch Normalization in Transformers',\n  'description': 'We found that NLP batch statistics exhibit large variance throughout training, which leads to poor BN performance.',\n  'tag': 'natural-language-processing'},\n {'title': 'ELECTRA: Pre-training Text Encoders as Discriminators',\n  'description': 'PyTorch implementation of the electra model from the paper: ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators',\n  'tag': 'natural-language-processing'},\n {'title': 'Pytest Board',\n  'description': 'Continuous pytest runner with awesome visualization.',\n  'tag': 'mlops'},\n {'title': 'Debugging Neural Networks with PyTorch and W&amp;B',\n  'description': 'A closer look at debugging common issues when training neural networks.',\n  'tag': 'mlops'}]\n</pre> <pre><code># Add assistant context\nassistant_content = f\"\"\"Here are some examples with the correct labels: {additional_context}\"\"\"\nprint (assistant_content)\n</code></pre> <pre>\nHere are some examples with the correct labels: [{'title': 'Comparison between YOLO and RCNN on real world videos', ... 'description': 'A closer look at debugging common issues when training neural networks.', 'tag': 'mlops'}]\n</pre> <p>Tip</p> <p>We could increase the number of samples by increasing the context length. We could also retrieve better few-shot samples by extracting examples from the training data that are similar to the current sample (ex. similar unique vocabulary).</p> <pre><code># Few-shot with GPT 3.5\nmethod = \"few_shot\"\nmodel = \"gpt-3.5-turbo-0613\"\ny_pred[method][model], performance[method][model] = evaluate(\n    test_df=test_df, model=model, system_content=system_content,\n    assistant_content=assistant_content, tags=tags)\n</code></pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 191/191 [04:18&lt;00:00,  1.35s/it]\n{\n  \"precision\": 0.8435247936255214,\n  \"recall\": 0.8586387434554974,\n  \"f1\": 0.8447984162323493\n}\n</pre> <pre><code># Few-shot with GPT 4\nmethod = \"few_shot\"\nmodel = \"gpt-4-0613\"\ny_pred[method][model], performance[method][model] = evaluate(\n    test_df=test_df, model=model, system_content=few_shot_context,\n    assistant_content=assistant_content, tags=tags)\n</code></pre> <pre>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 191/191 [02:11&lt;00:00,  1.46it/s]\n{\n  \"precision\": 0.9407759040163695,\n  \"recall\": 0.9267015706806283,\n  \"f1\": 0.9302632275594479\n}\n</pre> <p>As we can see, few shot learning performs better than it's respective zero shot counter part. GPT 4 has had considerable improvements in reducing hallucinations but for our supervised task this comes at an expense of high precision but lower recall and f1 scores. When GPT 4 is not confident, it would rather predict <code>other</code>.</p>"},{"location":"courses/mlops/training/#oss-llms","title":"OSS LLMs","text":"<p>So far, we've only been using closed-source models from OpenAI. While these are currently the gold-standard, there are many open-source models that are rapidly catching up (Falcon 40B, Llama 2, etc.). Before we see how these models perform on our task, let's first consider a few reasons why we should care about open-source models.</p> <ul> <li>data ownership: you can serve your models and pass data to your models, without having to share it with a third-party API endpoint.</li> <li>fine-tune: with access to our model's weights, we can actually fine-tune them, as opposed to experimenting with fickle prompting strategies.</li> <li>optimization: we have full freedom to optimize our deployed models for inference (ex. quantization, pruning, etc.) to reduce costs.</li> </ul> <pre><code># Coming soon in August!\n</code></pre>"},{"location":"courses/mlops/training/#results","title":"Results","text":"<p>Now let's compare all the results from our generative AI LLM benchmarks:</p> <pre><code>print(json.dumps(performance, indent=2))\n</code></pre> <pre><code>{\n\"zero_shot\": {\n\"gpt-3.5-turbo-0613\": {\n\"precision\": 0.7919133278407181,\n\"recall\": 0.806282722513089,\n\"f1\": 0.7807530967691199\n},\n\"gpt-4-0613\": {\n\"precision\": 0.9314722577069027,\n\"recall\": 0.9267015706806283,\n\"f1\": 0.9271956481845013\n}\n},\n\"few_shot\": {\n\"gpt-3.5-turbo-0613\": {\n\"precision\": 0.8435247936255214,\n\"recall\": 0.8586387434554974,\n\"f1\": 0.8447984162323493\n},\n\"gpt-4-0613\": {\n\"precision\": 0.9407759040163695,\n\"recall\": 0.9267015706806283,\n\"f1\": 0.9302632275594479\n}\n}\n}\n</code></pre> <p>And we can plot these on a bar plot to compare them visually.</p> <pre><code># Transform data into a new dictionary with four keys\nby_model_and_context = {}\nfor context_type, models_data in performance.items():\n    for model, metrics in models_data.items():\n        key = f\"{model}_{context_type}\"\n        by_model_and_context[key] = metrics\n</code></pre> <pre><code># Extracting the model names and the metric values\nmodels = list(by_model_and_context.keys())\nmetrics = list(by_model_and_context[models[0]].keys())\n\n# Plotting the bar chart with metric scores on top of each bar\nfig, ax = plt.subplots(figsize=(10, 4))\nwidth = 0.2\nx = range(len(models))\n\nfor i, metric in enumerate(metrics):\n    metric_values = [by_model_and_context[model][metric] for model in models]\n    ax.bar([pos + width * i for pos in x], metric_values, width, label=metric)\n    # Displaying the metric scores on top of each bar\n    for pos, val in zip(x, metric_values):\n        ax.text(pos + width * i, val, f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n\nax.set_xticks([pos + width for pos in x])\nax.set_xticklabels(models, rotation=0, ha='center', fontsize=8)\nax.set_ylabel('Performance')\nax.set_title('GPT Benchmarks')\nax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Our best model is GPT 4 with few shot learning at an f1 score of ~93%. We will see, in the rest of the course, how fine-tuning an LLM with a proper training dataset to change the actual weights of the last N layers (as opposed to the hard prompt tuning here) will yield similar/slightly better results to GPT 4 (at a fraction of the model size and inference costs).</p> <p>However, the best system might actually be a combination of using these few-shot hard prompt LLMs alongside fine-tuned LLMs. For example, our fine-tuned LLMs in the course will perform well when the test data is similar to the training data (similar distributions of vocabulary, etc.) but may not perform well on out of distribution. Whereas, these hard prompted LLMs, by themselves or augmented with additional context (ex. arXiv plugins in our case), could be used when our primary fine-tuned model is not so confident.</p>"},{"location":"courses/mlops/training/#setup","title":"Setup","text":"<p>We'll start by defining some setup utilities and configuring our model.</p> <pre><code>import os\nimport random\nimport torch\nfrom ray.data.preprocessor import Preprocessor\n</code></pre> <p>We'll define a <code>set_seeds</code> function that will set the seeds for reproducibility across our libraries (<code>np.random.seed</code>, <code>random.seed</code>, <code>torch.manual_seed</code> and <code>torch.cuda.manual_seed</code>). We'll also set the behavior for some torch backends to ensure deterministic results when we run our workloads on GPUs.</p> <pre><code>def set_seeds(seed=42):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    eval(\"setattr(torch.backends.cudnn, 'deterministic', True)\")\n    eval(\"setattr(torch.backends.cudnn, 'benchmark', False)\")\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n</code></pre> <p>Next, we'll define a simple <code>load_data</code> function to ingest our data from source (CSV files) and load it as a Ray Dataset.</p> <pre><code>def load_data(num_samples=None):\n    ds = ray.data.read_csv(DATASET_LOC)\n    ds = ds.random_shuffle(seed=1234)\n    ds = ray.data.from_items(ds.take(num_samples)) if num_samples else ds\n    return ds\n</code></pre> <p>Tip</p> <p>When working with very large datasets, it's a good idea to limit the number of samples in our dataset so that we can execute our code quickly and iterate on bugs, etc. This is why we have a <code>num_samples</code> input argument in our <code>load_data</code> function (<code>None</code> = no limit, all samples).</p> <p>We'll also define a custom preprocessor class that we'll to conveniently preprocess our dataset but also to save/load for later. When defining a preprocessor, we'll need to define a <code>_fit</code> method to learn how to fit to our dataset and a <code>_transform_{pandas|numpy}</code> method to preprocess the dataset using any components from the <code>_fit</code> method. We can either define a <code>_transform_pandas</code> method to apply our preprocessing to a Pandas DataFrame or a <code>_transform_numpy</code> method to apply our preprocessing to a NumPy array. We'll define the <code>_transform_pandas</code> method since our preprocessing function expects a batch of data as a Pandas DataFrame.</p> <pre><code>class CustomPreprocessor(Preprocessor):\n\"\"\"Custom preprocessor class.\"\"\"\n    def _fit(self, ds):\n        tags = ds.unique(column=\"tag\")\n        self.class_to_index = {tag: i for i, tag in enumerate(tags)}\n        self.index_to_class = {v:k for k, v in self.class_to_index.items()}\n    def _transform_pandas(self, batch):  # could also do _transform_numpy\n        return preprocess(batch, class_to_index=self.class_to_index)\n</code></pre>"},{"location":"courses/mlops/training/#model","title":"Model","text":"<p>Now we're ready to start defining our model architecture. We'll start by loading a pretrained LLM and then defining the components needed for fine-tuning it on our dataset. Our pretrained LLM here is a transformer-based model that has been pretrained on a large corpus of scientific text called scibert.</p> <p>If you're not familiar with transformer-based models like LLMs, be sure to check out the attention and Transformers lessons.</p> <pre><code>import torch.nn as nn\nfrom transformers import BertModel\n</code></pre> <p>We can load our pretrained model by using the from_pretrained` method.</p> <pre><code># Pretrained LLM\nllm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\nembedding_dim = llm.config.hidden_size\n</code></pre> <p>Once our model is loaded, we can tokenize an input text, convert it to torch tensors and pass it through our model to get a sequence and pooled representation of the text.</p> <pre><code># Sample\ntext = \"Transfer learning with transformers for text classification.\"\nbatch = tokenizer([text], return_tensors=\"np\", padding=\"longest\")\nbatch = {k:torch.tensor(v) for k,v in batch.items()}  # convert to torch tensors\nseq, pool = llm(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\nnp.shape(seq), np.shape(pool)\n</code></pre> <pre>\n(torch.Size([1, 10, 768]), torch.Size([1, 768]))\n</pre> <p>We're going to use this pretrained model to represent our input text features and add additional layers (linear classifier) on top of it for our specific classification task. In short, the pretrained LLM will process the tokenized text and return a sequence (one representation after each token) and pooled (combined) representation of the text. We'll use the pooled representation as input to our final fully-connection layer (<code>fc1</code>) to result in a vector of size <code>num_classes</code> (number of classes) that we can use to make predictions.</p> <pre><code>class FinetunedLLM(nn.Module):\n    def __init__(self, llm, dropout_p, embedding_dim, num_classes):\n        super(FinetunedLLM, self).__init__()\n        self.llm = llm\n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n\n    def forward(self, batch):\n        ids, masks = batch[\"ids\"], batch[\"masks\"]\n        seq, pool = self.llm(input_ids=ids, attention_mask=masks)\n        z = self.dropout(pool)\n        z = self.fc1(z)\n        return z\n\n    @torch.inference_mode()\n    def predict(self, batch):\n        self.eval()\n        z = self(inputs)\n        y_pred = torch.argmax(z, dim=1).cpu().numpy()\n        return y_pred\n\n    @torch.inference_mode()\n    def predict_proba(self, batch):\n        self.eval()\n        z = self(batch)\n        y_probs = F.softmax(z).cpu().numpy()\n        return y_probs\n</code></pre> <p>Let's initialize our model and inspect its layers:</p> <pre><code># Initialize model\nmodel = FinetunedLLM(llm=llm, dropout_p=0.5, embedding_dim=embedding_dim, num_classes=num_classes)\nprint (model.named_parameters)\n</code></pre> <pre>\n(llm): BertModel(\n(embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n)\n(encoder): BertEncoder(\n    (layer): ModuleList(\n    (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n        (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n        )\n        )\n        (intermediate): BertIntermediate(\n        (dense): Linear(in_features=768, out_features=3072, bias=True)\n        (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n        (dense): Linear(in_features=3072, out_features=768, bias=True)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        )\n    )\n    )\n)\n(pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n)\n)\n(dropout): Dropout(p=0.5, inplace=False)\n(fc1): Linear(in_features=768, out_features=4, bias=True)\n</pre>"},{"location":"courses/mlops/training/#batching","title":"Batching","text":"<p>We can iterate through our dataset in batches however we may have batches of different sizes. Recall that our tokenizer padded the inputs to the longest item in the batch (<code>padding=\"longest\"</code>). However, our batches for training will be smaller than our large data processing batches and so our batches here may have inputs with different lengths. To address this, we're going to define a custom <code>collate_fn</code> to repad the items in our training batches.</p> <pre><code>from ray.train.torch import get_device\n</code></pre> <p>Our <code>pad_array</code> function will take an array of arrays and pad the inner arrays to the longest length.</p> <pre><code>def pad_array(arr, dtype=np.int32):\n    max_len = max(len(row) for row in arr)\n    padded_arr = np.zeros((arr.shape[0], max_len), dtype=dtype)\n    for i, row in enumerate(arr):\n        padded_arr[i][:len(row)] = row\n    return padded_arr\n</code></pre> <p>And our <code>collate_fn</code> will take a batch of data to pad them and convert them to the appropriate PyTorch tensor types.</p> <pre><code>def collate_fn(batch):\n    batch[\"ids\"] = pad_array(batch[\"ids\"])\n    batch[\"masks\"] = pad_array(batch[\"masks\"])\n    dtypes = {\"ids\": torch.int32, \"masks\": torch.int32, \"targets\": torch.int64}\n    tensor_batch = {}\n    for key, array in batch.items():\n        tensor_batch[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n    return tensor_batch\n</code></pre> <p>Let's test our <code>collate_fn</code> on a sample batch from our dataset.</p> <pre><code># Sample batch\nsample_batch = sample_ds.take_batch(batch_size=128)\ncollate_fn(batch=sample_batch)\n</code></pre> <pre>\n{'ids': tensor([[  102,  5800, 14982,  ...,     0,     0,     0],\n         [  102,  7746,  2824,  ...,     0,     0,     0],\n         [  102,   502,  1371,  ...,     0,     0,     0],\n         ...,\n         [  102, 10431,   160,  ...,     0,     0,     0],\n         [  102,   124,   132,  ...,     0,     0,     0],\n         [  102, 12459, 28196,  ...,     0,     0,     0]], dtype=torch.int32),\n 'masks': tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32),\n 'targets': tensor([2, 0, 3, 2, 0, 3, 2, 0, 2, 0, 2, 2, 0, 3, 2, 0, 2, 3, 0, 2, 2, 0, 2, 2,\n         0, 1, 1, 0, 2, 0, 3, 2, 0, 3, 2, 0, 2, 0, 2, 2, 0, 2, 0, 3, 2, 0, 3, 2,\n         0, 2, 0, 2, 2, 0, 3, 2, 0, 2, 3, 0, 2, 2, 0, 2, 2, 0, 1, 1, 0, 3, 0, 0,\n         0, 3, 0, 1, 1, 0, 3, 2, 0, 2, 3, 0, 2, 2, 0, 2, 2, 0, 1, 1, 0, 3, 2, 0,\n         2, 3, 0, 2, 2, 0, 2, 2, 0, 1, 1, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 1, 1,\n         0, 0, 0, 1, 0, 0, 1, 0])}\n</pre>"},{"location":"courses/mlops/training/#utilities_1","title":"Utilities","text":"<p>Next, we'll implement set the necessary utility functions for distributed training.</p> <pre><code>from ray.air import Checkpoint, session\nfrom ray.air.config import CheckpointConfig, DatasetConfig, RunConfig, ScalingConfig\nimport ray.train as train\nfrom ray.train.torch import TorchCheckpoint, TorchTrainer\nimport torch.nn.functional as F\n</code></pre> <p>We'll start by defining what one step (or iteration) of training looks like. This will be a function that takes in a batch of data, a model, a loss function, and an optimizer. It will then perform a forward pass, compute the loss, and perform a backward pass to update the model's weights. And finally, it will return the loss.</p> <pre><code>def train_step(ds, batch_size, model, num_classes, loss_fn, optimizer):\n\"\"\"Train step.\"\"\"\n    model.train()\n    loss = 0.0\nds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\nfor i, batch in enumerate(ds_generator):\n        optimizer.zero_grad()  # reset gradients\n        z = model(batch)  # forward pass\n        targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n        J = loss_fn(z, targets)  # define loss\n        J.backward()  # backward pass\n        optimizer.step()  # update weights\n        loss += (J.detach().item() - loss) / (i + 1)  # cumulative loss\n    return loss\n</code></pre> <p>Note: We're using the <code>ray.data.iter_torch_batches</code> method instead of <code>torch.utils.data.DataLoader</code> to create a generator that will yield batches of data. In fact, this is the only line that's different from a typical PyTorch training loop and the actual training workflow remains untouched. Ray supports many other ways to load/consume data for different frameworks as well.</p> <p>The validation step is quite similar to the training step but we don't need to perform a backward pass or update the model's weights.</p> <pre><code>def eval_step(ds, batch_size, model, num_classes, loss_fn):\n\"\"\"Eval step.\"\"\"\n    model.eval()\n    loss = 0.0\n    y_trues, y_preds = [], []\nds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\nwith torch.inference_mode():\n        for i, batch in enumerate(ds_generator):\n            z = model(batch)\n            targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n            J = loss_fn(z, targets).item()\n            loss += (J - loss) / (i + 1)\n            y_trues.extend(batch[\"targets\"].cpu().numpy())\n            y_preds.extend(torch.argmax(z, dim=1).cpu().numpy())\n    return loss, np.vstack(y_trues), np.vstack(y_preds)\n</code></pre> <p>Next, we'll define the <code>train_loop_per_worker</code> which defines the overall training loop for each worker. It's important that we include operations like loading the datasets, models, etc. so that each worker will have its own copy of these objects. Ray takes care of combining all the workers' results at the end of each iteration, so from the user's perspective, it's the exact same as training on a single machine!</p> <p>The only additional lines of code we need to add compared to a typical PyTorch training loop are the following:</p> <ul> <li><code>session.get_dataset_shard(\"train\")</code> and <code>session.get_dataset_shard(\"val\")</code> to load the data splits (<code>session.get_dataset_shard</code>).</li> <li><code>model = train.torch.prepare_model(model)</code> to prepare the torch model for distributed execution (<code>train.torch.prepare_model</code>).</li> <li><code>batch_size_per_worker = batch_size // session.get_world_size()</code> to adjust the batch size for each worker (<code>session.get_world_size</code>).</li> <li><code>session.report(metrics, checkpoint=checkpoint)</code> to report metrics and save our model checkpoint (<code>session.report</code>).</li> </ul> <p>All the other lines of code are the same as a typical PyTorch training loop!</p> <pre><code># Training loop\ndef train_loop_per_worker(config):\n    # Hyperparameters\n    dropout_p = config[\"dropout_p\"]\n    lr = config[\"lr\"]\n    lr_factor = config[\"lr_factor\"]\n    lr_patience = config[\"lr_patience\"]\n    num_epochs = config[\"num_epochs\"]\n    batch_size = config[\"batch_size\"]\n    num_classes = config[\"num_classes\"]\n\n    # Get datasets\n    set_seeds()\ntrain_ds = session.get_dataset_shard(\"train\")\nval_ds = session.get_dataset_shard(\"val\")\n# Model\n    llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n    model = FinetunedLLM(llm=llm, dropout_p=dropout_p, embedding_dim=llm.config.hidden_size, num_classes=num_classes)\nmodel = train.torch.prepare_model(model)\n# Training components\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_factor, patience=lr_patience)\n\n    # Training\nbatch_size_per_worker = batch_size // session.get_world_size()\nfor epoch in range(num_epochs):\n        # Step\n        train_loss = train_step(train_ds, batch_size_per_worker, model, num_classes, loss_fn, optimizer)\n        val_loss, _, _ = eval_step(val_ds, batch_size_per_worker, model, num_classes, loss_fn)\n        scheduler.step(val_loss)\n\n        # Checkpoint\n        metrics = dict(epoch=epoch, lr=optimizer.param_groups[0][\"lr\"], train_loss=train_loss, val_loss=val_loss)\n        checkpoint = TorchCheckpoint.from_model(model=model)\nsession.report(metrics, checkpoint=checkpoint)\n</code></pre> <p></p> <p>Class imbalance</p> <p>Our dataset doesn't suffer from horrible class imbalance, but if it did, we could easily account for it through our loss function. There are also other strategies such as over-sampling less frequent classes and under-sampling popular classes.</p> <pre><code># Class weights\nbatch_counts = []\nfor batch in train_ds.iter_torch_batches(batch_size=256, collate_fn=collate_fn):\n    batch_counts.append(np.bincount(batch[\"targets\"].cpu().numpy()))\ncounts = [sum(count) for count in zip(*batch_counts)]\nclass_weights = np.array([1.0/count for i, count in enumerate(counts)])\nclass_weights_tensor = torch.Tensor(class_weights).to(get_device())\n\n# Training components\nloss_fn = nn.BCEWithLogitsLoss(weight=class_weights_tensor)\n...\n</code></pre>"},{"location":"courses/mlops/training/#configurations","title":"Configurations","text":"<p>Next, we'll define some configurations that will be used to train our model.</p> <pre><code># Train loop config\ntrain_loop_config = {\n    \"dropout_p\": 0.5,\n    \"lr\": 1e-4,\n    \"lr_factor\": 0.8,\n    \"lr_patience\": 3,\n    \"num_epochs\": 10,\n    \"batch_size\": 256,\n    \"num_classes\": num_classes,\n}\n</code></pre> <p>Next we'll define our scaling configuration (ScalingConfig) that will specify how we want to scale our training workload. We specify the number of workers (<code>num_workers</code>), whether to use GPU or not (<code>use_gpu</code>), the resources per worker (<code>resources_per_worker</code>) and how much CPU each worker is allowed to use (<code>_max_cpu_fraction_per_node</code>).</p> <pre><code># Scaling config\nscaling_config = ScalingConfig(\n    num_workers=num_workers,\n    use_gpu=bool(resources_per_worker[\"GPU\"]),\n    resources_per_worker=resources_per_worker,\n    _max_cpu_fraction_per_node=0.8,\n)\n</code></pre> <p><code>_max_cpu_fraction_per_node=0.8</code> indicates that 20% of CPU is reserved for non-training workloads that our workers will do such as data preprocessing (which we do prior to training anyway).</p> <p>Next, we'll define our <code>CheckpointConfig</code> which will specify how we want to checkpoint our model. Here we will just save one checkpoint (<code>num_to_keep</code>) based on the checkpoint with the <code>min</code> <code>val_loss</code>. We'll also configure a <code>RunConfig</code> which will specify the <code>name</code> of our run and where we want to save our checkpoints.</p> <pre><code># Run config\ncheckpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\nrun_config = RunConfig(name=\"llm\", checkpoint_config=checkpoint_config, local_dir=\"~/ray_results\")\n</code></pre> <p>We'll be naming our experiment <code>llm</code> and saving our results to <code>~/ray_results</code>, so a sample directory structure for our trained models would look like this:</p> <pre><code>/home/ray/ray_results/llm\n\u251c\u2500\u2500 TorchTrainer_fd40a_00000_0_2023-07-20_18-14-50/\n\u251c\u2500\u2500 basic-variant-state-2023-07-20_18-14-50.json\n\u251c\u2500\u2500 experiment_state-2023-07-20_18-14-50.json\n\u251c\u2500\u2500 trainer.pkl\n\u2514\u2500\u2500 tuner.pkl\n</code></pre> <p>The <code>TorchTrainer_</code> objects are the individuals runs in this experiment and each one will have the following contents:</p> <pre><code>/home/ray/ray_results/TorchTrainer_fd40a_00000_0_2023-07-20_18-14-50/\n\u251c\u2500\u2500 checkpoint_000009/  # we only save one checkpoint (the best)\n\u251c\u2500\u2500 events.out.tfevents.1689902160.ip-10-0-49-200\n\u251c\u2500\u2500 params.json\n\u251c\u2500\u2500 params.pkl\n\u251c\u2500\u2500 progress.csv\n\u2514\u2500\u2500 result.json\n</code></pre> <p>There are several other configs that we could set with Ray (ex. failure handling) so be sure to check them out here.</p> <p>Stopping criteria</p> <p>While we'll just let our experiments run for a certain number of epochs and stop automatically, our <code>RunConfig</code> accepts an optional stopping criteria (<code>stop</code>) which determines the conditions our training should stop for. It's entirely customizable and common examples include a certain metric value, elapsed time or even a custom class.</p>"},{"location":"courses/mlops/training/#training","title":"Training","text":"<p>Now we're finally ready to train our model using all the components we've setup above.</p> <pre><code># Load and split data\nds = load_data()\ntrain_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)\n</code></pre> <pre><code># Preprocess\npreprocessor = CustomPreprocessor()\ntrain_ds =  preprocessor.fit_transform(train_ds)\nval_ds = preprocessor.transform(val_ds)\ntrain_ds = train_ds.materialize()\nval_ds = val_ds.materialize()\n</code></pre> <p>Calling materialize here is important because it will cache the preprocessed data in memory. This will allow us to train our model without having to reprocess the data each time.</p> <p>Because we've preprocessed the data prior to training, we can use the <code>fit=False</code> and <code>transform=False</code> flags in our dataset config. This will allow us to skip the preprocessing step during training.</p> <pre><code># Dataset config\ndataset_config = {\n    \"train\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n    \"val\": DatasetConfig(fit=False, transform=False, randomize_block_order=False),\n}\n</code></pre> <p>We'll pass all of our functions and configs to the <code>TorchTrainer</code> class to start training. Ray supports a wide variety of framework Trainers so if you're using other frameworks, you can use the corresponding Trainer class instead.</p> <pre><code># Trainer\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n    datasets={\"train\": train_ds, \"val\": val_ds},\n    dataset_config=dataset_config,\n    preprocessor=preprocessor,\n)\n</code></pre> <p>Now let's fit our model to the data.</p> <pre><code># Train\nresults = trainer.fit()\n</code></pre> Trial name              status    loc               iter  total time (s)  epoch    lr  train_loss TorchTrainer_8c960_00000TERMINATED10.0.18.44:68577    10         76.3089      90.0001 0.000549661 <pre><code>results.metrics_dataframe\n</code></pre> epoch lr train_loss val_loss timestamp time_this_iter_s should_checkpoint done training_iteration trial_id date time_total_s pid hostname node_ip time_since_restore iterations_since_restore 0 0 0.0001 0.005196 0.004071 1689030896 14.162520 True False 1 8c960_00000 2023-07-10_16-14-59 14.162520 68577 ip-10-0-18-44 10.0.18.44 14.162520 1 1 1 0.0001 0.004033 0.003898 1689030905 8.704429 True False 2 8c960_00000 2023-07-10_16-15-08 22.866948 68577 ip-10-0-18-44 10.0.18.44 22.866948 2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9 9 0.0001 0.000550 0.001182 1689030958 6.604867 True False 10 8c960_00000 2023-07-10_16-16-01 76.308887 68577 ip-10-0-18-44 10.0.18.44 76.308887 10 <pre><code>results.best_checkpoints\n</code></pre> <pre>\n[(TorchCheckpoint(local_path=/home/ray/ray_results/llm/TorchTrainer_8c960_00000_0_2023-07-10_16-14-41/checkpoint_000009),\n  {'epoch': 9,\n   'lr': 0.0001,\n   'train_loss': 0.0005496611799268673,\n   'val_loss': 0.0011818759376183152,\n   'timestamp': 1689030958,\n   'time_this_iter_s': 6.604866981506348,\n   'should_checkpoint': True,\n   'done': True,\n   'training_iteration': 10,\n   'trial_id': '8c960_00000',\n   'date': '2023-07-10_16-16-01',\n   'time_total_s': 76.30888652801514,\n   'pid': 68577,\n   'hostname': 'ip-10-0-18-44',\n   'node_ip': '10.0.18.44',\n   'config': {'train_loop_config': {'dropout_p': 0.5,\n     'lr': 0.0001,\n     'lr_factor': 0.8,\n     'lr_patience': 3,\n     'num_epochs': 10,\n     'batch_size': 256,\n     'num_classes': 4}},\n   'time_since_restore': 76.30888652801514,\n   'iterations_since_restore': 10,\n   'experiment_tag': '0'})]\n</pre>"},{"location":"courses/mlops/training/#observability","title":"Observability","text":"<p>While our model is training, we can inspect our Ray dashboard to observe how our compute resources are being utilized.</p> <p>\ud83d\udcbb Local</p> <p>We can inspect our Ray dashboard by opening http://127.0.0.1:8265 on a browser window. Click on Cluster on the top menu bar and then we will be able to see a list of our nodes (head and worker) and their utilizations.</p> <p>\ud83d\ude80 Anyscale</p> <p>On Anyscale Workspaces, we can head over to the top right menu and click on \ud83d\udee0\ufe0f Tools \u2192 Ray Dashboard and this will open our dashboard on a new tab. Click on Cluster on the top menu bar and then we will be able to see a list of our nodes (head and worker) and their utilizations.</p> <p>Learn about all the other observability features on the Ray Dashboard through this video.</p>"},{"location":"courses/mlops/training/#evaluation","title":"Evaluation","text":"<p>Now that we've trained our model, we can evaluate it on a separate holdout test set. We'll cover the topic of evaluation much more extensively in our evaluation lesson but for now we'll calculate some quick overall metrics.</p> <pre><code>from ray.train.torch import TorchPredictor\nfrom sklearn.metrics import precision_recall_fscore_support\n</code></pre> <p>We'll define a function that can take in a dataset and a predictor and return the performance metrics.</p> <ol> <li>Load the predictor and preprocessor from the best checkpoint: <pre><code># Predictor\nbest_checkpoint = results.best_checkpoints[0][0]\npredictor = TorchPredictor.from_checkpoint(best_checkpoint)\npreprocessor = predictor.get_preprocessor()\n</code></pre></li> <li>Load and preprocess the test dataset that we want to evaluate on: <pre><code># Test (holdout) dataset\nHOLDOUT_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\ntest_ds = ray.data.read_csv(HOLDOUT_LOC)\npreprocessed_ds = preprocessor.transform(test_ds)\npreprocessed_ds.take(1)\n</code></pre></li> </ol> <pre>\n[{'ids': array([  102,  4905,  2069,  2470,  2848,  4905, 30132, 22081,   691,\n          4324,  7491,  5896,   341,  6136,   934, 30137,   103,     0,\n             0,     0,     0]),\n  'masks': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]),\n  'targets': 3}]\n</pre> <ol> <li>Retrieve the true label indices from the <code>targets</code> column by using ray.data.Dataset.select_column: <pre><code># y_true\nvalues = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\ny_true = np.stack([item[\"targets\"] for item in values])\nprint (y_true)\n</code></pre></li> </ol> <pre>\n[3 3 3 0 2 0 0 0 0 2 0 0 2 3 0 0 2 2 3 2 3 0 3 2 0 2 2 1 1 2 2 2 2 2 2 0 0\n 0 0 0 1 1 2 0 0 3 1 2 0 2 2 3 3 0 2 3 2 3 3 3 3 0 0 0 0 2 2 0 2 1 0 2 3 0\n 0 2 2 2 2 2 0 0 2 0 1 0 0 0 0 3 0 0 2 0 2 2 3 2 0 2 0 2 0 3 0 0 0 0 0 2 0\n 0 2 2 2 2 3 0 2 0 2 0 2 3 3 3 2 0 2 2 2 2 0 2 2 2 0 1 2 2 2 2 2 1 2 0 3 0\n 2 2 1 1 2 0 0 0 0 0 0 2 2 2 0 2 1 1 2 0 0 1 2 3 2 2 2 0 0 2 0 2 0 3 0 2 2\n 0 1 2 1 2 2]\n</pre> <ol> <li>Get our predicted label indices by using the <code>predictor</code>. Note that the <code>predictor</code> will automatically take care of the preprocessing for us. <pre><code># y_pred\nz = predictor.predict(data=test_ds.to_pandas())[\"predictions\"]\ny_pred = np.stack(z).argmax(1)\nprint (y_pred)\n</code></pre></li> </ol> <pre>\n[3 3 3 0 2 0 0 0 0 2 0 0 2 3 0 0 0 2 3 2 3 0 3 2 0 0 2 1 1 2 2 2 2 2 2 0 0\n 0 0 0 1 2 2 0 2 3 1 2 0 2 2 3 3 0 2 1 2 3 3 3 3 2 0 0 0 2 2 0 2 1 0 2 3 1\n 0 2 2 2 2 2 0 0 2 1 1 0 0 0 0 3 0 0 2 0 2 2 3 2 0 2 0 2 2 0 2 0 0 3 0 2 0\n 0 1 2 2 2 3 0 2 0 2 0 2 3 3 3 2 0 2 2 2 2 0 2 2 2 0 1 2 2 2 2 2 1 2 0 3 0\n 2 2 2 1 2 0 2 0 0 0 0 2 2 2 0 2 1 2 2 0 0 1 2 3 2 2 2 0 0 2 0 2 1 3 0 2 2\n 0 1 2 1 2 2]\n</pre> <ol> <li>Compute our metrics using the true and predicted labels indices. <pre><code># Evaluate\nmetrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n{\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n</code></pre></li> </ol> <pre>\n{'precision': 0.9147673308349523,\n 'recall': 0.9109947643979057,\n 'f1': 0.9115810676649443}\n</pre> <p>We're going to encapsulate all of these steps into one function so that we can call on it as we train more models soon.</p> <pre><code>def evaluate(ds, predictor):\n    # y_true\n    preprocessor = predictor.get_preprocessor()\n    preprocessed_ds = preprocessor.transform(ds)\n    values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n    y_true = np.stack([item[\"targets\"] for item in values])\n\n    # y_pred\n    z = predictor.predict(data=ds.to_pandas())[\"predictions\"]\n    y_pred = np.stack(z).argmax(1)\n\n    # Evaluate\n    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n    return performance\n</code></pre>"},{"location":"courses/mlops/training/#inference","title":"Inference","text":"<p>Now let's load our trained model for inference on new data. We'll create a few utility functions to format the probabilities into a dictionary for each class and to return predictions for each item in a dataframe.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code>def format_prob(prob, index_to_class):\n    d = {}\n    for i, item in enumerate(prob):\n        d[index_to_class[i]] = item\n    return d\n</code></pre> <pre><code>def predict_with_proba(df, predictor):\n    preprocessor = predictor.get_preprocessor()\n    z = predictor.predict(data=df)[\"predictions\"]\n    y_prob = torch.tensor(np.stack(z)).softmax(dim=1).numpy()\n    results = []\n    for i, prob in enumerate(y_prob):\n        tag = decode([z[i].argmax()], preprocessor.index_to_class)[0]\n        results.append({\"prediction\": tag, \"probabilities\": format_prob(prob, preprocessor.index_to_class)})\n    return results\n</code></pre> <p>We'll load our <code>predictor</code> from the best checkpoint and load it's <code>preprocessor</code>.</p> <pre><code># Preprocessor\npredictor = TorchPredictor.from_checkpoint(best_checkpoint)\npreprocessor = predictor.get_preprocessor()\n</code></pre> <p>And now we're ready to apply our model to new data. We'll create a sample dataframe with a title and description and then use our <code>predict_with_proba</code> function to get the predictions. Note that we use a placeholder value for <code>tag</code> since our input dataframe will automatically be preprocessed (and it expects a value in the <code>tag</code> column).</p> <pre><code># Predict on sample\ntitle = \"Transfer learning with transformers\"\ndescription = \"Using transformers for transfer learning on text classification tasks.\"\nsample_df = pd.DataFrame([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\npredict_with_proba(df=sample_df, predictor=predictor)\n</code></pre> <pre>\n[{'prediction': 'natural-language-processing',\n  'probabilities': {'computer-vision': 0.0007296873,\n   'mlops': 0.0008382588,\n   'natural-language-processing': 0.997829,\n   'other': 0.00060295867}}]\n</pre>"},{"location":"courses/mlops/training/#optimization","title":"Optimization","text":"<p>Distributed training strategies are great for when our data or models are too large for training but there are additional strategies to make the models itself smaller for serving. The following model compression techniques are commonly used to reduce the size of the model:</p> <ul> <li>Pruning: remove weights (unstructured) or entire channels (structured) to reduce the size of the network. The objective is to preserve the model\u2019s performance while increasing its sparsity.</li> <li>Quantization: reduce the memory footprint of the weights by reducing their precision (ex. 32 bit to 8 bit). We may loose some precision but it shouldn\u2019t affect performance too much.</li> <li>Distillation: training smaller networks to \u201cmimic\u201d larger networks by having it reproduce the larger network\u2019s layers\u2019 outputs.</li> </ul> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Training - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/tuning/","title":"Hyperparameter Tuning","text":""},{"location":"courses/mlops/tuning/#intuition","title":"Intuition","text":"<p>Hyperparameter tuning is the process of discovering a set of performant parameter values for our model. It can be a computationally involved process depending on the number of parameters, search space and model architectures. Hyperparameters don't just include the model's parameters but could also include parameters related to preprocessing, splitting, etc. When we look at all the different parameters that can be tuned, it quickly becomes a very large search space. However, just because something is a hyperparameter doesn't mean we need to tune it.</p> <ul> <li>It's absolutely acceptable to fix some hyperparameters (ex. using lower cased text [<code>lower=True</code>] during preprocessing).</li> <li>You can initially just tune a small, yet influential, subset of hyperparameters that you believe will yield great results.</li> </ul> <p>We want to optimize our hyperparameters so that we can understand how each of them affects our objective. By running many trials across a reasonable search space, we can determine near ideal values for our different parameters.</p>"},{"location":"courses/mlops/tuning/#frameworks","title":"Frameworks","text":"<p>There are many options for hyperparameter tuning (Ray tune, Optuna, Hyperopt, etc.). We'll be using Ray Tune with it's HyperOpt integration for it's simplicity and general popularity. Ray Tune also has a wide variety of support for many other tune search algorithms (Optuna, Bayesian, etc.).</p>"},{"location":"courses/mlops/tuning/#set-up","title":"Set up","text":"<p>There are many factors to consider when performing hyperparameter tuning. We'll be conducting a small study where we'll tune just a few key hyperparameters across a few trials. Feel free to include additional parameters and to increase the number trials in the tuning experiment.</p> <pre><code># Number of trials (small sample)\nnum_runs = 2\n</code></pre> <p>We'll start with some the set up, data and model prep as we've done in previous lessons.</p> <pre><code>from ray import tune\nfrom ray.tune import Tuner\nfrom ray.tune.schedulers import AsyncHyperBandScheduler\nfrom ray.tune.search import ConcurrencyLimiter\nfrom ray.tune.search.hyperopt import HyperOptSearch\n</code></pre> <p><pre><code># Set up\nset_seeds()\n</code></pre> <pre><code># Dataset\nds = load_data()\ntrain_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)\n</code></pre> <pre><code># Preprocess\npreprocessor = CustomPreprocessor()\ntrain_ds = preprocessor.fit_transform(train_ds)\nval_ds = preprocessor.transform(val_ds)\ntrain_ds = train_ds.materialize()\nval_ds = val_ds.materialize()\n</code></pre> <pre><code># Trainer\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    datasets={\"train\": train_ds, \"val\": val_ds},\n    dataset_config=dataset_config,\n    preprocessor=preprocessor,\n)\n</code></pre> <pre><code># MLflow callback\nmlflow_callback = MLflowLoggerCallback(\n    tracking_uri=MLFLOW_TRACKING_URI,\n    experiment_name=experiment_name,\n    save_artifact=True)\n</code></pre></p>"},{"location":"courses/mlops/tuning/#tune-configuration","title":"Tune configuration","text":"<p>We can think of tuning as training across different combinations of parameters. For this, we'll need to define several configurations around when to stop tuning (stopping criteria), how to define the next set of parameters to train with (search algorithm) and even the different values that the parameters can take (search space).</p> <p>We'll start by defining our <code>CheckpointConfig</code> and <code>RunConfig</code> as we did for training:</p> <pre><code># Run configuration\ncheckpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\nrun_config = RunConfig(\n    callbacks=[mlflow_callback],\n    checkpoint_config=checkpoint_config\n)\n</code></pre> <p>Notice that we use the same <code>mlflow_callback</code> from our experiment tracking lesson so all of our runs will be tracked to MLflow automatically.</p>"},{"location":"courses/mlops/tuning/#search-algorithm","title":"Search algorithm","text":"<p>Next, we're going to set the initial parameter values and the search algorithm (<code>HyperOptSearch</code>) for our tuning experiment. We're also going to set the maximum number of trials that can be run concurrently (<code>ConcurrencyLimiter</code>) based on the compute resources we have.</p> <pre><code># Hyperparameters to start with\ninitial_params = [{\"train_loop_config\": {\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}}]\nsearch_alg = HyperOptSearch(points_to_evaluate=initial_params)\nsearch_alg = ConcurrencyLimiter(search_alg, max_concurrent=2)\n</code></pre> <p>Tip</p> <p>It's a good idea to start with some initial parameter values that you think might be reasonable. This can help speed up the tuning process and also guarantee at least one experiment that will perform decently well.</p>"},{"location":"courses/mlops/tuning/#search-space","title":"Search space","text":"<p>Next, we're going to define the parameter search space by choosing the parameters, their distribution and range of values. Depending on the parameter type, we have many different distributions to choose from.</p> <pre><code># Parameter space\nparam_space = {\n    \"train_loop_config\": {\n        \"dropout_p\": tune.uniform(0.3, 0.9),\n        \"lr\": tune.loguniform(1e-5, 5e-4),\n        \"lr_factor\": tune.uniform(0.1, 0.9),\n        \"lr_patience\": tune.uniform(1, 10),\n    }\n}\n</code></pre>"},{"location":"courses/mlops/tuning/#scheduler","title":"Scheduler","text":"<p>Next, we're going to define a scheduler to prune unpromising trials. We'll be using <code>AsyncHyperBandScheduler</code> (ASHA), which is a very popular and aggressive early-stopping algorithm. Due to our aggressive scheduler, we'll set a <code>grace_period</code> to allow the trials to run for at least a few epochs before pruning and a maximum of <code>max_t</code> epochs.</p> <pre><code># Scheduler\nscheduler = AsyncHyperBandScheduler(\n    max_t=train_loop_config[\"num_epochs\"],  # max epoch (&lt;time_attr&gt;) per trial\n    grace_period=5,  # min epoch (&lt;time_attr&gt;) per trial\n)\n</code></pre>"},{"location":"courses/mlops/tuning/#tuner","title":"Tuner","text":"<p>Finally, we're going to define a <code>TuneConfig</code> that will combine the <code>search_alg</code> and <code>scheduler</code> we've defined above.</p> <pre><code># Tune config\ntune_config = tune.TuneConfig(\n    metric=\"val_loss\",\n    mode=\"min\",\n    search_alg=search_alg,\n    scheduler=scheduler,\n    num_samples=num_runs,\n)\n</code></pre> <p>And now, we'll pass in our <code>trainer</code> object with our configurations to create a <code>Tuner</code> object that we can run.</p> <pre><code># Tuner\ntuner = Tuner(\n    trainable=trainer,\n    run_config=run_config,\n    param_space=param_space,\n    tune_config=tune_config,\n)\n</code></pre> <pre><code># Tune\nresults = tuner.fit()\n</code></pre> Trial name status loc iter total time (s) epoch lr train_loss TorchTrainer_8e6e0_00000 TERMINATED 10.0.48.210:93017 10 76.2436 9 0.0001 0.0333853 <pre><code># All trials in experiment\nresults.get_dataframe()\n</code></pre> epoch lr train_loss val_loss timestamp time_this_iter_s should_checkpoint done training_iteration trial_id ... pid hostname node_ip time_since_restore iterations_since_restore config/train_loop_config/dropout_p config/train_loop_config/lr config/train_loop_config/lr_factor config/train_loop_config/lr_patience logdir 0 9 0.000100 0.04096 0.217990 1689460552 6.890944 True True 10 094e2a7e ... 94006 ip-10-0-48-210 10.0.48.210 76.588228 10 0.500000 0.000100 0.800000 3.000000 /home/ray/ray_results/TorchTrainer_2023-07-15_... 1 0 0.000027 0.63066 0.516547 1689460571 14.614296 True True 1 4f419368 ... 94862 ip-10-0-48-210 10.0.48.210 14.614296 1 0.724894 0.000027 0.780224 5.243006 /home/ray/ray_results/TorchTrainer_2023-07-15_... <p>And on our MLflow dashboard, we can create useful plots like a parallel coordinates plot to visualize the different hyperparameters and their values across the different trials.</p>"},{"location":"courses/mlops/tuning/#best-trial","title":"Best trial","text":"<p>And from these results, we can extract the best trial and its hyperparameters:</p> <pre><code># Best trial's epochs\nbest_trial = results.get_best_result(metric=\"val_loss\", mode=\"min\")\nbest_trial.metrics_dataframe\n</code></pre> epoch lr train_loss val_loss timestamp time_this_iter_s should_checkpoint done training_iteration trial_id date time_total_s pid hostname node_ip time_since_restore iterations_since_restore 0 0 0.0001 0.582092 0.495889 1689460489 14.537316 True False 1 094e2a7e 2023-07-15_15-34-53 14.537316 94006 ip-10-0-48-210 10.0.48.210 14.537316 1 1 1 0.0001 0.492427 0.430734 1689460497 7.144841 True False 2 094e2a7e 2023-07-15_15-35-00 21.682157 94006 ip-10-0-48-210 10.0.48.210 21.682157 2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9 9 0.0001 0.040960 0.217990 1689460552 6.890944 True True 10 094e2a7e 2023-07-15_15-35-55 76.588228 94006 ip-10-0-48-210 10.0.48.210 76.588228 10 <pre><code># Best trial's hyperparameters\nbest_trial.config[\"train_loop_config\"]\n</code></pre> <pre>\n{'dropout_p': 0.5, 'lr': 0.0001, 'lr_factor': 0.8, 'lr_patience': 3.0}\n</pre> <p>And now we'll load the best run from our experiment, which includes all the runs we've done so far (before and including the tuning runs).</p> <pre><code># Sorted runs\nsorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\nsorted_runs\n</code></pre> run_id experiment_id status artifact_uri start_time end_time metrics.lr metrics.epoch metrics.train_loss metrics.val_loss ... metrics.config/train_loop_config/num_classes params.train_loop_config/dropout_p params.train_loop_config/lr_patience params.train_loop_config/lr_factor params.train_loop_config/lr params.train_loop_config/num_classes params.train_loop_config/num_epochs params.train_loop_config/batch_size tags.mlflow.runName tags.trial_name 0 b140fdbc40804c4f94f9aef33e5279eb 999409133275979199 FINISHED file:///tmp/mlflow/999409133275979199/b140fdbc... 2023-07-15 22:34:39.108000+00:00 2023-07-15 22:35:56.260000+00:00 0.000100 9.0 0.040960 0.217990 ... NaN 0.5 3.0 0.8 0.0001 None None None TorchTrainer_094e2a7e TorchTrainer_094e2a7e 1 9ff8133613604564b0316abadc23b3b8 999409133275979199 FINISHED file:///tmp/mlflow/999409133275979199/9ff81336... 2023-07-15 22:33:05.206000+00:00 2023-07-15 22:34:24.322000+00:00 0.000100 9.0 0.033385 0.218394 ... 4.0 0.5 3 0.8 0.0001 4 10 256 TorchTrainer_8e6e0_00000 TorchTrainer_8e6e0_00000 2 e4f2d6be9eaa4302b3f697a36ed07d8c 999409133275979199 FINISHED file:///tmp/mlflow/999409133275979199/e4f2d6be... 2023-07-15 22:36:00.339000+00:00 2023-07-15 22:36:15.459000+00:00 0.000027 0.0 0.630660 0.516547 ... NaN 0.7248940325059469 5.243006476496198 0.7802237354477737 2.7345833037950673e-05 None None None TorchTrainer_4f419368 TorchTrainer_4f419368 <p>From this we can load the best checkpoint from the best run and evaluate it on the test split.</p> <pre><code># Evaluate on test split\nrun_id = sorted_runs.iloc[0].run_id\nbest_checkpoint = get_best_checkpoint(run_id=run_id)\npredictor = TorchPredictor.from_checkpoint(best_checkpoint)\nperformance = evaluate(ds=test_ds, predictor=predictor)\nprint (json.dumps(performance, indent=2))\n</code></pre> <pre>\n{\n  \"precision\": 0.9487609194455242,\n  \"recall\": 0.9476439790575916,\n  \"f1\": 0.9471734167970421\n}\n</pre> <p>And, just as we did in previous lessons, use our model for inference.</p> <p><pre><code># Preprocessor\npreprocessor = predictor.get_preprocessor()\n</code></pre> <pre><code># Predict on sample\ntitle = \"Transfer learning with transformers\"\ndescription = \"Using transformers for transfer learning on text classification tasks.\"\nsample_df = pd.DataFrame([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\npredict_with_proba(df=sample_df, predictor=predictor)\n</code></pre></p> <pre>\n[{'prediction': 'natural-language-processing',\n  'probabilities': {'computer-vision': 0.0003628606,\n   'mlops': 0.0002862369,\n   'natural-language-processing': 0.99908364,\n   'other': 0.0002672623}}]\n</pre> <p>Now that we're tuned our model, in the next lesson, we're going to perform a much more intensive evaluation on our model compared to just viewing it's overall metrics on a test set.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Tuning - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"courses/mlops/versioning/","title":"Versioning Code, Data and Models","text":""},{"location":"courses/mlops/versioning/#intuition","title":"Intuition","text":"<p>In this lesson, we're going to learn how to version our code, data and models to ensure reproducible behavior in our ML systems. It's imperative that we can reproduce our results and track changes to our system so we can debug and improve our application. Without it, it would be difficult to share our work, recreate our models in the event of system failures and fallback to previous versions in the event of regressions.</p>"},{"location":"courses/mlops/versioning/#code","title":"Code","text":"<p>To version our code, we'll be using git, which is a widely adopted version control system. In fact, when we cloned our repository in the setup lesson, we pulled code from a git repository that we had prepared for you.</p> <pre><code>git clone https://github.com/GokuMohandas/Made-With-ML.git .\n</code></pre> <p>We can then make changes to the code and Git, which is running locally on our computer, will keep track of our files and it's versions as we <code>add</code> and <code>commit</code> our changes. But it's not enough to just version our code locally, we need to <code>push</code> our work to a central location that can be <code>pull</code>ed by us and others we want to grant access to. This is where remote repositories like GitHub, GitLab, BitBucket, etc. provide a remote location to hold our versioned code in.</p> <p>Here's a simplified workflow for how we version our code using GitHub:</p> <pre><code>[make changes to code]\ngit add .\ngit commit -m \"message\"\ngit push origin &lt;branch-name&gt;\n</code></pre> <p>Tip</p> <p>If you're not familiar with Git, we highly recommend going through our Git lesson to learn the basics.</p>"},{"location":"courses/mlops/versioning/#artifacts","title":"Artifacts","text":"<p>While Git is ideal for saving our code, it's not ideal for saving artifacts like our datasets (especially unstructured data like text or images) and models. Also, recall that Git stores every version of our files and so large files that change frequently can very quickly take up space. So instead, it would be ideal if we can save locations (pointers) to these large artifacts in our code as opposed to the artifacts themselves. This way, we can version the locations of our artifacts and pull them as they're needed.</p>"},{"location":"courses/mlops/versioning/#data","title":"Data","text":"<p>While we're saving our dataset on GitHub for easy course access (and because our dataset is small), in a production setting, we would use a remote blob storage like S3 or a data warehouse like Snowflake. There are also many tools available for versioning our data, such as GitLFS, Dolt, Pachyderm, DVC, etc. With any of these solutions, we would be pointing to our remote storage location and versioning the pointer locations (ex. S3 bucket path) to our data instead of the data itself.</p>"},{"location":"courses/mlops/versioning/#models","title":"Models","text":"<p>And similarly, we currently store our models locally where the MLflow artifact and backend store are local directories.</p> <pre><code># Config MLflow\nMODEL_REGISTRY = Path(\"/tmp/mlflow\")\nPath(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\nMLFLOW_TRACKING_URI = \"file://\" + str(MODEL_REGISTRY.absolute())\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\nprint (mlflow.get_tracking_uri())\n</code></pre> <p>In a production setting, these would be remote such as S3 for the artifact store and a database service (ex. PostgreSQL RDS) as our backend store. This way, our models can be versioned and others, with the appropriate access credentials, can pull the model artifacts and deploy them.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p> <p>To cite this content, please use:</p> <pre><code>@article{madewithml,\nauthor       = {Goku Mohandas},\ntitle        = { Versioning - Made With ML },\nhowpublished = {\\url{https://madewithml.com/}},\nyear         = {2023}\n}\n</code></pre>"},{"location":"misc/calendar/","title":"Calendar","text":""},{"location":"misc/coming-soon/","title":"Coming soon","text":""},{"location":"misc/coming-soon/#_1","title":"Coming soon","text":"<p> This content will be coming soon! Be sure to subscribe and follow us on  Twitter and  LinkedIn for updates and tips.</p>"},{"location":"misc/confirmation/","title":"\ud83d\udcec Check your email","text":""},{"location":"misc/confirmation/#_1","title":"\ud83d\udcec Check your email","text":"<p>Thank you for subscribing to our newsletter! A confirmation email was sent to the email address you provided. Please click on the confirmation button in the email to complete your subscription. If you don\u2019t see it within a few minutes, be sure to check your promotions/spam/junk folder (and mark it <code>Not junk</code> so you receive our future emails).</p> <p>\u2190\u00a0 Return to home</p>"},{"location":"misc/newsletter/","title":"Newsletter","text":""},{"location":"misc/partnerships/","title":"Partnerships","text":""},{"location":"misc/partnerships/#_1","title":"Partnerships","text":""},{"location":"misc/partnerships/#our-mission","title":"Our Mission","text":"<p>We created Made With ML to educate and enable the community to responsibly develop, deploy and maintain production machine learning applications. While there are many facets to this mission, at its core are the relationships with teams who share this mission. We want to work together to help the community discover and use the best tools for their context and start the flywheel to make the tools even better.</p>"},{"location":"misc/partnerships/#brand","title":"Brand","text":"<p>A few numbers that reflect the 100% organic traction we've had over the past few years and the need it's filled in the community for bringing ML to production:</p> <ul> <li>#1 MLOps repository on GitHub (30K+ stars)</li> <li>40K+ subscribers to our community newsletter</li> <li>a highly recommended industry resource (testimonials, Twitter, LinkedIn)</li> <li>1.5M+ organic monthly website traffic (50K+ MAU)</li> <li>10K+ followers on Twitter &amp; LinkedIn (w/ great organic traction on every post)</li> <li>top organic SEO for common/popular search terms (mlops lessons, testing ml, experiment tracking, monitoring ml, data stack ml, etc.)</li> </ul>"},{"location":"misc/partnerships/#integration","title":"Integration","text":"<p>All of our lessons focus on first principles when approaching a concept. This helps develop the foundational understanding to be able to adapt to any stack. But to really solidify the understanding, we implement everything in code within the context of an end-to-end project. This helps understand the implicit value a tool provides and develop the decision making framework for constructing the appropriate stack. And because the community adopts what they've learned for their own use cases in industry, it's imperative that we use tools that can offer that enterprise maturity.</p> <p>Your product will be deeply integrated into the MLOps course, where thousands of developers everyday will use and assess your product for their industry contexts. All of this visibility and traction is invaluable for industry adoption, standing out in the competitive landscape and using the feedback to improve the product.</p> <p>We also have many downstream projects in progress to add more value on top of the content (async video course, private community, university labs, corporate onboarding, talent platform).</p>"},{"location":"misc/partnerships/#join-us","title":"Join us","text":"<p>If your team is interested in joining our mission, reach out to us via email to learn more!</p>"},{"location":"misc/reimbursement/","title":"Reimbursement Template","text":""},{"location":"misc/reimbursement/#_1","title":"Reimbursement Template","text":"<p>Instructions</p> <p>After you've applied and been accepted to the next cohort, copy and paste this email template below to send to your manager for reimbursement for the course. Feel free to add any additional details as you see fit.</p> <p>Subject: Reimbursement for Made With ML's MLOps Course</p> <p>Hi,</p> <p>Hope you're doing well. I was recently accepted into Made With ML's MLOps course, which is an interactive project-based course on MLOps fundamentals. The course costs $1,250 but the value I'll gain for myself and our team/company will pay for the course almost immediately. I added some key information about the course below and would love to get this career development opportunity reimbursed.</p> <p>Course page: https://madewithml.com/</p>"},{"location":"misc/reimbursement/#what-is-the-course","title":"What is the course?","text":"<p>An interactive project-based course to learn and apply the fundamentals of MLOps. I'll be learning to combine machine learning with software engineering best practices which I want to extend to build and improve our own systems. This course brings all of the MLOps best practices into one place, allowing me to quickly (and properly) learn it. And best of all, the course can be done before and after work, so it won't be interfering during work hours.</p> <p>Here's a quick look at the curriculum:</p>"},{"location":"misc/reimbursement/#whos-teaching-the-course","title":"Who's teaching the course?","text":"<p>The course is from Made With ML, one of the top ML repositories on GitHub (30K+ stars) with a growing community (30K+) and is a highly recommended resource used by industry. Their content not only covers MLOps concepts but they go deep into actually implementing everything with production quality code.</p>"},{"location":"misc/reimbursement/#how-will-this-help-me","title":"How will this help me?","text":"<p>I'll be learning the foundation I need to responsibly develop ML systems. This includes producing clean, production-grade code, testing my work, understanding MLOps (experiment management, monitoring, systems design, etc.) and data engineering (data stack, orchestration, feature stores) concepts.</p>"},{"location":"misc/reimbursement/#how-will-this-help-our-company","title":"How will this help our company?","text":"<p>What I learn will directly translate to better quality ML systems in our products. I'll also be able to engage in conversations with peers and management as we traverse this space to build what's right for us. And, most important of all, I'll be able to pass on what I learn as I collaborate with others in our team so we're all working towards building reliable ML systems.</p> <p>Thank you</p>"},{"location":"misc/subscribed/","title":"\u2705 Subscription confirmed","text":""},{"location":"misc/subscribed/#_1","title":"\u2705 Subscription confirmed","text":"<p>You're all set!</p> <p>1. Resource links</p> <ul> <li>start with the lessons on Made With ML</li> <li>check out the  GitHub repository for all the code</li> <li>connect on  Twitter,  LinkedIn and  Youtube</li> </ul> <p>2. Say hello</p> <p>Send me an email at goku@madewithml.com to say hi, a bit about yourself and what you're currently learning or working on. I personally respond to all emails and always love to meet people from the community.</p> <p>Upcoming live cohorts</p> <p>Sign up for our upcoming live cohort, where we'll provide live lessons + QA, compute (GPUs) and community to learn everything in one day.  </p>  Learn more <p></p>"},{"location":"styles/lesson/","title":"Lesson","text":""},{"location":"styles/page/","title":"Page","text":""}]}